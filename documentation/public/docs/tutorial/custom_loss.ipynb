{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sehh4woSx-LM"
      },
      "source": [
        "# Custom loss\n",
        "\n",
        "In gradient boosted trees, the loss is a function that takes a label value and a prediction, and returns the \"amount of error\" of this prediction. The model is trained to minimize the average loss over all the training examples. YDF implements various common losses. You can configure them with the \"loss\" parameter. You can see the list of available losses [here](https://ydf.readthedocs.io/en/latest/hyperparameters/#loss). If you don't specify the loss, it is selected automatically according to the model task. For instance, if the task is regression, the loss is set to mean-squared error by default.\n",
        "\n",
        "If YDF does not support a loss you needs, you can define it manually. This is called a \"custom loss\".\n",
        "\n",
        "In this introduction tutorial, we will create a custom **Regression Loss** called **Mean Squared Logarithmic Error**.\n",
        "\n",
        "## What is a custom loss?\n",
        "\n",
        "In YDF, a custom loss consists of four parts:\n",
        "*   **Initial prediction**: The initial prediction of the model, e.g. the average of the labels.\n",
        "*   **Gradient and Hessian**: A function that computes the gradient and the diagonal of the hessian of the loss given the label and the prediction of the model before the activation function (a.k.a. linkage function).\n",
        "*   **Loss**: A function that measures the quality of the current solution. While theory might dictate that the gradient and hessian are actually the gradient and hessian of the loss function, approximations do very well in practice.\n",
        "*   **Activation**: A function applied to the predictions to transform them to the correct space (e.g. probabilities for classification problems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyXWU-lHx-LO"
      },
      "source": [
        "# Training Gradient Boosted Trees with custom loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IL3OikYx-LO"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import ydf  # Yggdrasil Decision Forests\n",
        "import pandas as pd  # We use Pandas to load small datasets\n",
        "import numpy as np  # We use numpy for numerical operation\n",
        "import numpy.typing as npty\n",
        "from typing import Tuple\n",
        "\n",
        "# Download a regression dataset and load it as a Pandas DataFrame.\n",
        "ds_path = \"https://raw.githubusercontent.com/google/yggdrasil-decision-forests/main/yggdrasil_decision_forests/test_data/dataset\"\n",
        "all_ds = pd.read_csv(f\"{ds_path}/abalone.csv\")\n",
        "\n",
        "# Randomly split the dataset into a training (70%) and testing (30%) dataset\n",
        "all_ds = all_ds.sample(frac=1)\n",
        "split_idx = len(all_ds) * 7 // 10\n",
        "train_ds = all_ds.iloc[:split_idx]\n",
        "test_ds = all_ds.iloc[split_idx:]\n",
        "\n",
        "# Print the first 5 training examples\n",
        "train_ds.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvHNfWGex-LP"
      },
      "source": [
        "## Mean Squared Logarithmic Error\n",
        "\n",
        "We use **Mean Squared Logarithmic Error** (MSLE) loss for this tutorial. The MSLE is calculated as\n",
        "\n",
        "MSLE = $\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2$,\n",
        "\n",
        "where $n$ is the total number of observations, $p_i$ and $a_i$ are the prediction and label of example $i$, respectively, and $\\log$ denotes the natural logarithm.\n",
        "\n",
        "The gradient of the MSLE loss with respect to the prediction $p_i$ is\n",
        "\n",
        "$\\frac{1}{n} \\cdot \\frac{2(\\log(p_i + 1) - \\log(a_i+1))}{p_i + 1}$\n",
        "\n",
        "The hessian of the MSLE loss is a matrix. For simplicity and performance reasons, YDF only uses the diagonal of the hessian. The $i$th element of the diagonal is\n",
        "\n",
        "$\\frac{1}{n} \\cdot \\frac{2(1 - \\log(p_i + 1) + \\log(a_i+1))}{(p_i + 1)^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkIgVVOcx-LP"
      },
      "outputs": [],
      "source": [
        "# If predictions are close to -1, numerical instabilities will distort the\n",
        "# results. The predictions are therefore capped slightly above -1.\n",
        "PREDICTION_MINIMUM = -1 + 1e-6\n",
        "\n",
        "def loss_msle(\n",
        "    labels: npty.NDArray[np.float32],\n",
        "    predictions: npty.NDArray[np.float32],\n",
        "    weights: npty.NDArray[np.float32],\n",
        ") -\u003e np.float32:\n",
        "  clipped_pred = np.maximum(PREDICTION_MINIMUM, predictions)\n",
        "  return np.sum((np.log1p(labels) - np.log1p(clipped_pred))**2) / len(labels)\n",
        "\n",
        "def initial_predictions_msle(\n",
        "    labels: npty.NDArray[np.float32], _: npty.NDArray[np.float32]\n",
        ") -\u003e npty.NDArray[np.float32]:\n",
        "  return np.exp(np.mean(np.log1p(labels))) - 1\n",
        "\n",
        "def grad_msle(\n",
        "    labels: npty.NDArray[np.float32], predictions: npty.NDArray[np.float32]\n",
        ") -\u003e npty.NDArray[np.float32]:\n",
        "  gradient = (2/ len(labels))*(np.log1p(labels) - np.log1p(predictions)) / (predictions + 1)\n",
        "  return gradient\n",
        "\n",
        "def hessian_msle(\n",
        "    labels: npty.NDArray[np.float32], predictions: npty.NDArray[np.float32]\n",
        ") -\u003e npty.NDArray[np.float32]:\n",
        "  hessian =  (2/ len(labels))*(1 + np.log1p(labels) - np.log1p(predictions)) / (predictions + 1)**2\n",
        "  return hessian\n",
        "\n",
        "def gradient_and_hessian_msle(\n",
        "    labels: npty.NDArray[np.float32], predictions: npty.NDArray[np.float32]\n",
        ") -\u003e Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:\n",
        "  clipped_pred = np.maximum(PREDICTION_MINIMUM, predictions)\n",
        "  return [grad_msle(labels, clipped_pred), hessian_msle(labels, clipped_pred)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCUshyq0x-LP"
      },
      "outputs": [],
      "source": [
        "# Construct the loss object.\n",
        "msle_custom_loss = ydf.RegressionLoss(\n",
        "    initial_predictions=initial_predictions_msle,\n",
        "    gradient_and_hessian=gradient_and_hessian_msle,\n",
        "    loss=loss_msle,\n",
        "    activation=ydf.Activation.IDENTITY,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izg1y70Qx-LQ"
      },
      "source": [
        "The model is trained as usual with the loss object as a hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ3um0Xzx-LQ"
      },
      "outputs": [],
      "source": [
        "model = ydf.GradientBoostedTreesLearner(label=\"Rings\", task=ydf.Task.REGRESSION, loss=msle_custom_loss).train(train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGculbutNIl9"
      },
      "source": [
        "The model description shows the evolution of training loss and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2pNGD2ENGZq"
      },
      "outputs": [],
      "source": [
        "model.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_0c1Aqrx-LQ"
      },
      "source": [
        "We can compare this model to a model trained with RMSE loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxEOSboqx-LQ"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95wEE8NJx-LQ"
      },
      "outputs": [],
      "source": [
        "# A model trained with default regression loss (i.e. RMSE loss)\n",
        "model_rmse_loss = ydf.GradientBoostedTreesLearner(label=\"Rings\", task=ydf.Task.REGRESSION).train(train_ds)\n",
        "model_rmse_loss.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFjQJAovx-LQ"
      },
      "source": [
        "## Other custom losses\n",
        "\n",
        "### Binary Classification\n",
        "\n",
        "For **binary classification** problems, the labels are integers (1 for the\n",
        "positive class and 2 for the negative class). The model is expected to return\n",
        "the probability of the positive class. YDF supports the Sigmoid activation\n",
        "function for losses that do not operate in the probability space.\n",
        "\n",
        "For demonstration purposes, the code below re-implements the\n",
        "**Binomial Log Likelihood Loss** as a custom loss.\n",
        "Note that this loss is also available directly through the\n",
        "`loss=BINOMIAL_LOG_LIKELIHOOD` hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C25CRbAR1mww"
      },
      "outputs": [],
      "source": [
        "def binomial_initial_predictions(\n",
        "    labels: npty.NDArray[np.int32], weights: npty.NDArray[np.float32]\n",
        ") -\u003e np.float32:\n",
        "  sum_weights = np.sum(weights)\n",
        "  sum_weights_positive = np.sum((labels == 2) * weights)\n",
        "  ratio_positive = sum_weights_positive / sum_weights\n",
        "  if ratio_positive == 0.0:\n",
        "    return -np.iinfo(np.float32).max\n",
        "  elif ratio_positive == 1.0:\n",
        "    return np.iinfo(np.float32).max\n",
        "  return np.log(ratio_positive / (1 - ratio_positive))\n",
        "\n",
        "def binomial_gradient_and_hessian(\n",
        "    labels: npty.NDArray[np.int32], predictions: npty.NDArray[np.float32]\n",
        ") -\u003e Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:\n",
        "  pred_probability = 1.0 / (1.0 + np.exp(-predictions))\n",
        "  binary_labels = labels == 2\n",
        "  return (\n",
        "      binary_labels - pred_probability,\n",
        "      pred_probability * (1 - pred_probability),\n",
        "  )\n",
        "\n",
        "def binomial_loss(\n",
        "    labels: npty.NDArray[np.int32],\n",
        "    predictions: npty.NDArray[np.float32],\n",
        "    weights: npty.NDArray[np.float32],\n",
        ") -\u003e np.float32:\n",
        "  binary_labels = labels == 2\n",
        "  return (-2.0 * np.sum(\n",
        "        binary_labels * predictions- np.log(1.0 + np.exp(predictions))\n",
        "      ) / len(labels)\n",
        "  )\n",
        "\n",
        "binomial_custom_loss = ydf.BinaryClassificationLoss(\n",
        "    initial_predictions=binomial_initial_predictions,\n",
        "    gradient_and_hessian=binomial_gradient_and_hessian,\n",
        "    loss=binomial_loss,\n",
        "    activation=ydf.Activation.SIGMOID,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyq67KBU1mww"
      },
      "source": [
        "### Multi-class classification\n",
        "\n",
        "For **multi-class classification** problems, the labels are integers starting\n",
        "with 1. The loss function must provide a gradient and hessian\n",
        "*for each label class*. The gradient and hessian must return d-by-n matrices,\n",
        "where n is the number of examples and d is the number of label classes.\n",
        "Similarly, the model must provide an initial prediction for each label class as\n",
        "as a vector of d elements.\n",
        "\n",
        "YDF supports the Softmax activation function for losses that do not operate in\n",
        "the probability space.\n",
        "\n",
        "For demonstration purposes, the code below re-implements the\n",
        "**Multinomial Log Likelihood Loss** as a custom loss. Note that this loss is\n",
        "also available directly through the `loss=MULTINOMIAL_LOG_LIKELIHOOD`\n",
        "hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynBByhSF1mww"
      },
      "outputs": [],
      "source": [
        "def multinomial_initial_predictions(\n",
        "    labels: npty.NDArray[np.int32], _: npty.NDArray[np.float32]\n",
        ") -\u003e npty.NDArray[np.float32]:\n",
        "  dimension = np.max(labels)\n",
        "  return np.zeros(dimension, dtype=np.float32)\n",
        "\n",
        "def multinomial_gradient(\n",
        "    labels: npty.NDArray[np.int32], predictions: npty.NDArray[np.float32]\n",
        ") -\u003e Tuple[npty.NDArray[np.float32], npty.NDArray[np.float32]]:\n",
        "  dimension = np.max(labels)\n",
        "  normalization = 1.0 / np.sum(np.exp(predictions), axis=1)\n",
        "  normalized_predictions = np.exp(predictions) * normalization[:, None]\n",
        "  label_indicator = (\n",
        "      (labels - 1)[:, np.newaxis] == np.arange(dimension)\n",
        "  ).astype(int)\n",
        "  gradient = label_indicator - normalized_predictions\n",
        "  hessian = np.abs(gradient) * (1 - np.abs(gradient))\n",
        "  return (np.transpose(gradient), np.transpose(hessian))\n",
        "\n",
        "def multinomial_loss(\n",
        "    labels: npty.NDArray[np.int32],\n",
        "    predictions: npty.NDArray[np.float32],\n",
        "    weights: npty.NDArray[np.float32],\n",
        ") -\u003e np.float32:\n",
        "  dimension = np.max(labels)\n",
        "  sum_exp_pred = np.sum(np.exp(predictions), axis=1)\n",
        "  indicator_matrix = (\n",
        "      (labels - 1)[:, np.newaxis] == np.arange(dimension)\n",
        "  ).astype(int)\n",
        "  label_exp_pred = np.exp(np.sum(predictions * indicator_matrix, axis=1))\n",
        "  return (\n",
        "      -np.sum(np.log(label_exp_pred / sum_exp_pred)) / len(labels)\n",
        "  )\n",
        "\n",
        "multinomial_custom_loss = ydf.MultiClassificationLoss(\n",
        "    initial_predictions=multinomial_initial_predictions,\n",
        "    gradient_and_hessian=multinomial_gradient,\n",
        "    loss=multinomial_loss,\n",
        "    activation=ydf.Activation.SOFTMAX,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1uc55lW1G1O_kUp0FPbli7_M7h-52TMFc",
          "timestamp": 1708008633814
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
