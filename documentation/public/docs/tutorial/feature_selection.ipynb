{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/yggdrasil-decision-forests/blob/main/documentation/public/docs/tutorial/feature_selection.ipynb)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ydf -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is feature selection?\n",
    "\n",
    "**Feature selection** is the process of automatically identifying and removing unnecessary input features to improve the quality, inference speed, and subsequent training speed of a model. In YDF, feature selection is enabled by setting the `feature_selector` learner argument.\n",
    "\n",
    "Training a model with feature selection is slower than training a model without feature selection as the former involves training and processing multiple intermediate models. Therefore, feature selection is generally only done once at the end of the model development after model hyper-parameter and input features have been determined.\n",
    "\n",
    "In YDF, the feature selection algorithm is \"guided\" by two values: A quality metric to optimize (e.g., accuracy, logloss, AUC) and a variable importance metric (e.g., mean decrease in accuracy). Those are selected automatically unless manually specified by the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial\n",
    "\n",
    "In this tutorial, we train and compare two models: One without and one with feature selection on the [christine dataset](https://www.openml.org/search?type=data&sort=runs&id=41142&status=active)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V1627</th>\n",
       "      <th>V1628</th>\n",
       "      <th>V1629</th>\n",
       "      <th>V1630</th>\n",
       "      <th>V1631</th>\n",
       "      <th>V1632</th>\n",
       "      <th>V1633</th>\n",
       "      <th>V1634</th>\n",
       "      <th>V1635</th>\n",
       "      <th>V1636</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'0'</td>\n",
       "      <td>443.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>174.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'1'</td>\n",
       "      <td>486.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>...</td>\n",
       "      <td>147.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'1'</td>\n",
       "      <td>277.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>355.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'0'</td>\n",
       "      <td>220.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>...</td>\n",
       "      <td>303.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'1'</td>\n",
       "      <td>288.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>521.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1637 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class     V1     V2     V3     V4     V5     V6     V7     V8     V9  ...  \\\n",
       "0  b'0'  443.0  375.0  109.0  423.0  534.0  296.0   82.0  571.0  132.0  ...   \n",
       "1  b'1'  486.0  716.0  136.0  231.0  705.0  343.0  241.0  501.0  174.0  ...   \n",
       "2  b'1'  277.0  424.0   75.0  264.0  639.0  172.0   87.0  600.0  100.0  ...   \n",
       "3  b'0'  220.0  459.0  117.0  280.0  767.0  286.0  187.0  684.0  141.0  ...   \n",
       "4  b'1'  288.0  414.0  121.0  285.0  556.0  356.0  105.0  739.0  205.0  ...   \n",
       "\n",
       "   V1627  V1628  V1629  V1630  V1631  V1632  V1633  V1634  V1635  V1636  \n",
       "0  174.0   23.0   13.0  167.0  168.0  286.0  517.0  388.0  197.0  331.0  \n",
       "1  147.0  241.0   62.0  278.0  208.0  167.0  282.0  222.0    0.0  380.0  \n",
       "2  355.0   33.0   21.0  237.0  111.0  109.0  433.0  329.0    0.0  299.0  \n",
       "3  303.0  376.0   45.0  222.0  107.0  360.0  271.0  381.0    0.0  329.0  \n",
       "4  361.0    0.0    8.0  133.0  150.0  176.0  656.0  408.0  293.0  521.0  \n",
       "\n",
       "[5 rows x 1637 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with urllib.request.urlopen(\n",
    "    \"https://www.openml.org/data/download/19335515/file764d5d063390.arff\"\n",
    ") as response:\n",
    "  raw_data = arff.loadarff(io.StringIO(response.read().decode(\"utf-8\")))\n",
    "dataset = pd.DataFrame(raw_data[0])\n",
    "dataset.head(5)  # Print the first 5 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1637 features and 5418 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"The dataset contains {dataset.shape[1]} features and\"\n",
    "    f\" {dataset.shape[0]} rows.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into a training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = train_test_split(\n",
    "    dataset, test_size=0.3, random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model without feature selection\n",
    "\n",
    "Let's train and evaluate a model with all the default parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Warning] Column 'V496' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n",
      "Column 'V566' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n",
      "Column 'V588' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n",
      "Column 'V685' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n",
      "Column 'V741' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n",
      "Column 'V951' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n",
      "Column 'V1607' is detected as CATEGORICAL but its values look like numbers (e.g., 0, 1). Should the column not be NUMERICAL instead? If so, feed numerical values instead of strings or objects.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.416616\n",
      "Accuracy: 0.7170971709717097\n",
      "Number of features: 1636\n"
     ]
    }
   ],
   "source": [
    "model_1 = ydf.RandomForestLearner(label=\"class\").train(train_dataset)\n",
    "\n",
    "evaluation_1 = model_1.evaluate(test_dataset)\n",
    "\n",
    "print(\"Accuracy:\", evaluation_1.accuracy)\n",
    "print(\"Number of features:\", len(model_1.input_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all 1636 input features, the model's accuracy is 0.7171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model with feature selection\n",
    "\n",
    "Now, let's train a similar model with feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run backward feature selection on 1636 features\n",
      "[Iteration 0] Train model on 1636 features\n",
      "Optimizing metric \"loss\". The available metrics are ['loss', 'num_examples_weighted', 'accuracy']\n",
      "[Iteration 0] Score:-0.551959 Metrics:{'loss': 0.5519588122415486, 'num_examples_weighted': 3792.0, 'accuracy': 0.7241561181434599}\n",
      "Guide feature selection using \"MEAN_DECREASE_IN_AUC_1_VS_OTHERS\" variable importance. The available variable importances are ['MEAN_DECREASE_IN_PRAUC_1_VS_OTHERS', 'MEAN_DECREASE_IN_ACCURACY', 'MEAN_DECREASE_IN_AP_1_VS_OTHERS', 'NUM_AS_ROOT', 'SUM_SCORE', 'MEAN_DECREASE_IN_AUC_1_VS_OTHERS', 'NUM_NODES', 'INV_MEAN_MIN_DEPTH']\n",
      "[Iteration 1] Train model on 1473 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:54.265457\n",
      "[Iteration 1] Score:-0.546692 Metrics:{'loss': 0.5466921646462337, 'num_examples_weighted': 3792.0, 'accuracy': 0.7310126582278481}\n",
      "[Iteration 2] Train model on 1326 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:42.468182\n",
      "[Iteration 2] Score:-0.549135 Metrics:{'loss': 0.5491350598668656, 'num_examples_weighted': 3792.0, 'accuracy': 0.7273206751054853}\n",
      "[Iteration 3] Train model on 1194 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:32.116937\n",
      "[Iteration 3] Score:-0.549242 Metrics:{'loss': 0.5492424538299119, 'num_examples_weighted': 3792.0, 'accuracy': 0.7333860759493671}\n",
      "[Iteration 4] Train model on 1075 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:23.803081\n",
      "[Iteration 4] Score:-0.546712 Metrics:{'loss': 0.5467118822049962, 'num_examples_weighted': 3792.0, 'accuracy': 0.729957805907173}\n",
      "[Iteration 5] Train model on 968 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:15.065606\n",
      "[Iteration 5] Score:-0.546394 Metrics:{'loss': 0.5463942248336893, 'num_examples_weighted': 3792.0, 'accuracy': 0.7318037974683544}\n",
      "[Iteration 6] Train model on 872 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:07.834621\n",
      "[Iteration 6] Score:-0.547161 Metrics:{'loss': 0.5471607226444181, 'num_examples_weighted': 3792.0, 'accuracy': 0.7273206751054853}\n",
      "[Iteration 7] Train model on 785 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:01:00.677536\n",
      "[Iteration 7] Score:-0.543218 Metrics:{'loss': 0.5432177659424753, 'num_examples_weighted': 3792.0, 'accuracy': 0.7318037974683544}\n",
      "[Iteration 8] Train model on 707 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:54.859941\n",
      "[Iteration 8] Score:-0.544313 Metrics:{'loss': 0.5443127457315755, 'num_examples_weighted': 3792.0, 'accuracy': 0.7307489451476793}\n",
      "[Iteration 9] Train model on 637 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:49.434060\n",
      "[Iteration 9] Score:-0.54376 Metrics:{'loss': 0.5437597303006814, 'num_examples_weighted': 3792.0, 'accuracy': 0.7331223628691983}\n",
      "[Iteration 10] Train model on 574 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:44.480250\n",
      "[Iteration 10] Score:-0.541083 Metrics:{'loss': 0.5410828440831306, 'num_examples_weighted': 3792.0, 'accuracy': 0.7336497890295358}\n",
      "[Iteration 11] Train model on 517 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:40.098351\n",
      "[Iteration 11] Score:-0.541494 Metrics:{'loss': 0.5414938438361518, 'num_examples_weighted': 3792.0, 'accuracy': 0.7354957805907173}\n",
      "[Iteration 12] Train model on 466 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:35.729285\n",
      "[Iteration 12] Score:-0.542251 Metrics:{'loss': 0.5422514577470877, 'num_examples_weighted': 3792.0, 'accuracy': 0.7389240506329114}\n",
      "[Iteration 13] Train model on 420 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:32.801923\n",
      "[Iteration 13] Score:-0.538332 Metrics:{'loss': 0.5383323574063411, 'num_examples_weighted': 3792.0, 'accuracy': 0.745253164556962}\n",
      "[Iteration 14] Train model on 378 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:29.131270\n",
      "[Iteration 14] Score:-0.53685 Metrics:{'loss': 0.5368497337490705, 'num_examples_weighted': 3792.0, 'accuracy': 0.742879746835443}\n",
      "[Iteration 15] Train model on 341 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:26.563789\n",
      "[Iteration 15] Score:-0.534009 Metrics:{'loss': 0.5340092282124327, 'num_examples_weighted': 3792.0, 'accuracy': 0.7420886075949367}\n",
      "[Iteration 16] Train model on 307 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:23.634958\n",
      "[Iteration 16] Score:-0.530249 Metrics:{'loss': 0.5302487336902757, 'num_examples_weighted': 3792.0, 'accuracy': 0.7402426160337553}\n",
      "[Iteration 17] Train model on 277 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:21.292689\n",
      "[Iteration 17] Score:-0.528642 Metrics:{'loss': 0.528641837179702, 'num_examples_weighted': 3792.0, 'accuracy': 0.7420886075949367}\n",
      "[Iteration 18] Train model on 250 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:19.416434\n",
      "[Iteration 18] Score:-0.526555 Metrics:{'loss': 0.5265549925491921, 'num_examples_weighted': 3792.0, 'accuracy': 0.7431434599156118}\n",
      "[Iteration 19] Train model on 225 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:17.225508\n",
      "[Iteration 19] Score:-0.524635 Metrics:{'loss': 0.5246354631390934, 'num_examples_weighted': 3792.0, 'accuracy': 0.745253164556962}\n",
      "[Iteration 20] Train model on 203 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:15.566344\n",
      "[Iteration 20] Score:-0.523595 Metrics:{'loss': 0.5235947274656, 'num_examples_weighted': 3792.0, 'accuracy': 0.7460443037974683}\n",
      "[Iteration 21] Train model on 183 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:13.994888\n",
      "[Iteration 21] Score:-0.520246 Metrics:{'loss': 0.5202456676579996, 'num_examples_weighted': 3792.0, 'accuracy': 0.7502637130801688}\n",
      "[Iteration 22] Train model on 165 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:12.874504\n",
      "[Iteration 22] Score:-0.516373 Metrics:{'loss': 0.5163733388396974, 'num_examples_weighted': 3792.0, 'accuracy': 0.7539556962025317}\n",
      "[Iteration 23] Train model on 149 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:11.620316\n",
      "[Iteration 23] Score:-0.514395 Metrics:{'loss': 0.514394582862001, 'num_examples_weighted': 3792.0, 'accuracy': 0.7478902953586498}\n",
      "[Iteration 24] Train model on 135 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:10.350541\n",
      "[Iteration 24] Score:-0.513139 Metrics:{'loss': 0.5131387805265575, 'num_examples_weighted': 3792.0, 'accuracy': 0.7507911392405063}\n",
      "[Iteration 25] Train model on 122 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:09.355877\n",
      "[Iteration 25] Score:-0.515482 Metrics:{'loss': 0.5154824736191624, 'num_examples_weighted': 3792.0, 'accuracy': 0.7510548523206751}\n",
      "[Iteration 26] Train model on 110 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:08.438162\n",
      "[Iteration 26] Score:-0.512282 Metrics:{'loss': 0.51228179366039, 'num_examples_weighted': 3792.0, 'accuracy': 0.7563291139240507}\n",
      "[Iteration 27] Train model on 99 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:07.726586\n",
      "[Iteration 27] Score:-0.512737 Metrics:{'loss': 0.5127371263683907, 'num_examples_weighted': 3792.0, 'accuracy': 0.7470991561181435}\n",
      "[Iteration 28] Train model on 90 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:07.063153\n",
      "[Iteration 28] Score:-0.510277 Metrics:{'loss': 0.5102765016915485, 'num_examples_weighted': 3792.0, 'accuracy': 0.7536919831223629}\n",
      "[Iteration 29] Train model on 81 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:06.429041\n",
      "[Iteration 29] Score:-0.507455 Metrics:{'loss': 0.5074545453751522, 'num_examples_weighted': 3792.0, 'accuracy': 0.7539556962025317}\n",
      "[Iteration 30] Train model on 73 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:05.807677\n",
      "[Iteration 30] Score:-0.511792 Metrics:{'loss': 0.5117917528787612, 'num_examples_weighted': 3792.0, 'accuracy': 0.7515822784810127}\n",
      "[Iteration 31] Train model on 66 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:05.236060\n",
      "[Iteration 31] Score:-0.510255 Metrics:{'loss': 0.5102553019284876, 'num_examples_weighted': 3792.0, 'accuracy': 0.7534282700421941}\n",
      "[Iteration 32] Train model on 60 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:04.915625\n",
      "[Iteration 32] Score:-0.508708 Metrics:{'loss': 0.5087084588803497, 'num_examples_weighted': 3792.0, 'accuracy': 0.7473628691983122}\n",
      "[Iteration 33] Train model on 54 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:04.448664\n",
      "[Iteration 33] Score:-0.509896 Metrics:{'loss': 0.5098955553891491, 'num_examples_weighted': 3792.0, 'accuracy': 0.7536919831223629}\n",
      "[Iteration 34] Train model on 49 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:04.099602\n",
      "[Iteration 34] Score:-0.507959 Metrics:{'loss': 0.5079589437411898, 'num_examples_weighted': 3792.0, 'accuracy': 0.7531645569620253}\n",
      "[Iteration 35] Train model on 45 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:03.777165\n",
      "[Iteration 35] Score:-0.517889 Metrics:{'loss': 0.5178887767671561, 'num_examples_weighted': 3792.0, 'accuracy': 0.7560654008438819}\n",
      "[Iteration 36] Train model on 41 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:03.475699\n",
      "[Iteration 36] Score:-0.510443 Metrics:{'loss': 0.5104431864994142, 'num_examples_weighted': 3792.0, 'accuracy': 0.7552742616033755}\n",
      "[Iteration 37] Train model on 37 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:03.121523\n",
      "[Iteration 37] Score:-0.514292 Metrics:{'loss': 0.5142918971543031, 'num_examples_weighted': 3792.0, 'accuracy': 0.7518459915611815}\n",
      "[Iteration 38] Train model on 34 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.853675\n",
      "[Iteration 38] Score:-0.518338 Metrics:{'loss': 0.5183380168412207, 'num_examples_weighted': 3792.0, 'accuracy': 0.7507911392405063}\n",
      "[Iteration 39] Train model on 31 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.622884\n",
      "[Iteration 39] Score:-0.515307 Metrics:{'loss': 0.5153067024645223, 'num_examples_weighted': 3792.0, 'accuracy': 0.7481540084388185}\n",
      "[Iteration 40] Train model on 28 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.448911\n",
      "[Iteration 40] Score:-0.511005 Metrics:{'loss': 0.5110048497196892, 'num_examples_weighted': 3792.0, 'accuracy': 0.7531645569620253}\n",
      "[Iteration 41] Train model on 26 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.228971\n",
      "[Iteration 41] Score:-0.517346 Metrics:{'loss': 0.5173464120721311, 'num_examples_weighted': 3792.0, 'accuracy': 0.7507911392405063}\n",
      "[Iteration 42] Train model on 24 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.146538\n",
      "[Iteration 42] Score:-0.512426 Metrics:{'loss': 0.5124263804401507, 'num_examples_weighted': 3792.0, 'accuracy': 0.7489451476793249}\n",
      "[Iteration 43] Train model on 22 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.008157\n",
      "[Iteration 43] Score:-0.51368 Metrics:{'loss': 0.5136797969973202, 'num_examples_weighted': 3792.0, 'accuracy': 0.7510548523206751}\n",
      "[Iteration 44] Train model on 20 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.790101\n",
      "[Iteration 44] Score:-0.515758 Metrics:{'loss': 0.5157581914984032, 'num_examples_weighted': 3792.0, 'accuracy': 0.7507911392405063}\n",
      "[Iteration 45] Train model on 18 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.520261\n",
      "[Iteration 45] Score:-0.513439 Metrics:{'loss': 0.5134392472274821, 'num_examples_weighted': 3792.0, 'accuracy': 0.7505274261603375}\n",
      "[Iteration 46] Train model on 17 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.505416\n",
      "[Iteration 46] Score:-0.51819 Metrics:{'loss': 0.518190367240164, 'num_examples_weighted': 3792.0, 'accuracy': 0.7447257383966245}\n",
      "[Iteration 47] Train model on 16 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.383692\n",
      "[Iteration 47] Score:-0.516085 Metrics:{'loss': 0.516085256385727, 'num_examples_weighted': 3792.0, 'accuracy': 0.7507911392405063}\n",
      "[Iteration 48] Train model on 15 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.383366\n",
      "[Iteration 48] Score:-0.514716 Metrics:{'loss': 0.5147162458534207, 'num_examples_weighted': 3792.0, 'accuracy': 0.7529008438818565}\n",
      "[Iteration 49] Train model on 14 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.250410\n",
      "[Iteration 49] Score:-0.519566 Metrics:{'loss': 0.5195658261453505, 'num_examples_weighted': 3792.0, 'accuracy': 0.7515822784810127}\n",
      "[Iteration 50] Train model on 13 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.193855\n",
      "[Iteration 50] Score:-0.527063 Metrics:{'loss': 0.5270631320380118, 'num_examples_weighted': 3792.0, 'accuracy': 0.7515822784810127}\n",
      "[Iteration 51] Train model on 12 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.110411\n",
      "[Iteration 51] Score:-0.541045 Metrics:{'loss': 0.5410449845512016, 'num_examples_weighted': 3792.0, 'accuracy': 0.7463080168776371}\n",
      "[Iteration 52] Train model on 11 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.066272\n",
      "[Iteration 52] Score:-0.534168 Metrics:{'loss': 0.5341682232880006, 'num_examples_weighted': 3792.0, 'accuracy': 0.7478902953586498}\n",
      "[Iteration 53] Train model on 10 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.042781\n",
      "[Iteration 53] Score:-0.535473 Metrics:{'loss': 0.5354732009567486, 'num_examples_weighted': 3792.0, 'accuracy': 0.7468354430379747}\n",
      "[Iteration 54] Train model on 9 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.901856\n",
      "[Iteration 54] Score:-0.535162 Metrics:{'loss': 0.5351617793281924, 'num_examples_weighted': 3792.0, 'accuracy': 0.7373417721518988}\n",
      "[Iteration 55] Train model on 8 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.789105\n",
      "[Iteration 55] Score:-0.549058 Metrics:{'loss': 0.549057705600241, 'num_examples_weighted': 3792.0, 'accuracy': 0.7344409282700421}\n",
      "[Iteration 56] Train model on 7 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.716133\n",
      "[Iteration 56] Score:-0.551671 Metrics:{'loss': 0.5516710231109633, 'num_examples_weighted': 3792.0, 'accuracy': 0.7196729957805907}\n",
      "[Iteration 57] Train model on 6 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.636091\n",
      "[Iteration 57] Score:-0.567408 Metrics:{'loss': 0.5674077806517115, 'num_examples_weighted': 3792.0, 'accuracy': 0.7194092827004219}\n",
      "[Iteration 58] Train model on 5 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.569630\n",
      "[Iteration 58] Score:-0.588339 Metrics:{'loss': 0.5883394792095861, 'num_examples_weighted': 3792.0, 'accuracy': 0.7038502109704642}\n",
      "[Iteration 59] Train model on 4 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.485745\n",
      "[Iteration 59] Score:-0.611016 Metrics:{'loss': 0.6110159856788441, 'num_examples_weighted': 3792.0, 'accuracy': 0.705168776371308}\n",
      "[Iteration 60] Train model on 3 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.412591\n",
      "[Iteration 60] Score:-0.654282 Metrics:{'loss': 0.6542816546392914, 'num_examples_weighted': 3792.0, 'accuracy': 0.6964662447257384}\n",
      "[Iteration 61] Train model on 2 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.333269\n",
      "[Iteration 61] Score:-0.759617 Metrics:{'loss': 0.7596166488287905, 'num_examples_weighted': 3792.0, 'accuracy': 0.6737869198312236}\n",
      "[Iteration 62] Train model on 1 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.223924\n",
      "[Iteration 62] Score:-1.82378 Metrics:{'loss': 1.8237768061927428, 'num_examples_weighted': 3792.0, 'accuracy': 0.6215717299578059}\n",
      "The best subset of features was found at iteration 29 with score:-0.507455, metrics:{'loss': 0.5074545453751522, 'num_examples_weighted': 3792.0, 'accuracy': 0.7539556962025317} and 81/1636 selected features\n",
      "Accuracy: 0.7478474784747847\n",
      "Number of features: 81\n"
     ]
    }
   ],
   "source": [
    "model_2 = ydf.RandomForestLearner(\n",
    "    label=\"class\",\n",
    "    # Tell the Random Forest to compute Out-of-bag variable importances.\n",
    "    # This is expensive but gives the best results.\n",
    "    compute_oob_variable_importances=True,\n",
    "    # Enable feature selection using the \"backward selection\" algorithm.\n",
    "    feature_selector=ydf.BackwardSelectionFeatureSelector(),\n",
    ").train(train_dataset)\n",
    "\n",
    "evaluation_2 = model_2.evaluate(test_dataset)\n",
    "\n",
    "print(\"Accuracy:\", evaluation_2.accuracy)\n",
    "print(\"Number of features:\", len(model_2.input_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model take much longer to train. However, this model only uses 81 features (the previous model used 1636 features) and has a better accuracy of 0.7478 (the previous model had an accuracy of 0.7170).\n",
    "\n",
    "The feature selection logs are available in the \"Feature selection\" tab of the model description:\n",
    "\n",
    "**Click on the \"Feature selection\" tab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       ".tab_block .header {\n",
       "    flex-direction: row;\n",
       "    display: flex;\n",
       "}\n",
       "\n",
       ".tab_block .header .tab {\n",
       "    cursor: pointer;\n",
       "    background-color: #F6F5F5;\n",
       "    text-decoration: none;\n",
       "    text-align: center;\n",
       "    padding: 4px 12px;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".tab_block .header .tab.selected {\n",
       "    border-bottom: 2px solid #2F80ED;\n",
       "}\n",
       "\n",
       ".tab_block .header .tab:hover {\n",
       "    text-decoration: none;\n",
       "    background-color: #DCDCDC;\n",
       "}\n",
       "\n",
       ".tab_block .body .tab_content {\n",
       "    display: none;\n",
       "    padding: 5px;\n",
       "}\n",
       "\n",
       ".tab_block .body .tab_content.selected {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".ydf_pre {\n",
       "    font-size: medium;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       ".variable_importance {\n",
       "}\n",
       "\n",
       ".variable_importance select {\n",
       "}\n",
       "\n",
       ".variable_importance .content {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".variable_importance .content.selected {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".ydf_tuning_table {\n",
       "  border-collapse: collapse;\n",
       "  border: 1px solid lightgray;\n",
       "}\n",
       "\n",
       ".ydf_tuning_table th {\n",
       "  background-color: #ededed;\n",
       "  font-weight: bold;\n",
       "  text-align: left;\n",
       "  padding: 3px 4px;\n",
       "  border: 1px solid lightgray;\n",
       "}\n",
       "\n",
       ".ydf_tuning_table td {\n",
       "  text-align: right;\n",
       "  padding: 3px 4px;\n",
       "  border: 1px solid lightgray;\n",
       "}\n",
       "\n",
       ".ydf_tuning_table .best {\n",
       "  background-color: khaki;\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "\n",
       "function ydfShowTab(block_id, item) {\n",
       "    const block = document.getElementById(block_id);\n",
       "    \n",
       "    \n",
       "    console.log(\"HIDE first of:\",block.getElementsByClassName(\"tab selected\"));\n",
       "    console.log(\"HIDE first of:\",block.getElementsByClassName(\"tab_content selected\"));\n",
       "    \n",
       "    block.getElementsByClassName(\"tab selected\")[0].classList.remove(\"selected\");\n",
       "    block.getElementsByClassName(\"tab_content selected\")[0].classList.remove(\"selected\");\n",
       "    document.getElementById(block_id + \"_\" + item).classList.add(\"selected\");\n",
       "    document.getElementById(block_id + \"_body_\" + item).classList.add(\"selected\");\n",
       "}\n",
       "  \n",
       "\n",
       "function ydfShowVariableImportance(block_id) {\n",
       "    const block = document.getElementById(block_id);\n",
       "    const item = block.getElementsByTagName(\"select\")[0].value;\n",
       "    block.getElementsByClassName(\"content selected\")[0].classList.remove(\"selected\");\n",
       "    document.getElementById(block_id + \"_body_\" + item).classList.add(\"selected\");\n",
       "}\n",
       "\n",
       "</script>\n",
       "  <div class=\"tab_block\" id=\"57d7-96f5-1cb9-4452\"><div class=\"header\"><a id=\"57d7-96f5-1cb9-4452_model\" class=\"tab selected\" onclick=\"ydfShowTab('57d7-96f5-1cb9-4452', 'model')\">Model</a><a id=\"57d7-96f5-1cb9-4452_dataspec\" class=\"tab\" onclick=\"ydfShowTab('57d7-96f5-1cb9-4452', 'dataspec')\">Dataspec</a><a id=\"57d7-96f5-1cb9-4452_feature_selector\" class=\"tab\" onclick=\"ydfShowTab('57d7-96f5-1cb9-4452', 'feature_selector')\">Feature selection</a><a id=\"57d7-96f5-1cb9-4452_training\" class=\"tab\" onclick=\"ydfShowTab('57d7-96f5-1cb9-4452', 'training')\">Training</a><a id=\"57d7-96f5-1cb9-4452_variable_importance\" class=\"tab\" onclick=\"ydfShowTab('57d7-96f5-1cb9-4452', 'variable_importance')\">Variable importances</a><a id=\"57d7-96f5-1cb9-4452_structure\" class=\"tab\" onclick=\"ydfShowTab('57d7-96f5-1cb9-4452', 'structure')\">Structure</a></div><div class=\"body\"><div id=\"57d7-96f5-1cb9-4452_body_model\" class=\"tab_content selected\"><b>Name</b> : RANDOM_FOREST<br><b>Task</b> : CLASSIFICATION<br><b>Label</b> : class<br><b>Features (81)</b> : V673 V226 V1283 V1191 V871 V190 V574 V1226 V911 V1460 V1199 V1411 V1485 V1068 V637 V250 V1042 V394 V1553 V560 V189 V1286 V28 V517 V790 V1369 V158 V418 V1281 V512 V367 V195 V1571 V67 V955 V1523 V648 V1325 V630 V147 V569 V925 V445 V316 V625 V1181 V1619 V913 V1630 V650 V616 V374 V1189 V1344 V1319 V292 V969 V214 V1487 V245 V1301 V262 V647 V584 V688 V1048 V1010 V780 V1187 V239 V935 V1239 V833 V1256 V1356 V857 V257 V575 V478 V1240 V73<br><b>Weights</b> : None<br><b>Trained with tuner</b> : No<br><b>Model size</b> : 49714 kB<br></div><div id=\"57d7-96f5-1cb9-4452_body_dataspec\" class=\"tab_content\"><pre class=\"ydf_pre\">Number of records: 3792\n",
       "Number of columns: 82\n",
       "\n",
       "Number of columns by type:\n",
       "\tNUMERICAL: 57 (69.5122%)\n",
       "\tCATEGORICAL: 25 (30.4878%)\n",
       "\n",
       "Columns:\n",
       "\n",
       "NUMERICAL: 57 (69.5122%)\n",
       "\t0: &quot;V673&quot; NUMERICAL mean:178.918 min:23 max:856 sd:73.4139 dtype:DTYPE_FLOAT64\n",
       "\t1: &quot;V226&quot; NUMERICAL mean:251.927 min:2 max:859 sd:116.094 dtype:DTYPE_FLOAT64\n",
       "\t2: &quot;V1283&quot; NUMERICAL mean:136.142 min:0 max:764 sd:132.493 dtype:DTYPE_FLOAT64\n",
       "\t3: &quot;V1191&quot; NUMERICAL mean:404.42 min:0 max:921 sd:140.729 dtype:DTYPE_FLOAT64\n",
       "\t4: &quot;V871&quot; NUMERICAL mean:313.39 min:0 max:938 sd:137.656 dtype:DTYPE_FLOAT64\n",
       "\t5: &quot;V190&quot; NUMERICAL mean:323.018 min:34 max:841 sd:117.027 dtype:DTYPE_FLOAT64\n",
       "\t6: &quot;V574&quot; NUMERICAL mean:531.012 min:71 max:888 sd:94.0633 dtype:DTYPE_FLOAT64\n",
       "\t7: &quot;V1226&quot; NUMERICAL mean:508.677 min:59 max:946 sd:105.059 dtype:DTYPE_FLOAT64\n",
       "\t8: &quot;V911&quot; NUMERICAL mean:396.267 min:27 max:999 sd:100.429 dtype:DTYPE_FLOAT64\n",
       "\t9: &quot;V1460&quot; NUMERICAL mean:474.59 min:116 max:869 sd:107.92 dtype:DTYPE_FLOAT64\n",
       "\t10: &quot;V1199&quot; NUMERICAL mean:405.534 min:1 max:897 sd:116.598 dtype:DTYPE_FLOAT64\n",
       "\t11: &quot;V1411&quot; NUMERICAL mean:169.562 min:1 max:775 sd:74.9672 dtype:DTYPE_FLOAT64\n",
       "\t12: &quot;V1485&quot; NUMERICAL mean:213.265 min:0 max:999 sd:94.0367 dtype:DTYPE_FLOAT64\n",
       "\t13: &quot;V1068&quot; NUMERICAL mean:957.023 min:0 max:998 sd:56.7404 dtype:DTYPE_FLOAT64\n",
       "\t14: &quot;V637&quot; NUMERICAL mean:157.691 min:0 max:696 sd:84.7323 dtype:DTYPE_FLOAT64\n",
       "\t15: &quot;V250&quot; NUMERICAL mean:217.008 min:99 max:929 sd:54.8482 dtype:DTYPE_FLOAT64\n",
       "\t16: &quot;V1042&quot; NUMERICAL mean:206.41 min:0 max:973 sd:131.078 dtype:DTYPE_FLOAT64\n",
       "\t17: &quot;V394&quot; NUMERICAL mean:267.401 min:0 max:628 sd:49.9153 dtype:DTYPE_FLOAT64\n",
       "\t18: &quot;V1553&quot; NUMERICAL mean:244.364 min:0 max:999 sd:138.929 dtype:DTYPE_FLOAT64\n",
       "\t19: &quot;V560&quot; NUMERICAL mean:35.3489 min:0 max:509 sd:57.2433 dtype:DTYPE_FLOAT64\n",
       "\t20: &quot;V189&quot; NUMERICAL mean:189.481 min:0 max:836 sd:166.286 dtype:DTYPE_FLOAT64\n",
       "\t21: &quot;V1286&quot; NUMERICAL mean:108.302 min:0 max:434 sd:48.0757 dtype:DTYPE_FLOAT64\n",
       "\t22: &quot;V28&quot; NUMERICAL mean:219.905 min:0 max:912 sd:104.856 dtype:DTYPE_FLOAT64\n",
       "\t23: &quot;V517&quot; NUMERICAL mean:83.807 min:0 max:790 sd:118.835 dtype:DTYPE_FLOAT64\n",
       "\t24: &quot;V790&quot; NUMERICAL mean:144.694 min:11 max:686 sd:72.5368 dtype:DTYPE_FLOAT64\n",
       "\t25: &quot;V1369&quot; NUMERICAL mean:137.799 min:0 max:596 sd:98.8106 dtype:DTYPE_FLOAT64\n",
       "\t26: &quot;V158&quot; NUMERICAL mean:54.7888 min:0 max:704 sd:101.615 dtype:DTYPE_FLOAT64\n",
       "\t27: &quot;V418&quot; NUMERICAL mean:127.055 min:16 max:904 sd:55.4302 dtype:DTYPE_FLOAT64\n",
       "\t28: &quot;V1281&quot; NUMERICAL mean:18.9114 min:0 max:280 sd:15.789 dtype:DTYPE_FLOAT64\n",
       "\t29: &quot;V512&quot; NUMERICAL mean:98.2012 min:0 max:772 sd:78.9152 dtype:DTYPE_FLOAT64\n",
       "\t30: &quot;V367&quot; NUMERICAL mean:306.974 min:0 max:600 sd:62.7469 dtype:DTYPE_FLOAT64\n",
       "\t31: &quot;V195&quot; NUMERICAL mean:8.93987 min:0 max:504 sd:32.3283 dtype:DTYPE_FLOAT64\n",
       "\t32: &quot;V1571&quot; NUMERICAL mean:404.562 min:23 max:817 sd:103.4 dtype:DTYPE_FLOAT64\n",
       "\t33: &quot;V67&quot; NUMERICAL mean:68.5662 min:0 max:922 sd:128.304 dtype:DTYPE_FLOAT64\n",
       "\t34: &quot;V955&quot; NUMERICAL mean:134.335 min:0 max:727 sd:69.338 dtype:DTYPE_FLOAT64\n",
       "\t35: &quot;V1523&quot; NUMERICAL mean:100.096 min:1 max:864 sd:91.9147 dtype:DTYPE_FLOAT64\n",
       "\t36: &quot;V648&quot; NUMERICAL mean:68.8597 min:0 max:874 sd:71.6189 dtype:DTYPE_FLOAT64\n",
       "\t37: &quot;V1325&quot; NUMERICAL mean:239.85 min:13 max:800 sd:77.5818 dtype:DTYPE_FLOAT64\n",
       "\t38: &quot;V630&quot; NUMERICAL mean:489.864 min:429 max:655 sd:29.2899 dtype:DTYPE_FLOAT64\n",
       "\t39: &quot;V147&quot; NUMERICAL mean:74.86 min:6 max:689 sd:48.2236 dtype:DTYPE_FLOAT64\n",
       "\t40: &quot;V569&quot; NUMERICAL mean:427.103 min:0 max:773 sd:77.7194 dtype:DTYPE_FLOAT64\n",
       "\t41: &quot;V925&quot; NUMERICAL mean:143.522 min:7 max:429 sd:44.5555 dtype:DTYPE_FLOAT64\n",
       "\t42: &quot;V445&quot; NUMERICAL mean:273.825 min:0 max:999 sd:120.103 dtype:DTYPE_FLOAT64\n",
       "\t43: &quot;V316&quot; NUMERICAL mean:305.733 min:12 max:747 sd:86.0876 dtype:DTYPE_FLOAT64\n",
       "\t44: &quot;V625&quot; NUMERICAL mean:9.87342 min:0 max:697 sd:35.8046 dtype:DTYPE_FLOAT64\n",
       "\t45: &quot;V1181&quot; NUMERICAL mean:364.701 min:0 max:954 sd:128.446 dtype:DTYPE_FLOAT64\n",
       "\t46: &quot;V1619&quot; NUMERICAL mean:300.529 min:0 max:708 sd:58.9516 dtype:DTYPE_FLOAT64\n",
       "\t47: &quot;V913&quot; NUMERICAL mean:469.829 min:1 max:808 sd:103.402 dtype:DTYPE_FLOAT64\n",
       "\t48: &quot;V1630&quot; NUMERICAL mean:240.893 min:45 max:603 sd:61.1856 dtype:DTYPE_FLOAT64\n",
       "\t49: &quot;V650&quot; NUMERICAL mean:184.903 min:0 max:688 sd:96.8881 dtype:DTYPE_FLOAT64\n",
       "\t50: &quot;V616&quot; NUMERICAL mean:35.861 min:0 max:786 sd:92.3737 dtype:DTYPE_FLOAT64\n",
       "\t51: &quot;V374&quot; NUMERICAL mean:152.474 min:0 max:703 sd:108.749 dtype:DTYPE_FLOAT64\n",
       "\t52: &quot;V1189&quot; NUMERICAL mean:78.6355 min:0 max:593 sd:93.4225 dtype:DTYPE_FLOAT64\n",
       "\t53: &quot;V1344&quot; NUMERICAL mean:2.14504 min:0 max:294 sd:13.4267 dtype:DTYPE_FLOAT64\n",
       "\t54: &quot;V1319&quot; NUMERICAL mean:19.4246 min:0 max:732 sd:76.5741 dtype:DTYPE_FLOAT64\n",
       "\t55: &quot;V292&quot; NUMERICAL mean:452.447 min:0 max:801 sd:81.8825 dtype:DTYPE_FLOAT64\n",
       "\t56: &quot;V969&quot; NUMERICAL mean:0.53692 min:0 max:608 sd:14.4167 dtype:DTYPE_FLOAT64\n",
       "\n",
       "CATEGORICAL: 25 (30.4878%)\n",
       "\t57: &quot;V214&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t58: &quot;V1487&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t59: &quot;V245&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t60: &quot;V1301&quot; CATEGORICAL has-dict vocab-size:2 num-oods:1 (0.0263713%) most-frequent:&quot;1&quot; 3791 (99.9736%) dtype:DTYPE_BYTES\n",
       "\t61: &quot;V262&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t62: &quot;V647&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t63: &quot;V584&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t64: &quot;V688&quot; CATEGORICAL has-dict vocab-size:2 num-oods:1 (0.0263713%) most-frequent:&quot;0&quot; 3791 (99.9736%) dtype:DTYPE_BYTES\n",
       "\t65: &quot;V1048&quot; CATEGORICAL has-dict vocab-size:2 num-oods:1 (0.0263713%) most-frequent:&quot;0&quot; 3791 (99.9736%) dtype:DTYPE_BYTES\n",
       "\t66: &quot;V1010&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t67: &quot;V780&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;1&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t68: &quot;V1187&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;999&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t69: &quot;V239&quot; CATEGORICAL has-dict vocab-size:2 num-oods:1 (0.0263713%) most-frequent:&quot;0&quot; 3791 (99.9736%) dtype:DTYPE_BYTES\n",
       "\t70: &quot;V935&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;999&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t71: &quot;V1239&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t72: &quot;V833&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t73: &quot;V1256&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t74: &quot;V1356&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t75: &quot;V857&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;1&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t76: &quot;V257&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t77: &quot;V575&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;1&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t78: &quot;V478&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t79: &quot;V1240&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t80: &quot;V73&quot; CATEGORICAL has-dict vocab-size:2 zero-ood-items most-frequent:&quot;0&quot; 3792 (100%) dtype:DTYPE_BYTES\n",
       "\t81: &quot;class&quot; CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:&quot;0&quot; 1896 (50%) dtype:DTYPE_BYTES\n",
       "\n",
       "Terminology:\n",
       "\tnas: Number of non-available (i.e. missing) values.\n",
       "\tood: Out of dictionary.\n",
       "\tmanually-defined: Attribute whose type is manually defined by the user, i.e., the type was not automatically inferred.\n",
       "\ttokenized: The attribute value is obtained through tokenization.\n",
       "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
       "\tvocab-size: Number of unique values.\n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_body_feature_selector\" class=\"tab_content\"><p><a target=\"_blank\" href=\"https://ydf.readthedocs.io/en/latest/glossary#feature_selection\">Feature selection</a> automatical identify and remove unnecessary input features of the model.</p><p>The plots below show the score, number of features, and metric values for each iteration of the feature selection process.  The orange vertical line marks the highest score; which defines the features are used in the final model. The table below gives the detailed logs.</p><p><b>Note:</b>The feature selection logs are accessible programmatically in python with `model.feature_selection_logs()`. The selected features are accessible programmatically in python with `model.input_feature_names()`</p><p><b>Plots</b></p><div style='display: grid; gap: 0px; grid-auto-columns: min-content;'><div style='grid-row:1 / span 1; grid-column:1 / span 1;'><script src='https://www.gstatic.com/external_hosted/plotly/plotly.min.js'></script>\n",
       "<div id=\"chart_57d7_96f5_1cb9_4452feature_selector_item0\" style=\"display: inline-block;\" ></div>\n",
       "<script>\n",
       "  Plotly.newPlot(\n",
       "    'chart_57d7_96f5_1cb9_4452feature_selector_item0',\n",
       "    [{\n",
       "x: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62],\n",
       "y: [-0.551959,-0.546692,-0.549135,-0.549242,-0.546712,-0.546394,-0.547161,-0.543218,-0.544313,-0.54376,-0.541083,-0.541494,-0.542251,-0.538332,-0.53685,-0.534009,-0.530249,-0.528642,-0.526555,-0.524635,-0.523595,-0.520246,-0.516373,-0.514395,-0.513139,-0.515482,-0.512282,-0.512737,-0.510276,-0.507455,-0.511792,-0.510255,-0.508708,-0.509896,-0.507959,-0.517889,-0.510443,-0.514292,-0.518338,-0.515307,-0.511005,-0.517346,-0.512426,-0.51368,-0.515758,-0.513439,-0.51819,-0.516085,-0.514716,-0.519566,-0.527063,-0.541045,-0.534168,-0.535473,-0.535162,-0.549058,-0.551671,-0.567408,-0.58834,-0.611016,-0.654282,-0.759617,-1.82378],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "{\n",
       "x: [29,29],\n",
       "y: [-1.95541,-0.375822],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "],\n",
       "    {\n",
       "      width: 600,\n",
       "      height: 400,\n",
       "      title: '',\n",
       "      showlegend: false,\n",
       "      xaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'iteration',\n",
       "        },\n",
       "      font: {\n",
       "        size: 10,\n",
       "        },\n",
       "      yaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'score',\n",
       "        },\n",
       "      margin: {\n",
       "        l: 50,\n",
       "        r: 50,\n",
       "        b: 50,\n",
       "        t: 50,\n",
       "      },\n",
       "    },\n",
       "    {\n",
       "      modeBarButtonsToRemove: ['sendDataToCloud'],\n",
       "      displaylogo: false,displayModeBar: false,\n",
       "    }\n",
       "  );\n",
       "</script>\n",
       "</div><div style='grid-row:1 / span 1; grid-column:2 / span 1;'>\n",
       "<div id=\"chart_57d7_96f5_1cb9_4452feature_selector_item1\" style=\"display: inline-block;\" ></div>\n",
       "<script>\n",
       "  Plotly.newPlot(\n",
       "    'chart_57d7_96f5_1cb9_4452feature_selector_item1',\n",
       "    [{\n",
       "x: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62],\n",
       "y: [1636,1473,1326,1194,1075,968,872,785,707,637,574,517,466,420,378,341,307,277,250,225,203,183,165,149,135,122,110,99,90,81,73,66,60,54,49,45,41,37,34,31,28,26,24,22,20,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "{\n",
       "x: [29,29],\n",
       "y: [-162.5,1799.5],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "],\n",
       "    {\n",
       "      width: 600,\n",
       "      height: 400,\n",
       "      title: '',\n",
       "      showlegend: false,\n",
       "      xaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'iteration',\n",
       "        },\n",
       "      font: {\n",
       "        size: 10,\n",
       "        },\n",
       "      yaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'number of features',\n",
       "        },\n",
       "      margin: {\n",
       "        l: 50,\n",
       "        r: 50,\n",
       "        b: 50,\n",
       "        t: 50,\n",
       "      },\n",
       "    },\n",
       "    {\n",
       "      modeBarButtonsToRemove: ['sendDataToCloud'],\n",
       "      displaylogo: false,displayModeBar: false,\n",
       "    }\n",
       "  );\n",
       "</script>\n",
       "</div><div style='grid-row:2 / span 1; grid-column:1 / span 1;'>\n",
       "<div id=\"chart_57d7_96f5_1cb9_4452feature_selector_item2\" style=\"display: inline-block;\" ></div>\n",
       "<script>\n",
       "  Plotly.newPlot(\n",
       "    'chart_57d7_96f5_1cb9_4452feature_selector_item2',\n",
       "    [{\n",
       "x: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62],\n",
       "y: [0.724156,0.731013,0.727321,0.733386,0.729958,0.731804,0.727321,0.731804,0.730749,0.733122,0.73365,0.735496,0.738924,0.745253,0.74288,0.742089,0.740243,0.742089,0.743143,0.745253,0.746044,0.750264,0.753956,0.74789,0.750791,0.751055,0.756329,0.747099,0.753692,0.753956,0.751582,0.753428,0.747363,0.753692,0.753165,0.756065,0.755274,0.751846,0.750791,0.748154,0.753165,0.750791,0.748945,0.751055,0.750791,0.750527,0.744726,0.750791,0.752901,0.751582,0.751582,0.746308,0.74789,0.746835,0.737342,0.734441,0.719673,0.719409,0.70385,0.705169,0.696466,0.673787,0.621572],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "{\n",
       "x: [29,29],\n",
       "y: [0.608096,0.769805],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "],\n",
       "    {\n",
       "      width: 600,\n",
       "      height: 400,\n",
       "      title: '',\n",
       "      showlegend: false,\n",
       "      xaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'iteration',\n",
       "        },\n",
       "      font: {\n",
       "        size: 10,\n",
       "        },\n",
       "      yaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'accuracy',\n",
       "        },\n",
       "      margin: {\n",
       "        l: 50,\n",
       "        r: 50,\n",
       "        b: 50,\n",
       "        t: 50,\n",
       "      },\n",
       "    },\n",
       "    {\n",
       "      modeBarButtonsToRemove: ['sendDataToCloud'],\n",
       "      displaylogo: false,displayModeBar: false,\n",
       "    }\n",
       "  );\n",
       "</script>\n",
       "</div><div style='grid-row:2 / span 1; grid-column:2 / span 1;'>\n",
       "<div id=\"chart_57d7_96f5_1cb9_4452feature_selector_item3\" style=\"display: inline-block;\" ></div>\n",
       "<script>\n",
       "  Plotly.newPlot(\n",
       "    'chart_57d7_96f5_1cb9_4452feature_selector_item3',\n",
       "    [{\n",
       "x: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62],\n",
       "y: [0.551959,0.546692,0.549135,0.549242,0.546712,0.546394,0.547161,0.543218,0.544313,0.54376,0.541083,0.541494,0.542251,0.538332,0.53685,0.534009,0.530249,0.528642,0.526555,0.524635,0.523595,0.520246,0.516373,0.514395,0.513139,0.515482,0.512282,0.512737,0.510276,0.507455,0.511792,0.510255,0.508708,0.509896,0.507959,0.517889,0.510443,0.514292,0.518338,0.515307,0.511005,0.517346,0.512426,0.51368,0.515758,0.513439,0.51819,0.516085,0.514716,0.519566,0.527063,0.541045,0.534168,0.535473,0.535162,0.549058,0.551671,0.567408,0.58834,0.611016,0.654282,0.759617,1.82378],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "{\n",
       "x: [29,29],\n",
       "y: [0.375822,1.95541],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "],\n",
       "    {\n",
       "      width: 600,\n",
       "      height: 400,\n",
       "      title: '',\n",
       "      showlegend: false,\n",
       "      xaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'iteration',\n",
       "        },\n",
       "      font: {\n",
       "        size: 10,\n",
       "        },\n",
       "      yaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'loss',\n",
       "        },\n",
       "      margin: {\n",
       "        l: 50,\n",
       "        r: 50,\n",
       "        b: 50,\n",
       "        t: 50,\n",
       "      },\n",
       "    },\n",
       "    {\n",
       "      modeBarButtonsToRemove: ['sendDataToCloud'],\n",
       "      displaylogo: false,displayModeBar: false,\n",
       "    }\n",
       "  );\n",
       "</script>\n",
       "</div><div style='grid-row:3 / span 1; grid-column:1 / span 1;'>\n",
       "<div id=\"chart_57d7_96f5_1cb9_4452feature_selector_item4\" style=\"display: inline-block;\" ></div>\n",
       "<script>\n",
       "  Plotly.newPlot(\n",
       "    'chart_57d7_96f5_1cb9_4452feature_selector_item4',\n",
       "    [{\n",
       "x: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62],\n",
       "y: [3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792,3792],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "{\n",
       "x: [29,29],\n",
       "y: [3792,3792],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "],\n",
       "    {\n",
       "      width: 600,\n",
       "      height: 400,\n",
       "      title: '',\n",
       "      showlegend: false,\n",
       "      xaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'iteration',\n",
       "        },\n",
       "      font: {\n",
       "        size: 10,\n",
       "        },\n",
       "      yaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'num_examples_weighted',\n",
       "        },\n",
       "      margin: {\n",
       "        l: 50,\n",
       "        r: 50,\n",
       "        b: 50,\n",
       "        t: 50,\n",
       "      },\n",
       "    },\n",
       "    {\n",
       "      modeBarButtonsToRemove: ['sendDataToCloud'],\n",
       "      displaylogo: false,displayModeBar: false,\n",
       "    }\n",
       "  );\n",
       "</script>\n",
       "</div></div><p><b>Table</b></p><table class=\"ydf_tuning_table\"><tr><th>Iteration</th><th>Score</th><th>Metrics</th><th>Num features</th><th>Features</th></tr><tr><td>0</td><td>-0.551959</td><td> accuracy:0.724156 loss:0.551959 num_examples_weighted:3792</td><td>1636</td><td>V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95 V96 V97 V98 V99 V100 V101 V102 V103 V104 V105 V106 V107 V108 V109 V110 V111 V112 V113 V114 V115 V116 V117 V118 V119 V120 V121 V122 V123 V124 V125 V126 V127 V128 V129 V130 V131 V132 V133 V134 V135 V136 V137 V138 V139 V140 V141 V142 V143 V144 V145 V146 V147 V148 V149 V150 V151 V152 V153 V154 V155 V156 V157 V158 V159 V160 V161 V162 V163 V164 V165 V166 V167 V168 V169 V170 V171 V172 V173 V174 V175 V176 V177 V178 V179 V180 V181 V182 V183 V184 V185 V186 V187 V188 V189 V190 V191 V192 V193 V194 V195 V196 V197 V198 V199 V200 V201 V202 V203 V204 V205 V206 V207 V208 V209 V210 V211 V212 V213 V214 V215 V216 V217 V218 V219 V220 V221 V222 V223 V224 V225 V226 V227 V228 V229 V230 V231 V232 V233 V234 V235 V236 V237 V238 V239 V240 V241 V242 V243 V244 V245 V246 V247 V248 V249 V250 V251 V252 V253 V254 V255 V256 V257 V258 V259 V260 V261 V262 V263 V264 V265 V266 V267 V268 V269 V270 V271 V272 V273 V274 V275 V276 V277 V278 V279 V280 V281 V282 V283 V284 V285 V286 V287 V288 V289 V290 V291 V292 V293 V294 V295 V296 V297 V298 V299 V300 V301 V302 V303 V304 V305 V306 V307 V308 V309 V310 V311 V312 V313 V314 V315 V316 V317 V318 V319 V320 V321 V322 V323 V324 V325 V326 V327 V328 V329 V330 V331 V332 V333 V334 V335 V336 V337 V338 V339 V340 V341 V342 V343 V344 V345 V346 V347 V348 V349 V350 V351 V352 V353 V354 V355 V356 V357 V358 V359 V360 V361 V362 V363 V364 V365 V366 V367 V368 V369 V370 V371 V372 V373 V374 V375 V376 V377 V378 V379 V380 V381 V382 V383 V384 V385 V386 V387 V388 V389 V390 V391 V392 V393 V394 V395 V396 V397 V398 V399 V400 V401 V402 V403 V404 V405 V406 V407 V408 V409 V410 V411 V412 V413 V414 V415 V416 V417 V418 V419 V420 V421 V422 V423 V424 V425 V426 V427 V428 V429 V430 V431 V432 V433 V434 V435 V436 V437 V438 V439 V440 V441 V442 V443 V444 V445 V446 V447 V448 V449 V450 V451 V452 V453 V454 V455 V456 V457 V458 V459 V460 V461 V462 V463 V464 V465 V466 V467 V468 V469 V470 V471 V472 V473 V474 V475 V476 V477 V478 V479 V480 V481 V482 V483 V484 V485 V486 V487 V488 V489 V490 V491 V492 V493 V494 V495 V496 V497 V498 V499 V500 V501 V502 V503 V504 V505 V506 V507 V508 V509 V510 V511 V512 V513 V514 V515 V516 V517 V518 V519 V520 V521 V522 V523 V524 V525 V526 V527 V528 V529 V530 V531 V532 V533 V534 V535 V536 V537 V538 V539 V540 V541 V542 V543 V544 V545 V546 V547 V548 V549 V550 V551 V552 V553 V554 V555 V556 V557 V558 V559 V560 V561 V562 V563 V564 V565 V566 V567 V568 V569 V570 V571 V572 V573 V574 V575 V576 V577 V578 V579 V580 V581 V582 V583 V584 V585 V586 V587 V588 V589 V590 V591 V592 V593 V594 V595 V596 V597 V598 V599 V600 V601 V602 V603 V604 V605 V606 V607 V608 V609 V610 V611 V612 V613 V614 V615 V616 V617 V618 V619 V620 V621 V622 V623 V624 V625 V626 V627 V628 V629 V630 V631 V632 V633 V634 V635 V636 V637 V638 V639 V640 V641 V642 V643 V644 V645 V646 V647 V648 V649 V650 V651 V652 V653 V654 V655 V656 V657 V658 V659 V660 V661 V662 V663 V664 V665 V666 V667 V668 V669 V670 V671 V672 V673 V674 V675 V676 V677 V678 V679 V680 V681 V682 V683 V684 V685 V686 V687 V688 V689 V690 V691 V692 V693 V694 V695 V696 V697 V698 V699 V700 V701 V702 V703 V704 V705 V706 V707 V708 V709 V710 V711 V712 V713 V714 V715 V716 V717 V718 V719 V720 V721 V722 V723 V724 V725 V726 V727 V728 V729 V730 V731 V732 V733 V734 V735 V736 V737 V738 V739 V740 V741 V742 V743 V744 V745 V746 V747 V748 V749 V750 V751 V752 V753 V754 V755 V756 V757 V758 V759 V760 V761 V762 V763 V764 V765 V766 V767 V768 V769 V770 V771 V772 V773 V774 V775 V776 V777 V778 V779 V780 V781 V782 V783 V784 V785 V786 V787 V788 V789 V790 V791 V792 V793 V794 V795 V796 V797 V798 V799 V800 V801 V802 V803 V804 V805 V806 V807 V808 V809 V810 V811 V812 V813 V814 V815 V816 V817 V818 V819 V820 V821 V822 V823 V824 V825 V826 V827 V828 V829 V830 V831 V832 V833 V834 V835 V836 V837 V838 V839 V840 V841 V842 V843 V844 V845 V846 V847 V848 V849 V850 V851 V852 V853 V854 V855 V856 V857 V858 V859 V860 V861 V862 V863 V864 V865 V866 V867 V868 V869 V870 V871 V872 V873 V874 V875 V876 V877 V878 V879 V880 V881 V882 V883 V884 V885 V886 V887 V888 V889 V890 V891 V892 V893 V894 V895 V896 V897 V898 V899 V900 V901 V902 V903 V904 V905 V906 V907 V908 V909 V910 V911 V912 V913 V914 V915 V916 V917 V918 V919 V920 V921 V922 V923 V924 V925 V926 V927 V928 V929 V930 V931 V932 V933 V934 V935 V936 V937 V938 V939 V940 V941 V942 V943 V944 V945 V946 V947 V948 V949 V950 V951 V952 V953 V954 V955 V956 V957 V958 V959 V960 V961 V962 V963 V964 V965 V966 V967 V968 V969 V970 V971 V972 V973 V974 V975 V976 V977 V978 V979 V980 V981 V982 V983 V984 V985 V986 V987 V988 V989 V990 V991 V992 V993 V994 V995 V996 V997 V998 V999 V1000 V1001 V1002 V1003 V1004 V1005 V1006 V1007 V1008 V1009 V1010 V1011 V1012 V1013 V1014 V1015 V1016 V1017 V1018 V1019 V1020 V1021 V1022 V1023 V1024 V1025 V1026 V1027 V1028 V1029 V1030 V1031 V1032 V1033 V1034 V1035 V1036 V1037 V1038 V1039 V1040 V1041 V1042 V1043 V1044 V1045 V1046 V1047 V1048 V1049 V1050 V1051 V1052 V1053 V1054 V1055 V1056 V1057 V1058 V1059 V1060 V1061 V1062 V1063 V1064 V1065 V1066 V1067 V1068 V1069 V1070 V1071 V1072 V1073 V1074 V1075 V1076 V1077 V1078 V1079 V1080 V1081 V1082 V1083 V1084 V1085 V1086 V1087 V1088 V1089 V1090 V1091 V1092 V1093 V1094 V1095 V1096 V1097 V1098 V1099 V1100 V1101 V1102 V1103 V1104 V1105 V1106 V1107 V1108 V1109 V1110 V1111 V1112 V1113 V1114 V1115 V1116 V1117 V1118 V1119 V1120 V1121 V1122 V1123 V1124 V1125 V1126 V1127 V1128 V1129 V1130 V1131 V1132 V1133 V1134 V1135 V1136 V1137 V1138 V1139 V1140 V1141 V1142 V1143 V1144 V1145 V1146 V1147 V1148 V1149 V1150 V1151 V1152 V1153 V1154 V1155 V1156 V1157 V1158 V1159 V1160 V1161 V1162 V1163 V1164 V1165 V1166 V1167 V1168 V1169 V1170 V1171 V1172 V1173 V1174 V1175 V1176 V1177 V1178 V1179 V1180 V1181 V1182 V1183 V1184 V1185 V1186 V1187 V1188 V1189 V1190 V1191 V1192 V1193 V1194 V1195 V1196 V1197 V1198 V1199 V1200 V1201 V1202 V1203 V1204 V1205 V1206 V1207 V1208 V1209 V1210 V1211 V1212 V1213 V1214 V1215 V1216 V1217 V1218 V1219 V1220 V1221 V1222 V1223 V1224 V1225 V1226 V1227 V1228 V1229 V1230 V1231 V1232 V1233 V1234 V1235 V1236 V1237 V1238 V1239 V1240 V1241 V1242 V1243 V1244 V1245 V1246 V1247 V1248 V1249 V1250 V1251 V1252 V1253 V1254 V1255 V1256 V1257 V1258 V1259 V1260 V1261 V1262 V1263 V1264 V1265 V1266 V1267 V1268 V1269 V1270 V1271 V1272 V1273 V1274 V1275 V1276 V1277 V1278 V1279 V1280 V1281 V1282 V1283 V1284 V1285 V1286 V1287 V1288 V1289 V1290 V1291 V1292 V1293 V1294 V1295 V1296 V1297 V1298 V1299 V1300 V1301 V1302 V1303 V1304 V1305 V1306 V1307 V1308 V1309 V1310 V1311 V1312 V1313 V1314 V1315 V1316 V1317 V1318 V1319 V1320 V1321 V1322 V1323 V1324 V1325 V1326 V1327 V1328 V1329 V1330 V1331 V1332 V1333 V1334 V1335 V1336 V1337 V1338 V1339 V1340 V1341 V1342 V1343 V1344 V1345 V1346 V1347 V1348 V1349 V1350 V1351 V1352 V1353 V1354 V1355 V1356 V1357 V1358 V1359 V1360 V1361 V1362 V1363 V1364 V1365 V1366 V1367 V1368 V1369 V1370 V1371 V1372 V1373 V1374 V1375 V1376 V1377 V1378 V1379 V1380 V1381 V1382 V1383 V1384 V1385 V1386 V1387 V1388 V1389 V1390 V1391 V1392 V1393 V1394 V1395 V1396 V1397 V1398 V1399 V1400 V1401 V1402 V1403 V1404 V1405 V1406 V1407 V1408 V1409 V1410 V1411 V1412 V1413 V1414 V1415 V1416 V1417 V1418 V1419 V1420 V1421 V1422 V1423 V1424 V1425 V1426 V1427 V1428 V1429 V1430 V1431 V1432 V1433 V1434 V1435 V1436 V1437 V1438 V1439 V1440 V1441 V1442 V1443 V1444 V1445 V1446 V1447 V1448 V1449 V1450 V1451 V1452 V1453 V1454 V1455 V1456 V1457 V1458 V1459 V1460 V1461 V1462 V1463 V1464 V1465 V1466 V1467 V1468 V1469 V1470 V1471 V1472 V1473 V1474 V1475 V1476 V1477 V1478 V1479 V1480 V1481 V1482 V1483 V1484 V1485 V1486 V1487 V1488 V1489 V1490 V1491 V1492 V1493 V1494 V1495 V1496 V1497 V1498 V1499 V1500 V1501 V1502 V1503 V1504 V1505 V1506 V1507 V1508 V1509 V1510 V1511 V1512 V1513 V1514 V1515 V1516 V1517 V1518 V1519 V1520 V1521 V1522 V1523 V1524 V1525 V1526 V1527 V1528 V1529 V1530 V1531 V1532 V1533 V1534 V1535 V1536 V1537 V1538 V1539 V1540 V1541 V1542 V1543 V1544 V1545 V1546 V1547 V1548 V1549 V1550 V1551 V1552 V1553 V1554 V1555 V1556 V1557 V1558 V1559 V1560 V1561 V1562 V1563 V1564 V1565 V1566 V1567 V1568 V1569 V1570 V1571 V1572 V1573 V1574 V1575 V1576 V1577 V1578 V1579 V1580 V1581 V1582 V1583 V1584 V1585 V1586 V1587 V1588 V1589 V1590 V1591 V1592 V1593 V1594 V1595 V1596 V1597 V1598 V1599 V1600 V1601 V1602 V1603 V1604 V1605 V1606 V1607 V1608 V1609 V1610 V1611 V1612 V1613 V1614 V1615 V1616 V1617 V1618 V1619 V1620 V1621 V1622 V1623 V1624 V1625 V1626 V1627 V1628 V1629 V1630 V1631 V1632 V1633 V1634 V1635 V1636</td></tr><tr><td>1</td><td>-0.546692</td><td> accuracy:0.731013 loss:0.546692 num_examples_weighted:3792</td><td>1473</td><td>V1191 V871 V775 V717 V512 V1461 V1283 V1485 V908 V673 V1199 V1571 V1226 V467 V605 V1099 V1110 V1349 V464 V574 V1181 V729 V83 V1343 V1451 V1140 V408 V489 V1365 V508 V874 V1235 V530 V1411 V1195 V1141 V1159 V440 V1020 V993 V1263 V146 V190 V180 V507 V270 V1190 V590 V580 V1351 V261 V387 V273 V702 V21 V700 V821 V1220 V1071 V204 V160 V849 V1369 V1146 V1266 V1338 V433 V890 V1375 V1467 V1297 V1508 V516 V840 V224 V608 V1383 V1298 V692 V80 V1612 V1090 V272 V655 V1068 V1317 V342 V1175 V758 V1601 V1138 V149 V870 V832 V405 V1279 V1479 V1599 V938 V1209 V1546 V1406 V1363 V710 V1435 V154 V535 V579 V177 V701 V686 V1347 V962 V1401 V865 V1464 V1033 V1399 V975 V227 V994 V394 V450 V1257 V1069 V446 V1558 V1143 V925 V315 V1523 V3 V592 V1025 V899 V1094 V1409 V613 V1000 V456 V999 V57 V602 V355 V637 V943 V67 V505 V1207 V867 V488 V402 V1003 V913 V1084 V611 V992 V1268 V1294 V612 V1042 V980 V970 V331 V1498 V1313 V955 V1397 V158 V1269 V14 V374 V1497 V1182 V276 V784 V844 V502 V1543 V1590 V138 V714 V862 V1513 V186 V242 V662 V691 V212 V1149 V1205 V945 V1468 V764 V265 V468 V187 V324 V1164 V419 V1081 V399 V1008 V37 V1314 V425 V777 V889 V1292 V65 V918 V727 V829 V1620 V747 V661 V412 V1223 V370 V1050 V25 V308 V1165 V474 V1222 V292 V1325 V1073 V1101 V1506 V1499 V1357 V350 V839 V1227 V249 V534 V1183 V878 V1065 V106 V35 V5 V1470 V1144 V1490 V459 V98 V585 V708 V1483 V318 V1630 V167 V1554 V944 V26 V1582 V1447 V552 V810 V420 V1139 V676 V101 V1324 V622 V1107 V1 V1286 V1112 V1632 V545 V1354 V339 V887 V129 V807 V531 V1460 V415 V228 V1163 V1262 V1566 V274 V1032 V441 V916 V485 V1130 V1431 V640 V549 V623 V826 V1037 V280 V912 V135 V1471 V1252 V1210 V1521 V1211 V1091 V1563 V964 V940 V1018 V171 V559 V1315 V1564 V675 V202 V1436 V362 V22 V143 V1206 V812 V809 V963 V23 V553 V383 V298 V852 V176 V503 V537 V1440 V1475 V1381 V838 V79 V131 V385 V1171 V1500 V1078 V1542 V1634 V56 V1339 V444 V367 V1525 V750 V853 V1528 V47 V82 V411 V316 V321 V38 V1287 V1355 V1114 V1135 V77 V1492 V1119 V1402 V357 V207 V197 V1605 V811 V1242 V538 V1570 V18 V1039 V430 V371 V1395 V497 V490 V950 V779 V562 V785 V206 V495 V1318 V1264 V761 V141 V360 V1060 V1173 V191 V252 V97 V1002 V786 V1376 V1463 V529 V233 V1428 V632 V1567 V934 V1281 V429 V1540 V1426 V607 V243 V946 V544 V556 V20 V1046 V1115 V1478 V705 V1413 V1118 V462 V428 V134 V576 V1113 V527 V1265 V1345 V959 V1200 V1515 V651 V40 V59 V259 V363 V417 V745 V1412 V1225 V395 V884 V856 V334 V256 V658 V1455 V589 V1481 V615 V1636 V1093 V542 V815 V937 V299 V1458 V501 V1024 V1125 V330 V1253 V957 V136 V979 V48 V359 V365 V1614 V323 V739 V744 V1017 V630 V1403 V1341 V814 V70 V1293 V142 V401 V721 V1352 V805 V515 V213 V162 V336 V1215 V1606 V1231 V649 V923 V222 V1108 V953 V587 V396 V513 V947 V772 V698 V882 V901 V1036 V1238 V885 V1416 V1064 V1541 V220 V891 V1176 V1417 V888 V1261 V168 V42 V961 V517 V201 V1077 V1259 V87 V560 V343 V626 V541 V1043 V1055 V1552 V837 V850 V1009 V96 V1117 V1530 V1466 V1334 V1560 V930 V1086 V656 V900 V719 V1270 V1255 V1305 V484 V311 V1021 V1453 V275 V290 V287 V681 V1327 V520 V619 V905 V480 V555 V1172 V1282 V1527 V84 V1597 V1244 V1026 V19 V927 V1602 V1155 V1531 V217 V571 V266 V766 V1208 V1162 V1538 V895 V1385 V300 V904 V1396 V879 V932 V668 V1142 V390 V1544 V208 V93 V768 V344 V1177 V827 V60 V663 V974 V211 V581 V1059 V1121 V911 V1126 V781 V378 V828 V510 V823 V1346 V1504 V1198 V492 V491 V928 V426 V806 V1350 V1537 V1276 V1610 V109 V1384 V137 V1095 V1386 V92 V620 V669 V1044 V1388 V1193 V1083 V1330 V561 V981 V634 V1374 V1484 V452 V877 V720 V765 V802 V1166 V10 V277 V1067 V29 V1448 V748 V1622 V573 V41 V711 V1326 V641 V296 V790 V1035 V616 V258 V121 V1358 V1288 V120 V328 V627 V1576 V200 V1619 V150 V931 V31 V1511 V757 V53 V449 V652 V800 V288 V914 V845 V151 V690 V352 V671 V1514 V687 V892 V907 V883 V1221 V696 V110 V822 V1074 V1290 V184 V1407 V519 V1029 V1342 V1180 V697 V416 V1429 V902 V1595 V1445 V52 V771 V1437 V320 V310 V1547 V448 V548 V326 V1063 V1056 V1548 V1555 V1123 V400 V317 V1040 V1062 V966 V808 V1534 V1450 V816 V987 V174 V526 V1072 V102 V332 V1382 V1517 V600 V1001 V783 V547 V1486 V471 V380 V1272 V346 V597 V740 V1100 V817 V105 V76 V699 V438 V1557 V237 V28 V123 V289 V341 V797 V873 V1323 V348 V163 V1082 V1477 V521 V63 V1418 V1192 V851 V147 V1553 V1285 V133 V393 V1134 V413 V89 V682 V543 V1505 V1611 V896 V1241 V284 V564 V235 V372 V1105 V13 V924 V1353 V388 V99 V166 V694 V1578 V1373 V1127 V1179 V1551 V139 V715 V210 V1432 V996 V1372 V199 V414 V1405 V1635 V479 V46 V1196 V1367 V684 V1549 V893 V1245 V629 V170 V49 V1174 V1291 V558 V267 V639 V554 V66 V1607 V1316 V1120 V1390 V1524 V588 V1308 V423 V1503 V54 V737 V216 V982 V1410 V128 V1016 V801 V776 V618 V546 V1495 V647 V478 V1256 V1301 V214 V1260 V1577 V685 V688 V156 V935 V499 V496 V113 V1048 V951 V1356 V1010 V833 V239 V780 V1229 V857 V73 V262 V584 V1586 V245 V969 V575 V1487 V1239 V741 V1005 V1240 V1187 V257 V1160 V1415 V1587 V132 V933 V1150 V1387 V506 V1106 V769 V1023 V16 V599 V338 V1310 V958 V24 V532 V347 V1013 V818 V898 V1137 V454 V1047 V965 V1512 V175 V1608 V803 V566 V1236 V434 V929 V565 V1080 V1344 V169 V1438 V983 V58 V1476 V1414 V596 V297 V763 V460 V726 V1145 V646 V422 V340 V689 V435 V305 V236 V1061 V325 V735 V1568 V909 V268 V536 V1188 V1331 V1189 V386 V988 V598 V1583 V333 V799 V621 V88 V424 V1617 V1328 V509 V1309 V591 V586 V219 V1109 V303 V1128 V767 V1131 V642 V606 V118 V1158 V1006 V1430 V364 V625 V1096 V1319 V644 V1621 V358 V1581 V44 V941 V159 V525 V1366 V743 V463 V1168 V789 V1439 V1201 V279 V68 V667 V366 V1153 V973 V173 V195 V182 V880 V1392 V1295 V1588 V377 V221 V1593 V361 V869 V1280 V942 V1618 V1336 V189 V293 V403 V798 V126 V1184 V122 V353 V917 V1161 V205 V986 V373 V514 V1482 V486 V248 V1569 V127 V949 V285 V906 V295 V1378 V1398 V472 V731 V1389 V948 V493 V1427 V286 V1111 V1615 V712 V968 V1408 V725 V836 V335 V144 V742 V1254 V1380 V457 V1028 V633 V660 V1132 V976 V1185 V349 V91 V1434 V1364 V494 V628 V1575 V33 V791 V643 V706 V1129 V314 V304 V1420 V716 V115 V483 V762 V577 V432 V872 V728 V1136 V703 V164 V255 V345 V921 V74 V936 V1550 V1623 V1539 V108 V583 V1007 V1519 V1580 V1014 V1473 V972 V1518 V36 V188 V756 V1057 V192 V796 V226 V734 V1400 V1284 V859 V670 V1443 V998 V406 V111 V437 V570 V104 V568 V1098 V178 V1277 V1572 V17 V1230 V820 V238 V356 V241 V666 V114 V307 V15 V8 V1076 V1167 V1041 V1502 V1332 V841 V1004 V926 V71 V7 V861 V1079 V6 V770 V732 V157 V1022 V842 V196 V1053 V533 V442 V1591 V498 V713 V1151 V730 V81 V854 V1556 V792 V956 V683 V939 V1085 V379 V1204 V557 V1030 V337 V198 V453 V229 V1034 V903 V1322 V1019 V1456 V397 V1628 V738 V540 V524 V473 V920 V1573 V614 V1216 V1535 V1302 V886 V1433 V185 V61 V654 V107 V283 V1274 V894 V813 V1379 V990 V788 V1329 V78 V1425 V30 V858 V1154 V677 V752 V1312 V504 V465 V247 V831 V693 V754 V595 V100 V1156 V1027 V1219 V601 V294 V1333 V511 V835 V1197 V244 V391 V1603 V1304 V94 V753 V1306 V455 V1049 V250 V1462 V327 V62 V915 V445 V1251 V1585 V1442 V1202 V1250 V1545 V443 V1598 V1058 V1054 V795 V1562 V1625 V860 V1124 V1424 V1496 V985 V528 V234 V1214 V1088 V1203 V1038 V760 V659 V1052 V375 V351 V1070 V161 V172 V1320 V1066 V825 V1170 V1103 V843 V674 V1469 V90 V301 V1444 V4 V910 V117 V1472 V819 V1488 V500 V1594 V1589 V680 V794 V604 V1147 V1391 V152 V1212 V638 V271 V476 V269 V977 V1419 V181 V1097 V470 V302 V1423 V1311 V282 V1234 V1271 V991 V85 V368 V451 V1213 V751 V447 V260 V824 V281 V1480 V749 V657 V1452 V866 V376 V1561 V1247 V1465 V469 V116 V723 V1075 V1404 V1532 V722 V45 V1494 V1507 V1526 V1616 V1249 V919 V603 V1446 V55 V653 V461 V384 V369 V1596 V664 V624 V153 V1368 V1501 V1194 V635 V1522 V1609 V1012 V746 V322 V254 V39 V551 V1340 V569 V539 V995 V1371 V381 V1631 V278 V1289 V1228 V1232 V1278 V27 V215 V567 V1604 V309 V863 V578 V1300 V679 V1459 V648 V971 V1520 V1296 V165 V1224 V1348 V155 V1237 V1510 V967 V572 V1449 V1045 V523 V1267 V787 V1102 V225 V1491 V1454 V124 V1152 V650 V439 V1559 V960 V1051 V1233 V313 V707 V130 V1579 V550 V240 V11 V1536 V1248 V609 V418 V319 V1613 V1104 V1565 V1574</td></tr><tr><td>2</td><td>-0.549135</td><td> accuracy:0.727321 loss:0.549135 num_examples_weighted:3792</td><td>1326</td><td>V1191 V1283 V1571 V1226 V871 V512 V908 V702 V10 V405 V1511 V273 V1020 V1199 V673 V261 V226 V176 V832 V467 V717 V730 V1485 V775 V1468 V1090 V574 V365 V456 V188 V590 V1411 V1340 V1099 V1351 V530 V250 V577 V913 V1207 V149 V1630 V1263 V928 V1279 V940 V1195 V1292 V686 V1478 V535 V783 V779 V507 V808 V1114 V821 V911 V1543 V849 V1276 V1119 V1149 V1449 V80 V428 V460 V1570 V391 V367 V602 V690 V936 V983 V438 V1079 V67 V330 V1519 V721 V1349 V1623 V342 V207 V353 V18 V1311 V679 V1244 V991 V727 V551 V943 V691 V1375 V885 V569 V1294 V317 V116 V46 V825 V1369 V402 V644 V105 V1182 V749 V994 V1409 V666 V204 V300 V816 V1505 V25 V753 V454 V1144 V957 V1472 V158 V1152 V439 V1166 V182 V1082 V1030 V434 V1043 V1013 V375 V161 V90 V542 V705 V534 V608 V1281 V489 V30 V1500 V333 V1071 V70 V975 V1033 V1450 V165 V1140 V1350 V692 V515 V92 V887 V160 V394 V138 V1200 V137 V74 V1286 V1606 V152 V1130 V1379 V772 V1077 V1553 V1055 V620 V1185 V52 V1432 V1305 V37 V669 V1486 V573 V1247 V1538 V1266 V28 V784 V1083 V531 V595 V108 V1124 V763 V1470 V468 V1177 V66 V637 V916 V980 V228 V1112 V970 V664 V710 V1345 V842 V865 V1274 V937 V962 V968 V1162 V380 V1459 V1444 V54 V222 V1616 V1297 V68 V1069 V1436 V78 V1598 V1225 V1471 V817 V1399 V1056 V355 V760 V1381 V788 V6 V1132 V1039 V1428 V84 V444 V494 V294 V433 V1388 V358 V1552 V861 V642 V613 V972 V795 V1104 V719 V1442 V1196 V269 V862 V1558 V1417 V221 V1211 V886 V905 V227 V869 V720 V1165 V322 V319 V213 V604 V1223 V205 V1407 V1277 V766 V1391 V71 V648 V939 V513 V901 V102 V674 V1192 V65 V1625 V912 V1565 V35 V401 V399 V740 V362 V657 V552 V378 V191 V1597 V856 V1086 V1151 V1006 V792 V1108 V646 V1049 V1331 V93 V708 V592 V225 V1504 V661 V1050 V550 V1117 V1365 V320 V1322 V1147 V1380 V1156 V124 V1327 V622 V995 V1110 V1213 V1188 V1078 V94 V462 V1106 V129 V450 V331 V898 V1314 V128 V1027 V397 V1161 V1068 V313 V465 V1537 V829 V1230 V451 V547 V1103 V1363 V1201 V630 V1467 V919 V1262 V1329 V986 V459 V197 V806 V1212 V562 V1058 V1026 V629 V824 V903 V1137 V917 V268 V1473 V464 V463 V189 V190 V208 V1368 V1502 V153 V127 V1145 V1621 V527 V949 V891 V540 V1172 V877 V976 V1523 V1168 V738 V1017 V958 V1237 V1232 V361 V283 V1554 V1203 V175 V1508 V1197 V558 V580 V472 V1522 V449 V155 V1325 V316 V1517 V696 V276 V282 V564 V880 V185 V309 V616 V280 V368 V63 V948 V628 V851 V418 V1000 V785 V349 V698 V1227 V600 V99 V768 V1620 V946 V1544 V1113 V237 V1219 V295 V345 V440 V1575 V818 V1115 V839 V836 V1423 V324 V1425 V284 V638 V1107 V336 V1389 V765 V352 V554 V746 V699 V1102 V240 V787 V904 V307 V104 V1238 V803 V606 V1578 V1539 V1222 V609 V561 V109 V1446 V374 V514 V1343 V171 V243 V1477 V1167 V1430 V1251 V417 V249 V1269 V14 V697 V11 V426 V1022 V178 V1204 V623 V301 V1053 V826 V1135 V490 V229 V517 V83 V1602 V579 V757 V485 V304 V447 V549 V420 V683 V97 V509 V1252 V1008 V383 V132 V339 V1272 V947 V1180 V1573 V873 V924 V660 V1476 V511 V133 V838 V1583 V614 V844 V700 V114 V707 V1390 V1018 V639 V1373 V930 V525 V973 V254 V1295 V147 V1582 V157 V1480 V1091 V1310 V725 V748 V1095 V1259 V1481 V961 V445 V480 V1611 V332 V756 V275 V87 V1332 V1245 V38 V236 V1323 V889 V1123 V668 V110 V1497 V859 V1098 V1183 V633 V801 V1494 V271 V126 V455 V971 V1265 V217 V278 V1402 V1159 V925 V136 V1418 V658 V357 V1595 V453 V1534 V667 V347 V843 V1593 V1267 V181 V807 V45 V430 V1317 V5 V372 V715 V1622 V1483 V578 V1054 V747 V505 V712 V289 V1440 V895 V1029 V56 V1427 V726 V424 V219 V419 V281 V1254 V31 V343 V479 V820 V1061 V411 V794 V932 V1397 V1613 V701 V538 V828 V559 V1572 V676 V7 V443 V533 V202 V929 V974 V1139 V1021 V1318 V723 V1386 V1313 V815 V894 V1434 V813 V1184 V670 V1342 V1414 V555 V13 V1040 V1202 V529 V735 V544 V504 V1105 V82 V1067 V388 V926 V1042 V1574 V649 V599 V1532 V1003 V739 V503 V1231 V1009 V49 V1128 V1284 V302 V1035 V187 V296 V1590 V1496 V883 V1290 V348 V425 V1460 V44 V1129 V588 V1492 V1475 V1326 V1126 V568 V931 V706 V581 V934 V1576 V27 V143 V241 V1579 V412 V1181 V548 V866 V321 V1465 V1120 V537 V1012 V1447 V1315 V910 V166 V1376 V1047 V423 V652 V1193 V33 V144 V180 V754 V1443 V377 V491 V267 V1607 V519 V1559 V1566 V819 V201 V914 V625 V1285 V557 V252 V1416 V1452 V15 V452 V328 V483 V1549 V1498 V1316 V1367 V1455 V81 V1309 V1528 V1066 V134 V1060 V1404 V716 V474 V1618 V376 V1512 V58 V996 V1346 V1490 V1358 V314 V340 V603 V341 V1556 V172 V184 V100 V1293 V526 V39 V923 V1438 V938 V1298 V694 V879 V1445 V163 V1524 V1353 V618 V601 V170 V79 V1194 V1356 V951 V1010 V1301 V833 V214 V239 V1048 V496 V499 V935 V156 V688 V685 V1577 V1260 V1256 V478 V546 V647 V1495 V1229 V1160 V257 V1319 V1187 V1240 V1005 V741 V1239 V1487 V575 V245 V780 V857 V1586 V584 V262 V73 V290 V159 V1484 V435 V1234 V117 V1118 V1521 V1415 V1535 V835 V19 V244 V1051 V1412 V1555 V761 V200 V414 V1491 V687 V1339 V822 V1514 V335 V1632 V1312 V272 V1546 V1205 V121 V915 V192 V713 V1488 V956 V1158 V1037 V900 V1001 V89 V969 V22 V536 V1466 V524 V326 V1328 V1175 V1374 V448 V21 V1453 V1093 V500 V1562 V906 V1636 V1109 V565 V286 V1548 V987 V677 V918 V752 V566 V1216 V626 V350 V777 V416 V346 V1567 V814 V1257 V1072 V812 V1302 V1531 V24 V88 V446 V1063 V373 V29 V653 V1426 V1603 V1291 V1619 V303 V1041 V953 V429 V442 V1111 V1096 V945 V258 V742 V964 V933 V722 V1100 V1004 V151 V769 V1392 V234 V587 V363 V627 V20 V615 V1429 V663 V523 V641 V154 V789 V1150 V1016 V1059 V1288 V611 V1025 V1215 V703 V1551 V215 V1410 V406 V369 V798 V77 V1617 V510 V800 V1002 V1419 V501 V854 V693 V476 V1609 V1282 V259 V1131 V248 V173 V1499 V781 V413 V1179 V287 V1564 V982 V1628 V1550 V174 V659 V1594 V998 V845 V1568 V1236 V57 V162 V277 V1396 V36 V91 V874 V1515 V1174 V381 V1057 V797 V654 V714 V1224 V220 V1 V1383 V1344 V1408 V198 V256 V130 V1241 V400 V488 V1581 V106 V1045 V1173 V1228 V1501 V711 V432 V210 V1355 V387 V743 V366 V396 V457 V1270 V1134 V776 V292 V384 V643 V1334 V1028 V1525 V1541 V371 V1439 V990 V585 V41 V337 V732 V553 V1176 V684 V233 V840 V1604 V1634 V1385 V167 V1019 V841 V1580 V1146 V570 V539 V624 V1464 V596 V1127 V1153 V1506 V206 V1635 V810 V802 V675 V1448 V764 V1250 V993 V920 V1433 V1154 V1599 V979 V1560 V955 V16 V1052 V1268 V195 V1007 V344 V1248 V560 V896 V1462 V150 V408 V745 V1208 V53 V858 V390 V999 V1064 V1164 V235 V120 V942 V1387 V1507 V650 V541 V111 V471 V1382 V3 V469 V927 V486 V1330 V62 V799 V662 V325 V872 V1605 V621 V266 V1435 V1557 V1596 V1210 V1014 V744 V607 V211 V260 V1357 V1403 V59 V131 V113 V385 V586 V805 V1189 V60 V1527 V863 V882 V288 V360 V632 V1261 V1155 V1561 V1088 V671 V323 V827 V1289 V521 V1333 V1255 V651 V327 V790 V1437 V1406 V770 V884 V164 V791 V1138 V422 V1125 V853 V598 V1458 V1198 V359 V619 V247 V907 V495 V981 V899 V1454 V634 V1400 V878 V61 V1084 V656 V729 V270 V1023 V1085 V279 V532 V543 V305 V17 V831 V1463 V977 V1235 V123 V1591 V516 V497 V168 V23 V42 V1364 V850 V293 V1136 V334 V1036 V583 V308 V572 V1520 V1513 V1024 V1280 V796 V591 V1253 V612 V1341 V318 V1264 V1398 V1220 V351 V728 V635 V461 V767 V837 V364 V122 V941 V1163 V135 V1372 V370 V1320 V1062 V985 V734 V101 V567 V502 V403 V1608 V1540 V1070 V238 V1206 V1378 V55 V216 V47 V959 V26 V199 V473 V274 V1209 V852 V680 V1563 V1456 V1143 V1451 V1366 V1221 V1610 V1271 V589 V1503 V311 V1545 V992 V1214 V1076 V1384 V556 V1278 V196 V76 V255 V85 V682</td></tr><tr><td>3</td><td>-0.549242</td><td> accuracy:0.733386 loss:0.549242 num_examples_weighted:3792</td><td>1194</td><td>V512 V1199 V871 V1571 V673 V580 V1191 V679 V1411 V1226 V226 V1485 V190 V717 V1283 V74 V97 V464 V1576 V1020 V261 V702 V273 V643 V1351 V1552 V10 V687 V1068 V574 V840 V956 V746 V1159 V497 V349 V753 V180 V821 V428 V1033 V250 V1099 V405 V46 V1620 V1288 V1320 V542 V1043 V151 V430 V660 V940 V898 V1369 V559 V620 V970 V513 V994 V456 V1175 V1548 V1117 V1110 V24 V553 V365 V1202 V1286 V849 V176 V1140 V337 V879 V287 V1027 V204 V1538 V381 V1499 V862 V614 V1185 V784 V913 V783 V394 V1532 V524 V577 V439 V67 V296 V797 V863 V721 V1512 V1004 V911 V399 V983 V197 V968 V1380 V663 V1616 V1450 V914 V41 V1330 V705 V552 V1007 V1398 V1366 V930 V889 V1458 V1311 V374 V800 V1113 V939 V1 V808 V569 V227 V1263 V1365 V1017 V652 V370 V164 V1062 V37 V467 V1367 V433 V28 V1397 V1207 V1103 V1523 V1067 V818 V158 V1553 V896 V440 V1179 V1410 V1297 V1341 V1294 V385 V1118 V1364 V1419 V790 V1326 V877 V1325 V1392 V134 V1279 V986 V1084 V38 V122 V6 V767 V426 V1221 V917 V998 V1262 V1605 V173 V91 V85 V894 V671 V572 V420 V1155 V1079 V507 V920 V729 V275 V912 V14 V5 V927 V732 V489 V1349 V1293 V1315 V152 V339 V1241 V1076 V727 V1000 V1524 V33 V511 V1459 V345 V1013 V1389 V219 V434 V715 V358 V1077 V412 V63 V585 V906 V163 V206 V1582 V1460 V1355 V137 V979 V1124 V320 V1447 V686 V202 V130 V918 V943 V925 V856 V1340 V448 V271 V1102 V509 V390 V955 V490 V462 V447 V904 V1544 V739 V1456 V54 V42 V1204 V1511 V1057 V529 V406 V1302 V116 V1396 V1448 V903 V1312 V94 V999 V1219 V495 V1543 V1130 V153 V1109 V1519 V1636 V1205 V550 V1449 V1230 V1621 V1041 V278 V1613 V1149 V1515 V910 V1572 V624 V57 V551 V1055 V314 V579 V135 V1134 V1372 V66 V286 V664 V1430 V99 V1591 V1590 V249 V1201 V351 V25 V1266 V228 V627 V915 V1429 V1313 V623 V342 V869 V650 V305 V1323 V90 V1162 V765 V1445 V1551 V1603 V49 V1138 V1166 V192 V220 V1404 V658 V1593 V159 V519 V639 V114 V1465 V1045 V313 V1253 V1382 V596 V1427 V991 V282 V429 V505 V188 V1061 V1546 V1212 V1399 V418 V1066 V187 V1111 V1453 V674 V745 V1318 V993 V1192 V323 V859 V445 V289 V7 V1188 V748 V1570 V178 V586 V161 V1535 V1545 V348 V82 V1035 V36 V1137 V1090 V819 V615 V1491 V938 V882 V598 V861 V1223 V20 V587 V942 V52 V716 V1254 V622 V817 V932 V1250 V1610 V1173 V1281 V747 V1085 V1327 V1608 V1051 V236 V3 V740 V1566 V1560 V1578 V1180 V131 V962 V905 V618 V1127 V703 V742 V1196 V1156 V744 V106 V1451 V1611 V807 V1332 V1125 V332 V1058 V1210 V1415 V1470 V1368 V81 V21 V450 V292 V1161 V813 V1508 V564 V225 V544 V84 V23 V352 V959 V796 V68 V776 V573 V653 V1402 V1414 V1255 V928 V1492 V666 V79 V841 V975 V1037 V80 V133 V1280 V266 V414 V121 V602 V836 V1225 V1528 V629 V661 V259 V171 V1622 V1328 V707 V1143 V719 V11 V1497 V1534 V1412 V1635 V1480 V317 V1168 V1194 V680 V1222 V1426 V1193 V77 V205 V517 V1500 V516 V333 V1047 V1231 V768 V476 V357 V308 V677 V866 V1634 V557 V534 V387 V233 V1008 V1454 V805 V367 V1086 V503 V71 V1182 V324 V1484 V791 V578 V109 V766 V1522 V195 V480 V1213 V514 V1623 V972 V100 V104 V373 V738 V473 V949 V600 V1425 V698 V294 V555 V801 V1009 V126 V1088 V668 V485 V384 V886 V1378 V16 V210 V592 V1022 V27 V35 V1606 V654 V1236 V170 V1434 V908 V260 V523 V56 V1565 V1023 V160 V111 V941 V1248 V973 V438 V241 V601 V127 V355 V316 V1473 V589 V1559 V371 V1599 V400 V907 V710 V1537 V865 V1154 V463 V1375 V213 V491 V1581 V1557 V1472 V1177 V711 V996 V526 V1021 V981 V763 V958 V1483 V30 V453 V432 V1520 V1289 V1276 V611 V606 V321 V670 V411 V443 V1488 V1344 V222 V1235 V237 V157 V346 V532 V1501 V1106 V59 V344 V1628 V252 V378 V676 V1517 V769 V1423 V1119 V216 V752 V1132 V561 V89 V361 V632 V1385 V951 V1334 V258 V1619 V124 V683 V852 V815 V166 V1618 V1018 V472 V630 V706 V566 V657 V279 V154 V1063 V1071 V1442 V1464 V798 V527 V799 V149 V609 V1317 V123 V558 V1540 V1550 V435 V1556 V635 V304 V335 V1006 V812 V756 V1381 V822 V1197 V887 V1295 V1408 V1053 V971 V488 V621 V659 V1271 V1496 V309 V761 V147 V1112 V802 V1607 V105 V1224 V402 V691 V901 V1316 V1036 V295 V307 V616 V359 V857 V1301 V1586 V1356 V584 V214 V262 V73 V341 V969 V1010 V1239 V1005 V1187 V1319 V257 V741 V1160 V1229 V1495 V647 V546 V478 V1256 V1260 V833 V1487 V575 V1577 V688 V156 V935 V245 V499 V1048 V239 V780 V1240 V850 V1494 V1583 V1095 V496 V200 V120 V685 V1129 V1561 V1452 V916 V696 V388 V1216 V1227 V65 V1298 V1416 V1568 V1014 V377 V483 V536 V1567 V633 V604 V937 V1146 V1432 V1026 V608 V743 V872 V560 V1625 V1083 V375 V1562 V1598 V637 V828 V1632 V1531 V44 V61 V1513 V726 V19 V543 V591 V1208 V883 V961 V1105 V269 V1209 V1211 V985 V588 V699 V1153 V1563 V311 V595 V461 V749 V1463 V217 V1443 V457 V1136 V779 V1220 V1617 V424 V1147 V207 V1042 V1406 V143 V47 V486 V301 V343 V1481 V1064 V858 V234 V1558 V1477 V570 V1002 V788 V276 V1446 V360 V1150 V669 V929 V1507 V1357 V899 V1163 V1504 V1214 V1438 V1602 V1345 V1278 V129 V1462 V1232 V1343 V1579 V243 V1237 V254 V1282 V1580 V185 V1346 V690 V58 V1374 V238 V1476 V1203 V599 V182 V987 V1274 V174 V293 V700 V1176 V281 V540 V208 V425 V1251 V26 V92 V191 V1292 V1261 V1437 V634 V1115 V413 V175 V403 V1234 V1541 V244 V723 V325 V318 V535 V1514 V1001 V829 V1376 V725 V70 V290 V845 V814 V1455 V1167 V502 V1151 V539 V60 V277 V1409 V18 V816 V1238 V946 V1144 V322 V533 V13 V1183 V772 V675 V1564 V1291 V1391 V844 V720 V924 V562 V449 V1152 V641 V945 V372 V1502 V184 V181 V1342 V144 V1358 V240 V547 V150 V300 V1549 V1436 V93 V1114 V459 V284 V1268 V878 V215 V397 V1172 V792 V885 V853 V501 V347 V995 V474 V851 V1024 V789 V1056 V380 V1265 V854 V283 V693 V416 V760 V1025 V268 V777 V825 V1407 V713 V1016 V331 V31 V820 V165 V607 V990 V274 V78 V1339 V1322 V1174 V644 V1104 V1468 V1019 V1440 V401 V255 V364 V280 V948 V556 V29 V612 V900 V1575 V764 V1252 V1333 V419 V1189 V1403 V1503 V423 V442 V842 V1247 V1596 V515 V39 V714 V302 V1475 V1527 V396 V1126 V168 V667 V1269 V1244 V1135 V1467 V1082 V1498 V827 V648 V196 V824 V62 V1272 V1052 V102 V469 V1259 V256 V735 V1206 V873 V1096 V843 V521 V541 V1466 V172 V162 V108 V754 V1039 V982 V603 V1131 V504 V1072 V1433 V692 V1270 V1267 V454 V1195 V1128 V408 V221 V638 V128 V803 V87 V368 V1054 V1573 V1277 V810 V1574 V934 V510 V722 V880 V933 V1609 V1521 V1435 V625 V110 V1390 V728 V422 V1471 V500 V628 V1059 V895 V974 V247 V590 V451 V583 V336 V1373 V931 V734 V1040 V694 V839 V1123 V1383 V1285 V363 V1049 V1417 V565 V795 V1098 V22 V1028 V554 V465 V1091 V1630 V1379 V1331 V1215 V1108 V138 V1257 V235 V189 V113 V1388 V229 V957 V947 V891 V369 V567 V549 V1439 V953 V45 V460 V319 V1012 V211 V1525 V697 V494 V353 V272 V1181 V417 V781 V531 V831 V646 V327 V1093 V267 V1284</td></tr><tr><td>4</td><td>-0.546712</td><td> accuracy:0.729958 loss:0.546712 num_examples_weighted:3792</td><td>1075</td><td>V1283 V512 V1191 V1199 V702 V1485 V1226 V717 V405 V574 V673 V1068 V1110 V1343 V10 V1411 V871 V1043 V180 V261 V1571 V1079 V746 V1280 V602 V176 V821 V973 V614 V908 V52 V1263 V336 V158 V925 V784 V377 V1000 V940 V1114 V226 V1404 V999 V149 V650 V378 V1570 V80 V1223 V898 V556 V33 V1468 V994 V1262 V1294 V663 V1182 V998 V1071 V1519 V349 V190 V1477 V513 V1192 V332 V1090 V632 V1033 V307 V1491 V529 V664 V1574 V916 V707 V1575 V1269 V1470 V1351 V318 V234 V1281 V1365 V1380 V668 V90 V531 V473 V824 V1261 V853 V1456 V1027 V790 V120 V188 V1266 V993 V779 V489 V418 V607 V67 V1563 V459 V913 V808 V161 V1378 V42 V907 V1322 V783 V355 V372 V1203 V1325 V1331 V879 V28 V609 V886 V622 V1415 V1523 V1026 V949 V831 V339 V342 V666 V1500 V39 V92 V1438 V1636 V456 V1195 V905 V676 V1 V738 V147 V820 V721 V904 V235 V716 V1219 V494 V844 V789 V606 V505 V634 V1099 V1548 V1520 V991 V1613 V928 V1397 V910 V822 V37 V895 V430 V536 V453 V705 V162 V19 V151 V627 V1328 V1460 V1250 V428 V313 V1332 V1553 V394 V1590 V1537 V637 V480 V1512 V490 V813 V105 V123 V677 V365 V1430 V1349 V740 V941 V94 V1376 V1392 V1382 V918 V343 V621 V601 V1137 V113 V959 V970 V240 V1289 V943 V449 V1383 V1144 V920 V417 V1119 V719 V514 V1552 V1016 V1504 V1272 V1532 V752 V694 V1205 V814 V862 V715 V260 V1255 V1206 V1608 V24 V1389 V358 V620 V1402 V856 V533 V810 V630 V397 V1534 V1573 V882 V652 V641 V1528 V939 V1451 V106 V1619 V714 V219 V1317 V1366 V121 V1617 V1159 V585 V7 V433 V1177 V27 V1201 V744 V1503 V1257 V384 V570 V559 V1396 V247 V1082 V654 V222 V859 V595 V1333 V1154 V54 V46 V674 V62 V399 V1368 V648 V753 V11 V1109 V534 V396 V134 V962 V465 V658 V1267 V416 V327 V472 V1134 V1220 V36 V945 V1162 V615 V524 V725 V463 V839 V1259 V504 V99 V272 V1052 V165 V583 V660 V58 V1616 V720 V946 V486 V995 V983 V1064 V515 V1174 V1630 V1449 V586 V827 V1327 V325 V324 V1096 V241 V502 V1234 V450 V143 V1426 V1055 V138 V1515 V573 V1488 V948 V1316 V1062 V587 V723 V1149 V237 V1185 V1454 V659 V1412 V172 V35 V1004 V812 V1222 V1270 V1567 V1151 V693 V31 V974 V947 V803 V1462 V249 V629 V1399 V931 V63 V1344 V281 V1221 V1357 V1450 V929 V558 V1047 V1123 V1213 V1540 V1297 V13 V1102 V560 V671 V266 V1628 V448 V1543 V82 V173 V1556 V698 V975 V1409 V739 V442 V1163 V553 V1499 V440 V982 V972 V1334 V911 V425 V129 V1024 V1146 V825 V413 V1012 V1330 V236 V1035 V316 V1085 V1193 V1355 V554 V25 V735 V749 V667 V1135 V1497 V934 V917 V1481 V1582 V971 V309 V1557 V572 V766 V411 V268 V1467 V1093 V65 V1168 V256 V1358 V304 V589 V1189 V454 V1564 V569 V1525 V768 V703 V873 V604 V1632 V1225 V818 V135 V657 V406 V661 V174 V305 V208 V1202 V1086 V451 V124 V1173 V357 V591 V400 V1323 V816 V87 V555 V402 V277 V56 V1618 V866 V1385 V126 V1568 V903 V942 V955 V429 V439 V250 V1458 V961 V900 V77 V566 V1436 V1167 V796 V985 V1175 V287 V896 V144 V348 V1207 V1580 V1254 V205 V748 V1443 V743 V275 V599 V1379 V184 V401 V274 V1419 V164 V1623 V517 V1611 V608 V1111 V1381 V1326 V93 V798 V1535 V883 V761 V1572 V322 V842 V1088 V1311 V540 V801 V510 V580 V1284 V1427 V215 V1445 V1484 V345 V685 V1138 V509 V109 V1147 V284 V802 V1546 V228 V457 V1161 V1286 V527 V958 V872 V66 V178 V623 V335 V1061 V1565 V906 V836 V1271 V1408 V1560 V278 V927 V843 V252 V1118 V367 V1176 V221 V1364 V445 V799 V1227 V1022 V924 V200 V20 V756 V271 V1562 V1581 V300 V187 V1028 V938 V727 V1018 V628 V1439 V1204 V1210 V1232 V1180 V930 V374 V1433 V1124 V1398 V1126 V1603 V153 V1440 V699 V1598 V653 V375 V1549 V1230 V1112 V1432 V491 V935 V1240 V780 V588 V496 V546 V239 V478 V1048 V499 V245 V1577 V1256 V156 V1260 V833 V688 V257 V1487 V575 V1301 V1187 V1005 V1239 V1010 V969 V73 V262 V214 V584 V1356 V1586 V857 V741 V1160 V1229 V1495 V647 V159 V295 V61 V899 V1423 V1136 V302 V1375 V1635 V319 V217 V403 V805 V1550 V567 V388 V488 V1593 V1244 V435 V791 V1236 V1054 V390 V1224 V1108 V1155 V3 V1056 V170 V495 V85 V1274 V346 V160 V1372 V1312 V1517 V296 V612 V901 V1156 V1279 V423 V616 V1507 V1609 V706 V128 V841 V133 V497 V1023 V547 V321 V1551 V987 V227 V1319 V38 V1446 V1416 V686 V154 V341 V381 V578 V692 V1105 V317 V196 V49 V1252 V1277 V1414 V1566 V1607 V1583 V114 V767 V1132 V57 V1559 V361 V368 V690 V1514 V1434 V116 V1442 V202 V865 V211 V1063 V311 V254 V817 V592 V1513 V1057 V1008 V800 V1208 V216 V426 V1591 V754 V878 V1040 V207 V819 V1345 V1181 V1058 V229 V1049 V1544 V1367 V603 V951 V1125 V1037 V1480 V611 V829 V635 V532 V1214 V932 V1045 V1374 V1248 V1494 V255 V711 V877 V1131 V1036 V1006 V1216 V370 V852 V1459 V914 V788 V1621 V887 V1019 V1388 V1098 V171 V781 V894 V1153 V364 V424 V1302 V108 V915 V544 V726 V854 V1051 V618 V323 V1391 V1209 V734 V511 V1483 V373 V1041 V669 V957 V760 V1183 V1014 V1143 V175 V1127 V352 V1464 V1072 V1291 V1059 V91 V742 V168 V1524 V443 V244 V1502 V1083 V1561 V182 V697 V1002 V420 V643 V1390 V1241 V1471 V1320 V337 V795 V1521 V150 V45 V1407 V1373 V280 V71 V1538 V1313 V1268 V320 V469 V1403 V1475 V986 V258 V163 V1276 V851 V675 V1634 V289 V1452 V347 V683 V1455 V412 V639 V279 V562 V192 V408 V815 V638 V259 V371 V44 V680 V1346 V1188 V290 V14 V541 V314 V732 V519 V710 V1596 V937 V1472 V189 V1465 V282 V414 V474 V60 V1278 V1527 V1215 V220 V1622 V1453 V102 V772 V1578 V891 V100 V434 V1211 V462 V1558 V797 V225 V933 V1339 V807 V765 V89 V1541 V59 V293 V1113 V1179 V722 V1130 V500 V1066 V1315 V1172 V828 V625 V122 V700 V646 V747 V1197 V360 V432 V1508 V308 V111 V1435 V858 V1150 V385 V292 V845 V1194 V543 V1235 V861 V1437 V29 V1103 V5 V1531 V1579 V1053 V869 V301 V152 V110 V1017 V501 V380 V1605 V526 V1021 V579 V968 V1196 V70 V1129 V777 V564 V23 V213 V419 V78 V776 V238 V521 V953 V1425 V363 V696 V850 V127 V1369 V137 V516 V26 V539 V1013 V549 V1293 V598 V476 V1410 V1084 V1251 V561 V1295 V1042 V670 V1417 V130 V233 V18 V990 V1318 V1077 V1463 V353 V206 V485 V1128 V1106 V1476 V1466 V792 V131 V195 V1545 V769 V1247 V1298 V191 V889 V1285 V687 V1473 V104 V1292 V243 V713 V1253</td></tr><tr><td>5</td><td>-0.546394</td><td> accuracy:0.731804 loss:0.546394 num_examples_weighted:3792</td><td>968</td><td>V1199 V1283 V512 V1191 V580 V673 V1485 V1099 V1226 V261 V1311 V226 V1033 V871 V24 V1043 V940 V1090 V574 V1292 V717 V602 V1468 V784 V1068 V1411 V67 V1593 V1110 V190 V1113 V363 V1129 V1149 V556 V973 V10 V324 V149 V1297 V821 V1125 V1138 V634 V914 V908 V955 V1456 V394 V1266 V250 V1071 V1630 V372 V1345 V49 V767 V90 V1349 V161 V706 V46 V925 V620 V585 V1279 V337 V282 V28 V453 V206 V108 V1355 V1159 V790 V702 V1553 V1568 V663 V727 V569 V1276 V1369 V1373 V1430 V700 V433 V1179 V505 V604 V1616 V220 V120 V686 V71 V630 V495 V842 V1062 V635 V514 V249 V1459 V836 V1313 V1523 V714 V480 V489 V123 V1571 V1123 V195 V1162 V274 V416 V1014 V1458 V1608 V284 V1582 V1023 V385 V1016 V314 V396 V1144 V342 V555 V358 V1111 V1181 V1515 V189 V521 V1257 V1417 V490 V180 V534 V42 V1203 V666 V1543 V1623 V1548 V443 V1472 V428 V1396 V1367 V991 V38 V1502 V1180 V377 V1464 V3 V1375 V122 V316 V536 V664 V519 V259 V26 V106 V1538 V627 V1619 V1057 V1552 V1477 V827 V236 V406 V705 V1012 V886 V1284 V813 V1263 V1204 V1167 V1598 V33 V364 V500 V1216 V779 V899 V539 V1343 V559 V255 V150 V595 V1618 V1286 V561 V896 V121 V82 V713 V260 V1209 V348 V765 V485 V744 V133 V710 V1154 V986 V1392 V573 V176 V1414 V439 V1427 V196 V457 V858 V1155 V1573 V1331 V456 V126 V277 V707 V946 V1559 V281 V910 V1192 V900 V116 V158 V1408 V562 V1462 V302 V1439 V623 V1460 V1325 V1470 V1378 V307 V423 V1114 V974 V1054 V1103 V266 V1590 V777 V91 V799 V1130 V674 V20 V373 V414 V1364 V1188 V309 V472 V1433 V234 V1445 V346 V513 V752 V349 V1544 V163 V1252 V1376 V1390 V831 V187 V933 V1322 V734 V225 V1207 V233 V1027 V465 V153 V1504 V995 V789 V488 V1450 V182 V1327 V553 V1077 V1253 V1213 V614 V1423 V295 V814 V374 V237 V608 V353 V629 V653 V970 V510 V697 V365 V397 V1036 V14 V1021 V1106 V1182 V70 V1018 V151 V244 V1269 V917 V1438 V990 V929 V268 V815 V805 V213 V92 V1295 V766 V1561 V1471 V221 V462 V1476 V1545 V692 V1491 V486 V287 V300 V434 V124 V985 V1591 V1443 V927 V37 V114 V527 V659 V252 V1579 V615 V958 V1521 V228 V943 V1499 V504 V23 V1507 V418 V1022 V1365 V1368 V796 V987 V172 V693 V491 V1328 V669 V305 V211 V134 V816 V450 V1234 V219 V810 V1136 V1605 V311 V1581 V272 V1366 V1088 V1024 V586 V77 V1389 V352 V1135 V1517 V1334 V229 V240 V1131 V357 V654 V1271 V1236 V949 V698 V517 V1146 V1202 V143 V1119 V1609 V652 V622 V1534 V612 V1227 V1055 V1223 V639 V685 V1247 V339 V417 V1220 V918 V915 V426 V1124 V726 V390 V1109 V1189 V445 V873 V1449 V572 V400 V1143 V1173 V241 V646 V658 V942 V1002 V1613 V999 V824 V403 V1632 V1316 V547 V411 V52 V625 V637 V1147 V841 V451 V1082 V367 V412 V1118 V911 V293 V845 V865 V1635 V1041 V138 V554 V1483 V852 V1267 V1407 V1437 V144 V85 V18 V420 V159 V711 V137 V589 V898 V63 V1475 V435 V191 V128 V994 V102 V1013 V1557 V1318 V772 V1293 V1281 V1580 V803 V1177 V941 V208 V560 V104 V1358 V405 V1575 V1261 V1040 V1583 V87 V361 V820 V1037 V532 V110 V696 V1280 V1250 V1440 V1064 V650 V1072 V425 V540 V1432 V275 V748 V1232 V1272 V879 V791 V1480 V57 V722 V1042 V934 V256 V853 V1035 V903 V1051 V916 V894 V1503 V1176 V598 V607 V318 V1219 V1108 V1546 V583 V895 V1319 V1150 V44 V741 V524 V1434 V343 V660 V723 V1128 V1564 V99 V747 V566 V154 V812 V1172 V1323 V822 V675 V817 V907 V200 V1066 V1488 V1404 V1312 V1551 V56 V1225 V238 V360 V440 V388 V738 V1549 V951 V1194 V643 V1005 V833 V780 V1239 V1187 V1260 V156 V1301 V575 V1487 V1256 V1577 V245 V1048 V478 V935 V1240 V239 V546 V257 V688 V588 V969 V73 V262 V214 V584 V1356 V1586 V857 V1160 V1229 V1495 V647 V1010 V322 V1215 V27 V754 V975 V1566 V1026 V1532 V801 V1061 V819 V761 V901 V499 V549 V1079 V496 V1028 V347 V317 V1508 V94 V1497 V304 V39 V920 V381 V13 V611 V1380 V1381 V65 V1412 V1333 V756 V494 V476 V1112 V1607 V891 V370 V618 V825 V1435 V296 V1326 V1059 V105 V235 V1045 V227 V45 V1574 V1320 V80 V135 V1268 V957 V768 V616 V289 V1222 V587 V205 V1563 V1134 V321 V1221 V473 V1556 V1455 V732 V1494 V60 V740 V147 V301 V1374 V721 V526 V170 V878 V1346 V783 V469 V66 V130 V703 V222 V59 V1208 V1004 V1105 V807 V1351 V279 V1409 V1636 V599 V592 V1344 V184 V1512 V29 V1481 V529 V511 V160 V1098 V808 V290 V111 V869 V1085 V1274 V670 V325 V699 V308 V1254 V743 V1339 V677 V769 V541 V175 V883 V89 V419 V781 V1330 V1535 V162 V192 V851 V1332 V1163 V1019 V601 V972 V877 V579 V1550 V424 V1156 V1466 V1500 V531 V19 V109 V1244 V591 V1513 V341 V1391 V1161 V1540 V1527 V1514 V641 V720 V1399 V58 V1315 V1578 V558 V1562 V1416 V968 V327 V380 V375 V448 V1560 V1196 V1567 V1473 V993 V661 V533 V1634 V543 V632 V93 V368 V332 V165 V800 V1558 V1047 V1388 V1096 V1537 V694 V378 V216 V683 V449 V1102 V1093 V1628 V667 V953 V948 V828 V628 V1302 V454 V648 V1185 V463 V429 V1603 V945 V61 V280 V1270 V174 V1259 V1528 V839 V1379 V1255 V1565 V715 V882 V937 V802 V1452 V931 V1224 V11 V924 V818 V1053 V1235 V843 V1372 V402 V866 V959 V913 V1006 V1525 V164 V1132 V887 V906 V1063 V5 V278 V725 V742 V1214 V178 V1086 V371 V336 V168 V676 V258 V320 V657 V1127 V319 V603 V753 V1056 V788 V982 V113 V930 V1484 V25 V1201 V1153 V1174 V31 V856 V1 V719 V1000 V567 V1467 V947 V1442 V797 V962 V1426 V1289 V1193 V1052 V928 V323 V1291 V690 V668 V1436 V100 V859 V292 V1385 V1262 V776 V1611 V36 V1596 V1183 V983 V1541 V131 V1278 V1453 V127 V609 V355 V442 V335 V35 V1248 V173 V1206 V570 V247 V188 V905 V217 V1049 V1572 V408 V872 V1151</td></tr><tr><td>6</td><td>-0.547161</td><td> accuracy:0.727321 loss:0.547161 num_examples_weighted:3792</td><td>872</td><td>V1191 V1283 V871 V1199 V261 V717 V574 V226 V176 V1226 V1181 V673 V1068 V1485 V512 V1311 V405 V580 V1532 V1571 V365 V918 V28 V614 V10 V106 V1079 V1523 V175 V1263 V1271 V348 V190 V994 V908 V489 V999 V1043 V1432 V420 V1343 V911 V456 V690 V428 V534 V3 V907 V1366 V1566 V394 V1551 V664 V1411 V1636 V1538 V26 V1476 V608 V250 V721 V1508 V686 V182 V377 V1090 V1369 V1295 V886 V973 V1130 V943 V486 V1439 V852 V488 V783 V906 V23 V514 V1556 V158 V1213 V1375 V195 V1196 V454 V1553 V1099 V1315 V180 V1201 V1325 V1281 V450 V1339 V374 V700 V465 V592 V67 V295 V1351 V1330 V70 V521 V1484 V1535 V1609 V1182 V349 V819 V1515 V1216 V100 V652 V1464 V1192 V122 V1548 V1247 V927 V527 V121 V1269 V314 V903 V1460 V646 V505 V33 V1012 V821 V1236 V414 V803 V1435 V1500 V213 V127 V1156 V529 V277 V453 V1042 V5 V1267 V1244 V1605 V189 V913 V396 V234 V1390 V602 V406 V898 V439 V411 V1206 V983 V767 V985 V42 V882 V696 V1222 V372 V553 V82 V1513 V572 V59 V641 V710 V126 V1291 V163 V433 V174 V1057 V1579 V1110 V1349 V947 V147 V1049 V583 V342 V666 V1004 V1323 V1128 V1574 V1355 V1514 V676 V316 V1358 V569 V131 V25 V109 V707 V378 V738 V134 V816 V1259 V1608 V865 V595 V1135 V1266 V1103 V178 V810 V1344 V1071 V1565 V554 V154 V327 V625 V1507 V1590 V715 V765 V658 V1318 V620 V1455 V1085 V859 V1167 V1143 V1013 V494 V472 V227 V598 V20 V31 V222 V650 V1151 V1392 V161 V367 V402 V36 V236 V1456 V375 V517 V1223 V727 V1153 V268 V1473 V1618 V495 V1132 V373 V801 V851 V370 V714 V1430 V993 V219 V1541 V615 V238 V637 V1412 V335 V18 V184 V1138 V1372 V1037 V556 V1149 V1112 V744 V1491 V235 V1527 V332 V1316 V1583 V1619 V541 V915 V1414 V1552 V1221 V1380 V1559 V1172 V1611 V925 V301 V390 V229 V683 V1417 V1385 V443 V567 V587 V1248 V1027 V659 V1578 V1280 V63 V440 V1062 V1591 V1150 V352 V39 V1061 V463 V797 V1047 V705 V162 V324 V799 V1452 V416 V946 V1123 V1494 V896 V1568 V448 V296 V579 V1437 V1632 V1278 V769 V725 V1105 V91 V1254 V814 V1396 V1234 V1481 V1279 V1471 V1564 V788 V1179 V937 V1416 V418 V102 V1364 V164 V618 V105 V510 V317 V663 V279 V311 V1581 V442 V191 V1488 V457 V1194 V1227 V1550 V1573 V941 V1557 V706 V661 V1014 V1136 V699 V827 V124 V623 V45 V1035 V160 V627 V1286 V1154 V1502 V1252 V1537 V601 V1173 V400 V266 V916 V790 V1497 V1155 V480 V322 V1399 V1072 V304 V586 V1053 V476 V667 V611 V692 V1408 V346 V1635 V1174 V910 V883 V462 V13 V293 V143 V536 V1 V371 V388 V669 V1381 V959 V539 V748 V955 V547 V1082 V172 V719 V1470 V196 V768 V648 V843 V108 V726 V1253 V368 V1176 V65 V962 V1613 V1512 V1203 V532 V1367 V951 V839 V1189 V1220 V1388 V353 V1561 V114 V1055 V1262 V150 V403 V309 V1102 V60 V71 V1322 V1368 V820 V732 V1033 V104 V361 V247 V933 V566 V723 V280 V1503 V675 V258 V948 V445 V1235 V1161 V1467 V290 V1022 V895 V1106 V573 V325 V473 V385 V900 V19 V1544 V113 V1607 V1426 V628 V995 V149 V1558 V698 V1018 V777 V740 V570 V1582 V1131 V287 V1204 V110 V44 V1534 V562 V412 V1327 V318 V616 V90 V1219 V1389 V116 V1525 V1628 V337 V958 V1328 V917 V1202 V1159 V776 V1160 V780 V833 V1239 V1229 V1495 V1005 V647 V1010 V499 V156 V1048 V262 V584 V73 V969 V688 V257 V546 V239 V1240 V935 V478 V1187 V245 V1577 V1256 V1487 V575 V1356 V1301 V1586 V1260 V857 V214 V77 V1177 V1475 V1443 V137 V942 V1284 V424 V192 V741 V1376 V49 V1438 V339 V341 V945 V1215 V1119 V233 V1086 V321 V1404 V364 V986 V828 V123 V320 V355 V813 V1274 V588 V1580 V205 V1567 V734 V1041 V423 V549 V496 V1147 V991 V323 V1292 V540 V1436 V56 V953 V1109 V800 V1270 V805 V111 V685 V38 V1026 V1098 V1442 V1391 V920 V1289 V1634 V292 V1453 V1458 V1326 V1006 V560 V66 V1332 V168 V1059 V1320 V485 V240 V357 V901 V1603 V974 V37 V1409 V1056 V1051 V1517 V856 V1575 V1545 V1002 V781 V879 V526 V603 V1224 V272 V1379 V1546 V924 V138 V220 V1093 V1549 V772 V612 V170 V1214 V491 V891 V1162 V711 V187 V381 V632 V417 V928 V1333 V1450 V252 V237 V533 V559 V1209 V1596 V1146 V1111 V1088 V753 V46 V1207 V825 V591 V1064 V1276 V779 V807 V1302 V1096 V1118 V1445 V1023 V789 V654 V1572 V52 V657 V1144 V435 V451 V1319 V796 V1250 V504 V256 V1499 V85 V1163 V842 V609 V249 V275 V1459 V972 V1028 V128 V130 V281 V668 V358 V866 V694 V165 V914 V1108 V761 V469 V1434 V622 V1562 V244 V289 V635 V1423 V905 V260 V35 V802 V1016 V639 V747 V1504 V434 V634 V408 V1124 V308 V1521 V791 V754 V630 V1268 V206 V887 V784 V561 V841 V426 V1331 V425 V133 V1630 V1466 V1407 V1180 V968 V1021 V878 V490 V815 V255 V931 V278 V1593 V1232 V987 V702 V756 V1560 V1483 V982 V1063 V1378 V307 V677 V159 V27 V975 V1346 V89 V1373 V1616 V1127 V766 V720 V1054 V228 V87 V930 V92 V845 V1598 V934 V241 V1188 V336 V742 V894 V1193 V812 V225 V1293 V29 V274 V1255 V1134 V1019 V99 V1334 V58 V200 V449 V347 V653 V1257 V173 V713 V1528 V949 V80 V1066 V693 V1374 V360 V722 V869 V957 V660 V11 V1462 V703 V543 V599 V818 V57 V120 V1272</td></tr><tr><td>7</td><td>-0.543218</td><td> accuracy:0.731804 loss:0.543218 num_examples_weighted:3792</td><td>785</td><td>V1191 V512 V1283 V1199 V1485 V261 V1226 V580 V673 V1311 V1068 V405 V702 V226 V190 V1271 V994 V871 V908 V489 V1266 V428 V973 V574 V250 V222 V1181 V1460 V176 V1552 V1553 V1630 V666 V1279 V1349 V913 V178 V1118 V1033 V70 V573 V26 V1062 V916 V534 V650 V42 V423 V783 V1252 V394 V569 V669 V790 V1513 V717 V367 V1538 V357 V1636 V1267 V630 V505 V433 V1042 V1159 V134 V692 V236 V1325 V1483 V930 V1591 V914 V417 V572 V494 V683 V900 V514 V1274 V911 V789 V450 V316 V618 V579 V59 V1006 V108 V337 V1192 V734 V1523 V180 V1619 V953 V1375 V1475 V614 V46 V348 V767 V993 V1286 V372 V162 V1456 V843 V1086 V1571 V249 V825 V1521 V869 V296 V137 V114 V521 V859 V841 V349 V1182 V1207 V1259 V1112 V906 V274 V213 V943 V1262 V1388 V887 V147 V1055 V1014 V1411 V408 V1180 V67 V1558 V28 V1111 V414 V1438 V1276 V1138 V60 V324 V1173 V1188 V1450 V1079 V130 V1430 V227 V599 V955 V1323 V1364 V896 V527 V1609 V1281 V1090 V406 V292 V766 V727 V272 V620 V1346 V457 V1372 V418 V660 V128 V368 V1412 V91 V10 V39 V536 V553 V1316 V35 V657 V229 V234 V370 V1453 V675 V335 V999 V641 V1220 V1439 V1527 V244 V583 V779 V18 V1546 V160 V301 V304 V1369 V336 V706 V1573 V13 V713 V290 V925 V562 V19 V1443 V339 V1189 V686 V124 V1385 V233 V714 V1051 V445 V556 V559 V703 V797 V442 V1464 V66 V634 V1216 V648 V1373 V705 V591 V325 V240 V100 V532 V11 V361 V1467 V639 V1119 V1399 V1150 V1234 V1534 V1109 V1214 V266 V396 V1499 V991 V1093 V928 V488 V309 V637 V720 V635 V719 V332 V1132 V260 V628 V80 V453 V45 V1219 V777 V255 V616 V625 V451 V400 V1564 V1582 V903 V23 V895 V133 V1566 V1135 V228 V420 V360 V723 V1476 V1103 V44 V886 V769 V1481 V165 V1155 V627 V1579 V1545 V192 V31 V1063 V365 V1572 V592 V1215 V595 V71 V693 V866 V89 V1085 V196 V753 V1248 V1494 V894 V652 V924 V206 V1344 V388 V1082 V1416 V882 V77 V659 V1263 V1056 V237 V1144 V173 V1634 V587 V358 V1272 V307 V1213 V252 V490 V768 V1235 V342 V308 V154 V1327 V788 V1151 V603 V747 V1177 V314 V698 V1502 V322 V1544 V424 V1224 V931 V1500 V1253 V1355 V1396 V995 V1556 V320 V1548 V819 V280 V355 V856 V1289 V1596 V947 V676 V1167 V805 V323 V1333 V1445 V1378 V1583 V1236 V143 V1047 V1172 V121 V1575 V986 V510 V541 V1390 V472 V1176 V1134 V982 V374 V126 V1128 V1389 V122 V1284 V958 V123 V440 V1072 V920 V1016 V449 V1581 V561 V1291 V456 V1368 V1269 V1204 V1322 V1254 V1334 V554 V710 V87 V1064 V1351 V1257 V57 V566 V828 V948 V1250 V82 V1562 V1193 V174 V1202 V485 V721 V781 V1 V761 V517 V694 V170 V1021 V63 V815 V37 V722 V279 V278 V496 V1331 V1049 V756 V1608 V1380 V711 V27 V85 V1131 V1632 V974 V150 V951 V917 V1535 V1549 V1611 V1244 V403 V499 V654 V791 V1027 V602 V905 V1319 V962 V439 V1613 V748 V346 V1124 V725 V898 V195 V435 V1607 V327 V796 V1586 V780 V857 V1187 V245 V239 V1577 V1260 V478 V1301 V1356 V1240 V575 V1256 V935 V1487 V647 V257 V688 V969 V73 V584 V262 V1048 V156 V1010 V214 V1005 V1495 V1229 V1239 V833 V1160 V546 V434 V1508 V469 V1559 V1541 V685 V667 V1565 V1343 V1096 V865 V945 V149 V1268 V25 V1507 V663 V754 V772 V52 V182 V1528 V661 V1635 V1002 V588 V187 V741 V287 V164 V5 V416 V539 V364 V1491 V1035 V465 V842 V1504 V1408 V200 V1466 V1057 V225 V1146 V385 V1102 V776 V1458 V381 V1567 V1442 V800 V411 V1149 V268 V1154 V1618 V646 V163 V668 V1517 V1550 V784 V707 V810 V175 V110 V611 V807 V878 V615 V1532 V1328 V1105 V1462 V560 V318 V105 V658 V533 V463 V1295 V1156 V801 V1437 V1514 V341 V321 V491 V845 V189 V985 V901 V612 V983 V598 V941 V1434 V1435 V1127 V454 V803 V1326 V378 V205 V1136 V56 V1366 V1066 V1484 V1332 V476 V1391 V1280 V1379 V765 V740 V158 V852 V390 V1473 V1012 V1432 V377 V317 V238 V883 V934 V1574 V609 V373 V601 V102 V1392 V813 V220 V1616 V549 V1153 V1209 V987 V168 V623 V1162 V127 V1161 V480 V1580 V1367 V1318 V818 V891 V29 V138 V184 V1557 V1452 V1223 V473 V1201 V347 V526 V58 V1598 V295 V1302 V448 V1404 V1255 V851 V131 V608 V277 V968 V1232 V1023 V258 V1179 V1381 V425 V570 V1227 V1525 V1426 V311 V975 V1414 V821 V1059 V241 V1512 V677 V113 V949 V275 V1221 V352 V109 V159 V653 V742 V547 V1099 V38 V289 V1320 V946 V20 V92 V247 V375 V1106 V1022 V543 V1409 V1054 V1071 V1143 V1293 V1503 V1315 V443 V1018 V726 V957 V1247 V933 V353 V1019 V1603 V1130 V504 V1605 V111 V879 V1270 V1194 V1628 V1568 V915 V1590 V1098 V1339 V235 V942 V104 V1222 V907 V1061 V1561 V120 V1037 V36 V1376 V664 V1004</td></tr><tr><td>8</td><td>-0.544313</td><td> accuracy:0.730749 loss:0.544313 num_examples_weighted:3792</td><td>707</td><td>V1283 V1191 V512 V871 V717 V1099 V908 V702 V574 V994 V1068 V673 V580 V1199 V365 V1226 V1181 V779 V666 V1571 V190 V394 V686 V175 V1042 V1485 V226 V1582 V1254 V1456 V28 V367 V31 V292 V176 V900 V1364 V1574 V405 V1411 V898 V1538 V348 V1159 V727 V790 V934 V67 V1207 V1167 V1628 V1014 V658 V342 V1276 V527 V1507 V85 V975 V463 V783 V456 V138 V165 V206 V274 V514 V378 V767 V122 V189 V660 V1464 V488 V1565 V521 V425 V625 V1223 V719 V1561 V1553 V1311 V159 V1376 V71 V765 V1351 V336 V56 V433 V573 V807 V911 V1392 V247 V1453 V1132 V1523 V1439 V134 V1532 V237 V675 V669 V723 V620 V1064 V133 V234 V608 V657 V173 V1235 V553 V1263 V1 V650 V1508 V314 V332 V734 V615 V1573 V1630 V1366 V1049 V311 V147 V1182 V491 V556 V810 V180 V539 V1150 V1556 V1373 V648 V1426 V23 V1138 V1430 V813 V111 V1396 V485 V1544 V195 V1450 V301 V236 V1590 V986 V1369 V569 V591 V235 V277 V1381 V1033 V238 V825 V1202 V517 V1517 V268 V1189 V913 V859 V1268 V178 V346 V104 V1093 V933 V1090 V1632 V113 V449 V490 V1635 V423 V182 V59 V1452 V1204 V1385 V87 V360 V1151 V439 V1071 V722 V1127 V250 V725 V1315 V789 V1316 V60 V457 V1598 V1534 V445 V80 V1372 V1027 V1271 V1481 V1500 V766 V948 V121 V109 V1066 V1355 V1063 V368 V896 V1096 V1153 V635 V327 V1255 V114 V1435 V473 V137 V1253 V504 V1156 V18 V290 V160 V634 V150 V748 V222 V5 V82 V1252 V1608 V1248 V1267 V955 V1188 V1328 V1037 V19 V187 V225 V1504 V355 V279 V1525 V1528 V945 V652 V128 V883 V249 V781 V943 V1176 V1112 V845 V123 V229 V420 V465 V1494 V726 V1272 V653 V526 V443 V272 V454 V754 V1259 V630 V1344 V1004 V1458 V668 V612 V879 V667 V352 V1119 V788 V706 V851 V289 V1173 V916 V411 V278 V168 V1012 V1291 V614 V1557 V442 V1105 V1180 V800 V1302 V1550 V974 V1266 V1192 V882 V321 V126 V654 V1215 V1111 V1059 V747 V1023 V1546 V261 V1596 V322 V973 V1269 V710 V35 V1086 V801 V1521 V58 V953 V374 V1580 V252 V1378 V266 V1035 V962 V609 V1286 V761 V611 V1002 V480 V1295 V742 V424 V1281 V1467 V692 V1257 V1018 V1136 V616 V1149 V1262 V1279 V1437 V227 V1103 V308 V1404 V91 V105 V1220 V1057 V951 V347 V869 V543 V931 V532 V1575 V1131 V164 V220 V1412 V127 V1331 V244 V886 V947 V131 V1346 V184 V891 V942 V1219 V1124 V1318 V768 V63 V390 V37 V659 V1438 V920 V856 V1194 V843 V1389 V982 V1379 V39 V957 V448 V1607 V639 V1216 V819 V1564 V1006 V987 V1319 V1476 V469 V1085 V663 V588 V1146 V958 V887 V566 V579 V494 V562 V995 V1559 V337 V280 V1325 V403 V408 V130 V1603 V575 V584 V1549 V73 V969 V499 V688 V257 V647 V1487 V935 V1256 V262 V1240 V1356 V1301 V478 V1260 V1577 V239 V245 V1586 V780 V857 V1187 V1048 V546 V1160 V833 V1239 V1229 V1495 V1005 V1010 V156 V214 V11 V637 V903 V1134 V541 V110 V983 V1434 V707 V1475 V1609 V89 V36 V1047 V772 V260 V536 V1143 V26 V924 V685 V592 V1102 V1443 V1527 V20 V1616 V852 V1491 V435 V713 V1368 V703 V1144 V1432 V646 V572 V1172 V1130 V1209 V741 V370 V154 V1193 V711 V1098 V27 V205 V1513 V192 V1054 V1636 V1289 V1473 V1535 V1055 V496 V1214 V1201 V1161 V1390 V1327 V561 V453 V418 V587 V1222 V472 V554 V1061 V1109 V476 V1388 V1545 V510 V240 V143 V1343 V1016 V10 V865 V1445 V1155 V570 V925 V77 V547 V341 V361 V878 V375 V1274 V583 V388 V1483 V1618 V968 V1177 V29 V1284 V1399 V1022 V233 V1380 V1462 V108 V1227 V258 V406 V769 V603 V714 V38 V776 V1613 V434 V791 V450 V949 V1619 V13 V158 V505 V339 V92 V694 V842 V325 V601 V1605 V821 V353 V661 V1213 V335 V917 V25 V1051 V1326 V323 V683 V1128 V906 V100 V170 V1334 V1568 V1293 V677 V304 V740 V985 V1339 V1548 V416 V1579 V756 V941 V1323 V1634 V1021 V1583 V598 V309 V1567 V1322 V52 V275 V1502 V66 V489 V1408 V1250 V316 V428 V174 V1512 V1056 V1414 V1333 V623 V1562 V1558 V296 V828 V364 V307 V928 V815 V162 V120 V1514 V805 V318 V102 V372 V1062 V1581 V1442 V1611 V440 V287 V1466 V1375 V1072 V414 V1280 V705 V1367 V196 V1541 V777 V163 V451 V1332 V1234 V213 V618 V1019 V1460 V641 V915 V417 V560 V993 V946 V1082 V358 V930 V1135 V1118 V796</td></tr><tr><td>9</td><td>-0.54376</td><td> accuracy:0.733122 loss:0.54376 num_examples_weighted:3792</td><td>637</td><td>V1191 V1283 V512 V1485 V1199 V250 V1311 V1226 V405 V1099 V1411 V190 V717 V1090 V574 V994 V1207 V1548 V1201 V10 V433 V261 V365 V226 V1351 V67 V637 V1177 V1068 V26 V673 V686 V1263 V28 V592 V394 V85 V1369 V1565 V428 V175 V108 V658 V121 V705 V625 V418 V948 V1150 V917 V583 V925 V1012 V1355 V911 V1144 V928 V1093 V777 V274 V1582 V1042 V734 V1134 V180 V490 V1475 V335 V234 V1295 V514 V1613 V364 V727 V165 V1054 V1333 V517 V1281 V442 V59 V1456 V878 V1375 V301 V1019 V1180 V355 V1103 V130 V1259 V694 V916 V287 V1316 V801 V1562 V1628 V1136 V445 V1033 V178 V898 V957 V805 V420 V661 V138 V456 V454 V962 V1286 V612 V143 V1366 V714 V1331 V1512 V807 V113 V1538 V713 V580 V240 V663 V1450 V1368 V1392 V891 V277 V646 V236 V776 V1159 V983 V213 V11 V1346 V1494 V505 V1284 V1127 V1234 V1204 V1580 V723 V235 V1571 V1502 V941 V615 V618 V346 V813 V147 V1523 V423 V480 V473 V933 V1598 V639 V77 V800 V1192 V1268 V27 V195 V1132 V20 V164 V1619 V289 V1590 V908 V1262 V105 V192 V489 V1535 V476 V1521 V1376 V71 V677 V92 V560 V825 V815 V227 V608 V378 V1532 V1453 V641 V1573 V1412 V336 V630 V309 V1063 V598 V547 V469 V1018 V1151 V1630 V206 V654 V1438 V114 V783 V1430 V1388 V1476 V1559 V1138 V747 V946 V845 V1579 V342 V1146 V949 V163 V162 V1408 V1193 V1167 V189 V457 V137 V1209 V150 V1327 V131 V494 V58 V36 V368 V603 V722 V60 V1027 V225 V601 V87 V543 V1194 V1334 V1414 V882 V789 V485 V1583 V1527 V591 V1466 V1534 V526 V1181 V1130 V1149 V521 V1014 V296 V871 V322 V1437 V1219 V19 V247 V1319 V943 V222 V1220 V372 V947 V1250 V1462 V1491 V63 V341 V275 V339 V154 V308 V1616 V931 V539 V268 V852 V123 V920 V122 V541 V1404 V406 V1276 V1086 V1564 V451 V1189 V205 V1188 V443 V1222 V173 V707 V1500 V280 V66 V318 V1182 V35 V1609 V374 V1202 V573 V561 V1112 V1381 V102 V1525 V842 V886 V819 V434 V741 V1023 V1173 V1574 V1253 V986 V1248 V1556 V89 V896 V588 V987 V414 V134 V196 V1291 V388 V569 V859 V572 V1364 V719 V616 V160 V411 V1596 V314 V304 V440 V1214 V1059 V174 V347 V496 V272 V810 V249 V417 V1056 V29 V37 V100 V424 V710 V435 V667 V360 V711 V614 V1568 V1215 V133 V1160 V127 V109 V1096 V1315 V307 V1187 V833 V1495 V1229 V1239 V857 V1577 V239 V478 V1301 V1356 V1240 V262 V1256 V245 V935 V1487 V647 V1586 V780 V214 V1005 V257 V688 V499 V969 V73 V1260 V584 V575 V1048 V546 V1010 V156 V685 V126 V1131 V924 V1507 V1098 V1339 V1618 V290 V527 V1460 V1172 V187 V1124 V1513 V869 V13 V80 V1545 V742 V1343 V968 V120 V951 V995 V772 V566 V1155 V1252 V1432 V1156 V1549 V311 V1272 V1332 V851 V1390 V706 V985 V1266 V23 V765 V1057 V510 V1326 V323 V1216 V942 V1016 V367 V1213 V692 V828 V1037 V1434 V1055 V740 V650 V570 V325 V1135 V748 V170 V1607 V1267 V416 V913 V1608 V1508 V1128 V754 V1289 V623 V110 V652 V348 V1176 V1035 V945 V1581 V1302 V439 V620 V1603 V790 V238 V168 V883 V1082 V668 V111 V358 V1557 V536 V1022 V158 V887 V353 V1389 V1553 V1483 V1372 V609 V39 V266 V1636 V1071 V1 V1541 V1279 V725 V865 V327 V879 V761 V1452 V1085 V843 V337 V982 V1445 V1439 V488 V903 V1119 V791 V1464 V1111 V556 V244 V1021 V781 V856 V1227 V1442 V1611 V453 V176 V634 V1632 V159 V703 V321 V1373 V390 V332 V1504 V1223 V1105 V352 V532 V1528 V25 V1344 V403 V975 V182 V955 V292 V184 V683 V504 V1328 V220 V1396 V726 V1380 V675 V553 V1066 V1605 V1061 V611 V91 V934 V1367 V1379 V1109 V1517 V1322 V648 V766 V1143 V18 V279 V1062 V635 V579 V31 V1072 V38 V756 V56 V1514 V450 V1325 V702 V316 V1635 V958 V1269 V1575 V1161 V233 V1051 V1064 V906 V425 V974 V5 V653 V491 V361 V128 V1443 V1004 V587</td></tr><tr><td>10</td><td>-0.541083</td><td> accuracy:0.73365 loss:0.541083 num_examples_weighted:3792</td><td>574</td><td>V512 V1283 V226 V1199 V190 V1226 V871 V717 V456 V394 V1181 V1191 V1099 V1485 V1062 V574 V1571 V348 V250 V908 V580 V1311 V405 V1411 V1369 V569 V428 V182 V1548 V560 V67 V686 V648 V261 V1286 V1351 V180 V1068 V521 V1538 V931 V138 V517 V1272 V957 V1379 V945 V367 V1096 V886 V418 V620 V1276 V374 V1090 V1259 V158 V658 V536 V1082 V1227 V913 V287 V1263 V1630 V1149 V433 V1267 V995 V28 V911 V1565 V1456 V1504 V488 V292 V1207 V532 V1582 V1159 V127 V364 V994 V296 V1209 V66 V1033 V1325 V1063 V1066 V332 V1553 V1414 V1368 V36 V1037 V1462 V1527 V318 V280 V903 V1396 V634 V955 V457 V108 V1579 V815 V316 V663 V5 V766 V87 V948 V652 V714 V941 V625 V845 V975 V195 V791 V485 V480 V748 V1373 V272 V1295 V1064 V790 V1042 V147 V1281 V1027 V539 V1494 V510 V983 V612 V133 V543 V1012 V336 V1525 V1156 V1573 V1611 V1143 V630 V1085 V1253 V986 V527 V573 V1128 V122 V526 V1051 V1328 V1332 V18 V1628 V958 V1119 V233 V789 V813 V308 V1445 V1215 V587 V1289 V236 V1513 V1528 V1366 V1213 V1291 V445 V184 V417 V164 V176 V1608 V1284 V347 V109 V856 V58 V1559 V713 V896 V131 V1438 V1523 V227 V547 V439 V150 V1466 V234 V1517 V342 V1250 V1214 V403 V323 V504 V1574 V1339 V56 V1443 V307 V390 V143 V1564 V1322 V1266 V274 V361 V1632 V205 V163 V355 V1376 V1151 V1434 V598 V268 V247 V882 V121 V170 V1460 V1192 V1022 V898 V425 V1268 V711 V865 V1279 V777 V583 V416 V1450 V346 V1583 V1176 V1344 V928 V1545 V309 V154 V1534 V1375 V592 V358 V353 V946 V781 V1613 V1018 V304 V91 V442 V1146 V450 V561 V1521 V29 V1177 V710 V341 V719 V196 V1319 V883 V1189 V828 V725 V60 V339 V325 V1223 V1061 V1605 V742 V244 V925 V27 V514 V1216 V703 V1346 V616 V1388 V110 V1161 V1452 V685 V1502 V879 V667 V1507 V100 V1204 V1464 V654 V19 V1059 V1056 V566 V1607 V1172 V1635 V705 V1103 V588 V949 V974 V647 V1048 V935 V546 V1010 V156 V575 V1586 V780 V584 V214 V1005 V1187 V239 V1577 V1301 V1356 V857 V1239 V1229 V1240 V1495 V833 V1487 V257 V688 V262 V499 V1256 V73 V1260 V245 V478 V951 V120 V969 V1131 V378 V490 V891 V570 V1316 V756 V126 V556 V11 V987 V192 V1150 V859 V1160 V869 V852 V1557 V1442 V111 V105 V1453 V1182 V1315 V368 V825 V727 V1609 V491 V301 V289 V89 V800 V917 V327 V130 V1155 V13 V25 V1023 V618 V266 V1202 V435 V1193 V843 V31 V1596 V722 V476 V423 V92 V1326 V1004 V920 V615 V496 V1111 V249 V1430 V1437 V734 V765 V614 V20 V1549 V1568 V1035 V968 V741 V1262 V1491 V608 V754 V1188 V1269 V611 V801 V1134 V159 V906 V1618 V189 V1109 V572 V424 V240 V1132 V683 V1514 V279 V1136 V314 V934 V673 V740 V420 V761 V1105 V454 V187 V916 V1590 V137 V473 V1019 V1343 V113 V541 V675 V819 V1144 V805 V637 V1535 V1054 V985 V1014 V1619 V723 V1071 V406 V1333 V1252 V1167 V178 V1135 V851 V388 V810 V59 V1112 V1173 V1130 V174 V1562 V1390 V168 V1404 V1364 V887 V162 V553 V706 V726 V776 V747 V39 V668 V26 V489 V1222 V38 V173 V1432 V933 V1580 V1 V1194 V942 V1016 V1581 V1355 V80 V1541 V1408 V77 V451 V411 V1512 V707 V1381 V623 V235 V591 V1598 V1098 V23 V469 V1331 V694 V443 V1093 V225 V1372 V650 V639 V1219 V943 V635 V123 V71 V1389 V222 V641 V982 V37 V290 V924 V609 V360 V277 V1248 V677 V220 V807 V1180 V505 V579 V1483 V1575 V842 V440 V35 V1636 V206 V1302</td></tr><tr><td>11</td><td>-0.541494</td><td> accuracy:0.735496 loss:0.541494 num_examples_weighted:3792</td><td>517</td><td>V1191 V1283 V226 V261 V190 V1485 V574 V512 V176 V1068 V673 V592 V1199 V908 V1181 V67 V1042 V1226 V886 V1571 V450 V1456 V580 V898 V489 V1369 V1062 V428 V871 V521 V488 V1548 V180 V28 V174 V1286 V433 V418 V292 V791 V1180 V983 V1590 V1192 V1262 V1582 V1268 V925 V553 V1411 V480 V1507 V652 V620 V658 V1090 V367 V1149 V1523 V308 V1167 V182 V569 V727 V650 V948 V147 V941 V36 V573 V1279 V1351 V1613 V113 V536 V1027 V1215 V159 V1291 V1033 V1450 V1269 V1430 V1263 V187 V1204 V38 V1376 V1517 V1538 V1071 V150 V126 V31 V943 V1159 V1173 V504 V456 V417 V1012 V1619 V250 V842 V906 V454 V105 V1227 V1414 V958 V637 V1368 V353 V266 V957 V59 V1177 V249 V742 V852 V309 V234 V137 V1037 V1512 V327 V91 V1388 V1553 V987 V648 V274 V1 V138 V1514 V686 V1326 V133 V1559 V1223 V945 V608 V304 V591 V1541 V220 V439 V527 V505 V332 V163 V1575 V710 V339 V1202 V1630 V1332 V29 V316 V272 V355 V734 V917 V476 V1364 V714 V71 V663 V911 V1525 V1635 V1063 V1096 V683 V233 V1373 V195 V1222 V240 V347 V89 V374 V995 V1581 V913 V616 V235 V583 V405 V425 V615 V1266 V435 V162 V80 V1333 V1443 V100 V473 V675 V1109 V1372 V416 V510 V1379 V23 V789 V726 V1035 V1016 V706 V843 V1609 V879 V20 V443 V1056 V1085 V341 V164 V667 V13 V388 V1105 V1302 V247 V539 V1209 V130 V556 V1093 V1452 V1344 V1325 V514 V800 V1636 V543 V1568 V1134 V598 V423 V630 V609 V1390 V865 V196 V951 V1319 V1135 V741 V825 V1061 V178 V928 V931 V1339 V955 V747 V1607 V1494 V1144 V1389 V325 V703 V1161 V896 V766 V1579 V517 V268 V1103 V236 V1022 V1250 V110 V1580 V566 V158 V1219 V318 V869 V490 V1437 V856 V1611 V1605 V192 V18 V66 V754 V35 V1460 V859 V39 V225 V969 V920 V92 V205 V227 V722 V1018 V314 V570 V323 V713 V748 V1528 V1143 V56 V1408 V1432 V725 V815 V765 V1193 V257 V1487 V833 V1495 V1240 V1229 V1239 V857 V1356 V239 V688 V262 V499 V1256 V73 V1260 V245 V478 V1160 V647 V685 V1301 V1048 V935 V546 V1010 V156 V575 V780 V1577 V1187 V1005 V584 V1586 V214 V805 V572 V1491 V1557 V1502 V411 V547 V109 V170 V1182 V1023 V588 V883 V705 V975 V1004 V301 V1176 V1483 V394 V244 V143 V1549 V1564 V19 V756 V1434 V845 V933 V1248 V11 V851 V290 V296 V280 V1151 V496 V1213 V1574 V406 V120 V560 V924 V882 V639 V634 V1146 V1119 V1131 V1462 V625 V801 V1504 V1189 V457 V986 V1521 V1628 V1346 V561 V403 V1207 V781 V776 V358 V1322 V1150 V1618 V623 V1466 V740 V27 V279 V1188 V1315 V445 V1573 V1632 V122 V123 V307 V77 V1281 V420 V1216 V612 V790 V378 V532 V1059 V127 V1014 V985 V485 V442 V1194 V364 V819 V154 V360 V277 V108 V677 V58 V1355 V1284 V1445 V1366 V1128 V1453 V287 V1156 V982 V807 V289 V618 V1272 V579 V424 V1545 V361 V168 V25 V60 V336 V711 V942 V1172 V1513 V173 V777 V1562 V949 V1064 V368 V1331 V37 V1596 V1375 V111 V121 V828 V1598 V668 V611 V346 V1253 V1608 V717 V1019 V1396 V1583 V1066 V934 V1098 V184 V813 V1111 V189 V587 V1565 V87 V1464 V1267 V707</td></tr><tr><td>12</td><td>-0.542251</td><td> accuracy:0.738924 loss:0.542251 num_examples_weighted:3792</td><td>466</td><td>V1191 V190 V1283 V512 V580 V226 V908 V1226 V673 V67 V250 V261 V1090 V176 V456 V569 V1042 V1411 V1485 V28 V637 V574 V1286 V1181 V505 V418 V489 V1279 V648 V428 V911 V1460 V147 V925 V790 V394 V433 V1159 V182 V1571 V1207 V420 V1583 V958 V521 V560 V1521 V367 V995 V634 V625 V1630 V1071 V1491 V450 V1351 V1266 V517 V553 V845 V1033 V113 V1582 V791 V1375 V247 V869 V220 V425 V11 V1388 V1144 V138 V734 V1263 V417 V886 V1302 V748 V37 V346 V1192 V296 V1016 V1541 V173 V316 V686 V618 V108 V1272 V639 V80 V898 V650 V683 V195 V663 V801 V913 V705 V527 V1562 V1199 V717 V289 V1119 V620 V957 V1096 V1202 V616 V1548 V358 V986 V1430 V941 V879 V1504 V279 V1146 V579 V361 V943 V1619 V1596 V543 V1538 V355 V249 V488 V439 V1369 V1223 V1182 V485 V928 V725 V1177 V35 V25 V572 V490 V268 V71 V457 V896 V859 V592 V184 V318 V235 V740 V706 V983 V1575 V573 V1134 V1061 V951 V608 V924 V747 V1636 V327 V630 V304 V807 V1253 V92 V1281 V56 V1172 V120 V1559 V1315 V1037 V158 V110 V143 V1608 V1189 V1568 V1366 V189 V137 V1151 V360 V280 V1105 V1514 V677 V480 V187 V1379 V374 V27 V1213 V828 V843 V815 V1156 V1528 V609 V29 V703 V89 V178 V598 V1456 V514 V272 V77 V1128 V1605 V1609 V19 V163 V1373 V1027 V1437 V180 V59 V871 V931 V1523 V765 V1368 V547 V813 V1443 V406 V754 V1432 V162 V1565 V1 V583 V1322 V1111 V435 V1581 V1525 V741 V675 V339 V323 V587 V1574 V442 V287 V825 V1131 V1173 V1150 V1332 V1494 V615 V1149 V1632 V1188 V1291 V1590 V20 V714 V411 V532 V1250 V1628 V781 V800 V852 V423 V726 V130 V1066 V105 V955 V347 V325 V1452 V851 V1167 V121 V445 V566 V1019 V244 V496 V685 V1607 V499 V309 V341 V1160 V1331 V588 V1193 V1549 V987 V478 V1356 V239 V262 V1187 V1256 V73 V1260 V688 V780 V245 V214 V1005 V1586 V1577 V584 V647 V575 V1301 V1048 V935 V156 V546 V1010 V1239 V1229 V857 V1240 V1495 V833 V1487 V257 V969 V332 V123 V1018 V1553 V227 V667 V1344 V1109 V164 V1064 V777 V1333 V510 V883 V1434 V1023 V611 V1618 V591 V1268 V1355 V154 V364 V623 V949 V424 V1325 V1512 V13 V1513 V234 V756 V127 V1319 V1062 V942 V742 V1557 V205 V975 V18 V1262 V1068 V388 V192 V307 V1564 V1408 V536 V1598 V1580 V1098 V1204 V865 V1176 V612 V906 V416 V727 V1453 V934 V985 V23 V31 V1063 V561 V1161 V1135 V378 V1507 V1194 V87 V1326 V713 V920 V314 V1059 V1219 V100 V789 V126 V1284 V1414 V109 V556 V36 V168 V1012 V1450 V122 V1022 V1222 V819 V1545 V1466 V39 V368 V111 V476 V1613 V805 V668 V292 V1346 V945 V1085 V982 V1248 V66 V776 V1611 V1216 V1227 V1014 V570 V266 V658 V443 V150 V1339 V652 V353 V711 V1502 V1035 V1445 V1093 V1143 V236 V174 V159 V403 V58</td></tr><tr><td>13</td><td>-0.538332</td><td> accuracy:0.745253 loss:0.538332 num_examples_weighted:3792</td><td>420</td><td>V190 V1191 V1283 V512 V717 V261 V871 V574 V394 V1068 V1226 V673 V1199 V433 V367 V226 V28 V1286 V137 V67 V1062 V250 V592 V908 V1181 V1369 V1411 V1613 V569 V450 V296 V896 V1571 V790 V995 V1630 V147 V428 V505 V1598 V630 V1562 V945 V425 V514 V1262 V911 V1460 V648 V1090 V39 V658 V510 V1281 V957 V869 V1523 V1366 V1507 V368 V327 V1315 V975 V886 V488 V713 V1553 V652 V113 V289 V612 V1609 V706 V292 V1037 V138 V490 V403 V27 V318 V898 V476 V845 V683 V457 V727 V1528 V1485 V943 V1042 V879 V378 V609 V1159 V268 V1022 V1636 V620 V623 V304 V1514 V1149 V615 V1541 V1161 V173 V1105 V1538 V859 V480 V517 V1250 V1085 V611 V754 V791 V742 V192 V1432 V1150 V1207 V587 V178 V1172 V598 V776 V1202 V579 V158 V1443 V154 V618 V1066 V234 V650 V547 V1035 V122 V925 V1452 V1033 V205 V1408 V573 V941 V1156 V1430 V1176 V1559 V1144 V527 V1513 V341 V777 V35 V309 V20 V347 V1151 V56 V1272 V1494 V553 V1189 V983 V1590 V726 V913 V931 V1608 V280 V445 V159 V358 V815 V1549 V339 V1143 V1466 V489 V1173 V163 V443 V227 V825 V1344 V37 V1248 V748 V703 V123 V1016 V637 V325 V1263 V1109 V1568 V1525 V1063 V566 V667 V1012 V969 V955 V1204 V920 V1061 V734 V843 V1437 V423 V686 V499 V1356 V647 V1229 V857 V1239 V1240 V575 V1495 V1260 V780 V1577 V1010 V546 V478 V1301 V156 V935 V1048 V688 V1160 V496 V262 V1187 V239 V584 V1586 V1005 V214 V1256 V73 V245 V257 V1487 V833 V1064 V105 V249 V189 V66 V828 V59 V279 V741 V1284 V807 V521 V1291 V1339 V1512 V608 V1575 V588 V536 V1607 V174 V1580 V1319 V235 V110 V1574 V19 V781 V87 V355 V316 V819 V951 V272 V360 V120 V1502 V307 V1581 V195 V1322 V485 V934 V800 V1227 V435 V361 V958 V639 V747 V1564 V1146 V411 V1071 V1445 V765 V1596 V1491 V164 V388 V1019 V1216 V705 V266 V1545 V1027 V1119 V616 V583 V685 V1182 V561 V1450 V314 V789 V1131 V740 V71 V1279 V1333 V625 V711 V1368 V756 V1111 V985 V126 V111 V987 V244 V1253 V1379 V29 V332 V418 V883 V31 V1059 V121 V143 V353 V1373 V58 V1331 V928 V416 V1618 V36 V13 V813 V323 V1355 V1548 V865 V1557 V1219 V591 V1619 V556 V92 V1456 V11 V442 V424 V982 V287 V949 V924 V220 V714 V1325 V1326 V1414 V1453 V852 V1628 V89 V1223 V560 V1014 V374 V1194 V668 V1375 V187 V1583 V986 V580 V247 V543 V532 V906 V1582 V1134 V1135 V1167 V77 V1222 V677 V100 V1096 V439 V346 V1128 V1213 V1565 V942 V801 V25 V1521 V1268 V572 V23</td></tr><tr><td>14</td><td>-0.53685</td><td> accuracy:0.74288 loss:0.53685 num_examples_weighted:3792</td><td>378</td><td>V190 V512 V1191 V580 V574 V1283 V1199 V1062 V1571 V1485 V1286 V367 V1042 V1411 V394 V648 V428 V1226 V1548 V250 V898 V1037 V261 V553 V1181 V943 V1159 V673 V425 V1262 V433 V1456 V727 V569 V1250 V25 V226 V521 V925 V327 V986 V1281 V1090 V292 V714 V825 V1022 V147 V1272 V1202 V67 V790 V1373 V1263 V913 V1369 V592 V747 V683 V928 V955 V942 V488 V1119 V1414 V1582 V1071 V138 V37 V1366 V1408 V573 V1432 V1156 V1460 V869 V1033 V59 V572 V543 V630 V1619 V717 V296 V941 V1630 V1194 V1279 V801 V765 V560 V777 V1014 V418 V36 V71 V1096 V11 V137 V1173 V1523 V983 V1149 V1445 V1182 V1016 V247 V677 V1368 V908 V439 V1068 V158 V1379 V1608 V178 V1613 V316 V485 V637 V1375 V608 V668 V361 V658 V314 V623 V1565 V234 V1151 V1066 V705 V1331 V1207 V489 V1144 V1063 V620 V1189 V945 V650 V1443 V1146 V781 V843 V27 V1059 V686 V591 V29 V639 V1512 V1253 V1355 V995 V1219 V1161 V1559 V547 V443 V1430 V1322 V883 V819 V951 V195 V378 V667 V911 V579 V1344 V807 V442 V1491 V1450 V924 V1525 V1549 V1213 V726 V1222 V975 V561 V615 V1514 V815 V445 V934 V339 V163 V1528 V748 V279 V266 V741 V192 V1227 V652 V174 V1580 V1105 V1131 V920 V1502 V1172 V358 V1150 V323 V985 V159 V457 V1035 V374 V896 V711 V1596 V244 V1564 V1319 V566 V1248 V517 V616 V611 V1628 V1223 V189 V353 V514 V287 V1061 V588 V105 V969 V1583 V625 V35 V1339 V1012 V1577 V1284 V20 V450 V857 V1256 V1229 V647 V262 V1356 V499 V1160 V688 V1048 V935 V156 V1301 V478 V546 V1010 V780 V1239 V833 V1005 V1260 V1586 V584 V1487 V257 V245 V239 V1495 V73 V214 V575 V1187 V1240 V1513 V756 V341 V1553 V1064 V871 V13 V505 V23 V120 V1333 V28 V685 V77 V612 V1618 V879 V496 V845 V403 V931 V110 V332 V713 V355 V411 V346 V987 V31 V1562 V1109 V89 V388 V1607 V309 V703 V587 V1437 V583 V1143 V1128 V742 V1452 V476 V1581 V1466 V1291 V205 V325 V740 V598 V776 V87 V490 V982 V1315 V1545 V865 V423 V1557 V113 V1568 V304 V111 V618 V1538 V220 V1521 V187 V480 V347 V164 V122 V272 V791 V227 V435 V734 V1135 V39 V360 V1326 V121 V424 V1216 V143 V556 V280 V1325 V527 V1111 V1176 V1268 V19 V957 V368 V1167 V173 V318 V154 V249 V789</td></tr><tr><td>15</td><td>-0.534009</td><td> accuracy:0.742089 loss:0.534009 num_examples_weighted:3792</td><td>341</td><td>V190 V1283 V261 V1191 V512 V1226 V673 V1199 V574 V1062 V580 V717 V1485 V925 V1090 V790 V1411 V871 V1369 V367 V1460 V394 V1571 V1548 V1159 V189 V983 V1022 V418 V226 V28 V1450 V1014 V1286 V553 V433 V428 V592 V1068 V1119 V637 V1279 V727 V521 V67 V1630 V450 V423 V648 V1366 V620 V442 V1181 V122 V1223 V1071 V314 V195 V896 V36 V374 V527 V234 V35 V879 V292 V957 V353 V1628 V815 V250 V37 V898 V734 V561 V1143 V1565 V147 V913 V625 V1538 V476 V686 V573 V928 V1545 V869 V1207 V113 V1167 V1564 V358 V677 V1161 V457 V39 V658 V1523 V714 V756 V819 V598 V1333 V1619 V174 V801 V777 V1325 V1182 V488 V280 V71 V791 V120 V205 V1326 V1344 V975 V789 V1562 V1379 V1173 V121 V781 V272 V623 V439 V1339 V1521 V668 V748 V220 V1368 V1151 V650 V986 V403 V1596 V425 V1096 V1248 V1202 V163 V1042 V1066 V547 V1580 V1176 V327 V945 V747 V87 V1064 V309 V159 V705 V1525 V89 V1150 V1227 V934 V572 V924 V1194 V31 V987 V304 V955 V1109 V1445 V249 V360 V20 V713 V943 V1553 V1583 V616 V1608 V1213 V652 V1408 V187 V1012 V266 V23 V920 V740 V1582 V566 V1135 V1219 V355 V1111 V615 V27 V1061 V435 V111 V287 V1063 V1581 V296 V579 V1549 V505 V1528 V192 V13 V1131 V499 V969 V517 V588 V1513 V496 V19 V1322 V137 V667 V575 V214 V1187 V546 V478 V1356 V1240 V1301 V156 V935 V1048 V688 V1160 V1010 V257 V780 V1239 V73 V833 V1005 V1260 V1495 V239 V1586 V245 V584 V1487 V647 V1229 V1256 V857 V262 V1577 V1355 V1281 V1263 V424 V361 V1146 V883 V1607 V1373 V388 V244 V685 V325 V608 V995 V339 V1284 V807 V490 V1262 V480 V845 V1149 V341 V1105 V741 V1222 V443 V865 V630 V612 V25 V1559 V908 V569 V11 V332 V411 V1319 V1291 V941 V711 V105 V1331 V1568 V560 V323 V618 V158 V143 V1156 V765 V591 V77 V1430 V951 V1618 V164 V1189 V318 V485 V543 V1557 V931 V29 V911 V1514 V1250 V1035 V1437 V1502 V1033 V1315 V316 V583 V611 V985 V1443 V843 V378 V1375 V1466 V368 V445 V742 V1452 V346 V514</td></tr><tr><td>16</td><td>-0.530249</td><td> accuracy:0.740243 loss:0.530249 num_examples_weighted:3792</td><td>307</td><td>V1191 V1283 V1226 V717 V190 V394 V512 V226 V1068 V871 V1411 V673 V428 V1630 V367 V1181 V67 V1548 V517 V1199 V1485 V1286 V250 V569 V1369 V261 V911 V574 V925 V1263 V28 V314 V1071 V488 V579 V433 V1553 V1521 V158 V423 V928 V287 V296 V1033 V1523 V1022 V975 V1315 V374 V1460 V648 V425 V476 V1042 V686 V1368 V439 V591 V941 V637 V316 V625 V630 V650 V1149 V1096 V747 V1450 V592 V1279 V527 V346 V147 V234 V1213 V1143 V1325 V553 V1582 V913 V727 V1564 V943 V189 V457 V1568 V418 V1281 V318 V1430 V789 V1355 V1109 V1502 V879 V266 V27 V1063 V1062 V790 V1291 V612 V580 V616 V1322 V87 V1194 V714 V1538 V403 V1223 V1608 V1150 V1366 V485 V1064 V11 V1571 V945 V572 V618 V480 V1182 V192 V1466 V292 V1167 V163 V1618 V1090 V1066 V924 V309 V748 V1344 V325 V1250 V521 V37 V1151 V573 V353 V195 V1619 V341 V187 V1596 V819 V1189 V547 V514 V598 V1562 V740 V1565 V445 V1207 V1557 V1375 V1111 V424 V360 V174 V1445 V13 V450 V39 V1156 V77 V1061 V1219 V358 V19 V1012 V120 V1319 V89 V865 V583 V1014 V1131 V411 V560 V623 V869 V443 V1559 V29 V1176 V490 V368 V388 V685 V1379 V36 V1119 V1437 V1607 V1580 V304 V588 V1035 V741 V339 V969 V845 V883 V31 V1160 V777 V499 V931 V608 V1495 V584 V1260 V1005 V833 V73 V1239 V780 V257 V1010 V239 V1586 V245 V1577 V647 V1229 V1256 V857 V1487 V262 V1048 V688 V575 V214 V1187 V546 V478 V1356 V1240 V1301 V156 V935 V496 V1525 V1583 V71 V1331 V801 V955 V1549 V807 V121 V355 V1173 V327 V1545 V566 V332 V280 V1373 V765 V677 V1333 V20 V983 V711 V957 V249 V505 V111 V1227 V986 V122 V713 V705 V561 V1443 V611 V435 V951 V1146 V35 V159 V1105 V442 V164 V934 V1339 V896 V1452 V543 V220 V1135 V143 V105 V1326 V920 V987 V1248 V756 V113 V1159 V1628 V995</td></tr><tr><td>17</td><td>-0.528642</td><td> accuracy:0.742089 loss:0.528642 num_examples_weighted:3792</td><td>277</td><td>V1283 V512 V871 V1191 V261 V1199 V190 V1485 V1068 V367 V574 V1226 V673 V428 V686 V1523 V1207 V580 V560 V975 V1538 V1062 V1571 V67 V394 V592 V442 V374 V147 V31 V727 V650 V637 V569 V1325 V1090 V28 V1281 V983 V911 V1181 V450 V521 V1279 V1263 V418 V226 V1562 V1159 V630 V250 V777 V1331 V1149 V913 V1502 V928 V158 V1553 V790 V1109 V292 V1460 V931 V573 V934 V1286 V748 V143 V1521 V1322 V514 V296 V121 V957 V1450 V1071 V111 V332 V925 V553 V1375 V423 V543 V756 V1548 V1369 V439 V488 V403 V1033 V316 V187 V1213 V941 V945 V105 V1189 V943 V346 V1557 V1146 V476 V1227 V433 V425 V616 V879 V71 V457 V677 V485 V11 V1545 V807 V37 V77 V1411 V113 V36 V189 V705 V445 V714 V309 V1344 V1250 V1564 V1173 V39 V325 V1452 V27 V517 V360 V955 V1248 V1042 V192 V747 V1437 V618 V249 V1445 V648 V623 V765 V1291 V339 V611 V304 V713 V1182 V1096 V1618 V358 V368 V819 V318 V1326 V685 V1443 V1549 V164 V87 V1596 V411 V220 V341 V19 V598 V566 V1630 V1430 V1628 V969 V1607 V174 V435 V591 V1156 V951 V711 V156 V1010 V257 V1356 V1240 V1301 V780 V1239 V478 V935 V1495 V584 V1260 V1005 V833 V73 V1577 V1048 V688 V1256 V857 V1229 V1487 V647 V575 V214 V245 V1586 V1187 V546 V239 V262 V588 V1355 V499 V1066 V1160 V1319 V1379 V1568 V1150 V424 V741 V13 V1151 V280 V561 V29 V490 V883 V496 V1131 V1176 V547 V1035 V1580 V388 V1064 V35 V234 V1143 V1219 V579 V789 V195 V612 V120 V995 V845 V89 V1373 V1167 V625 V163 V987 V1194 V920 V608 V480 V1608 V1022 V1565 V122 V1582 V1061 V801 V1559 V1619 V572 V1111 V986 V266 V327 V1333</td></tr><tr><td>18</td><td>-0.526555</td><td> accuracy:0.743143 loss:0.526555 num_examples_weighted:3792</td><td>250</td><td>V512 V190 V261 V1283 V1191 V673 V871 V1042 V1199 V1207 V428 V1226 V1068 V1485 V28 V394 V1571 V226 V911 V1181 V1548 V250 V648 V975 V1411 V1553 V574 V1286 V553 V67 V1630 V1523 V1430 V569 V790 V450 V1062 V1538 V1460 V955 V637 V120 V1325 V445 V147 V195 V1071 V425 V514 V234 V1281 V360 V433 V1159 V928 V986 V1131 V925 V714 V1582 V618 V560 V517 V1173 V1564 V1521 V1150 V1149 V121 V727 V367 V1167 V677 V625 V457 V1559 V27 V442 V591 V1151 V1373 V189 V1369 V187 V1250 V1096 V19 V105 V439 V943 V1375 V1111 V36 V819 V941 V789 V756 V913 V686 V945 V1619 V111 V158 V592 V995 V1344 V1291 V1450 V747 V292 V543 V339 V580 V485 V630 V1445 V711 V358 V309 V1022 V801 V1580 V1568 V87 V1061 V1333 V296 V1628 V89 V1033 V951 V374 V1596 V573 V612 V987 V713 V316 V1109 V845 V748 V341 V1248 V705 V1565 V11 V777 V220 V1066 V1194 V1607 V934 V957 V163 V1035 V1189 V1437 V1176 V35 V1545 V1326 V969 V1355 V1549 V1557 V318 V920 V499 V424 V1319 V332 V566 V1160 V73 V1577 V833 V1005 V1048 V1260 V1256 V857 V239 V584 V1229 V1487 V647 V575 V214 V245 V1586 V1187 V688 V546 V262 V935 V478 V1239 V780 V1301 V1240 V1356 V257 V1010 V156 V1495 V1143 V608 V588 V192 V71 V616 V611 V883 V741 V1219 V1379 V572 V280 V685 V547 V983 V435 V418 V807 V327 V561 V496 V1213 V304 V623 V249 V1146 V174 V39 V411 V143 V1443 V650 V13 V931 V1452 V388 V579 V879 V1562 V113 V488 V346 V598 V1322</td></tr><tr><td>19</td><td>-0.524635</td><td> accuracy:0.745253 loss:0.524635 num_examples_weighted:3792</td><td>225</td><td>V190 V512 V871 V1283 V1191 V226 V1411 V1485 V1068 V1199 V580 V1226 V394 V673 V316 V574 V1538 V1181 V442 V1523 V367 V261 V433 V425 V1071 V925 V1033 V250 V569 V1571 V1553 V1042 V913 V648 V280 V428 V1369 V592 V790 V957 V358 V955 V573 V650 V1150 V1286 V625 V1325 V439 V418 V67 V1630 V147 V28 V1151 V561 V1460 V579 V934 V517 V630 V727 V611 V1149 V1619 V911 V1445 V1565 V1219 V346 V374 V163 V1548 V36 V1167 V975 V1580 V1379 V1559 V1557 V1628 V450 V1248 V318 V1564 V234 V189 V249 V928 V411 V986 V424 V616 V341 V174 V158 V1568 V485 V327 V560 V1159 V591 V71 V292 V756 V618 V187 V296 V598 V1061 V705 V623 V339 V1437 V845 V801 V920 V1562 V1322 V1375 V941 V1194 V1066 V1109 V547 V1596 V435 V566 V309 V1452 V1582 V807 V637 V220 V1022 V1443 V1291 V13 V572 V89 V1319 V1344 V741 V1213 V711 V588 V143 V1176 V35 V686 V1607 V39 V245 V1586 V1256 V214 V575 V1187 V647 V1487 V1229 V584 V239 V857 V688 V546 V262 V935 V478 V1239 V780 V1301 V1240 V1356 V257 V1010 V156 V1495 V1048 V1005 V833 V1577 V73 V1260 V499 V969 V1160 V685 V1281 V195 V1189 V1131 V1373 V714 V496 V388 V121 V608 V777 V945 V11 V1430 V951 V1549 V987 V883 V120 V457 V747 V105 V748 V1207 V19 V1521 V27 V488 V879 V1173 V1450 V1035 V713 V819 V1326 V445 V1111</td></tr><tr><td>20</td><td>-0.523595</td><td> accuracy:0.746044 loss:0.523595 num_examples_weighted:3792</td><td>203</td><td>V1283 V190 V1191 V673 V871 V512 V226 V1485 V1181 V925 V790 V394 V67 V574 V1226 V1199 V1571 V1207 V1548 V28 V1411 V261 V1159 V433 V249 V1523 V913 V955 V1167 V428 V625 V316 V1068 V1565 V1033 V517 V1325 V327 V36 V1042 V418 V358 V727 V975 V442 V1071 V195 V250 V648 V705 V457 V1630 V560 V630 V147 V1286 V1375 V374 V120 V292 V439 V1619 V39 V598 V121 V957 V1582 V573 V187 V1219 V411 V234 V801 V879 V445 V911 V845 V547 V561 V1109 V158 V1460 V748 V1111 V1580 V485 V1322 V1557 V450 V756 V569 V1326 V1248 V1189 V572 V1443 V713 V280 V425 V1213 V1344 V339 V650 V714 V13 V1291 V1628 V367 V1430 V11 V611 V309 V318 V1559 V637 V807 V591 V296 V1281 V1319 V819 V616 V969 V1150 V341 V163 V685 V496 V986 V951 V499 V920 V19 V935 V1301 V780 V1239 V478 V1229 V262 V546 V688 V857 V239 V584 V1260 V1240 V1356 V257 V1010 V156 V1495 V1160 V1256 V1048 V1005 V833 V1577 V73 V647 V1187 V575 V214 V1487 V1586 V245 V566 V1061 V741 V174 V1568 V1176 V588 V1607 V618 V1437 V27 V987 V424 V592 V388 V1562 V488 V1549 V1035 V1149 V1445 V711 V1151 V623 V777 V686 V928 V580 V1553 V189 V1369 V1373 V89 V346 V1173 V934 V71</td></tr><tr><td>21</td><td>-0.520246</td><td> accuracy:0.750264 loss:0.520246 num_examples_weighted:3792</td><td>183</td><td>V871 V1283 V226 V190 V1199 V673 V394 V1485 V1226 V261 V512 V1181 V1071 V250 V1460 V1191 V580 V911 V28 V1571 V367 V1042 V650 V569 V1375 V1286 V913 V1553 V790 V442 V1281 V686 V36 V1582 V1159 V67 V1630 V1068 V121 V189 V957 V433 V1189 V418 V574 V928 V428 V374 V1565 V457 V618 V975 V748 V425 V1430 V625 V147 V89 V934 V630 V517 V27 V1369 V158 V309 V727 V1219 V955 V1033 V648 V339 V1291 V637 V1559 V560 V234 V292 V411 V1151 V1373 V1322 V777 V1173 V1619 V986 V296 V318 V249 V187 V845 V1523 V195 V925 V713 V591 V388 V1213 V163 V488 V19 V316 V1411 V572 V174 V714 V71 V1580 V879 V120 V1176 V424 V1437 V685 V485 V547 V1150 V573 V807 V13 V969 V1549 V616 V496 V1607 V1344 V1319 V566 V1149 V1160 V1356 V1586 V257 V1010 V156 V1495 V1256 V1048 V1005 V478 V833 V73 V647 V1187 V575 V214 V1487 V245 V1260 V1240 V935 V1301 V780 V1239 V1229 V546 V688 V262 V857 V239 V584 V588 V561 V1557 V499 V1577 V920 V346 V741 V39 V1167 V1109 V450 V1548 V1207 V623 V1325 V327 V987 V951 V358 V445 V611 V592</td></tr><tr><td>22</td><td>-0.516373</td><td> accuracy:0.753956 loss:0.516373 num_examples_weighted:3792</td><td>165</td><td>V190 V1283 V673 V1191 V1181 V1068 V871 V925 V574 V580 V394 V1199 V1485 V261 V1226 V790 V1553 V637 V1207 V955 V67 V569 V630 V158 V28 V1369 V512 V1286 V147 V1548 V316 V748 V1150 V1151 V517 V1167 V488 V1630 V442 V1580 V616 V1042 V428 V911 V713 V1565 V250 V1619 V1109 V292 V189 V913 V1033 V226 V120 V1149 V418 V1523 V561 V650 V727 V560 V1460 V450 V121 V39 V1189 V367 V71 V623 V234 V433 V1281 V411 V572 V36 V195 V1571 V1411 V934 V249 V457 V1291 V1559 V1071 V163 V19 V374 V1176 V1322 V1582 V1325 V777 V1437 V318 V879 V425 V1219 V975 V591 V309 V388 V566 V445 V1344 V1375 V346 V424 V685 V1160 V547 V951 V89 V741 V1159 V499 V1607 V575 V214 V1487 V245 V647 V1260 V833 V73 V1187 V1240 V935 V1301 V780 V1239 V1229 V546 V688 V262 V1586 V857 V239 V584 V1005 V1048 V1256 V1495 V156 V1010 V257 V1356 V478 V969 V1577 V496 V611 V1549 V1557 V920 V358 V1213 V27 V625 V485 V1319 V588 V187 V648 V13</td></tr><tr><td>23</td><td>-0.514395</td><td> accuracy:0.74789 loss:0.514395 num_examples_weighted:3792</td><td>149</td><td>V1191 V1283 V512 V226 V1199 V871 V190 V1485 V673 V1226 V450 V955 V790 V637 V1068 V428 V648 V433 V1582 V1286 V1181 V234 V925 V1460 V261 V1553 V1071 V28 V569 V574 V394 V580 V250 V1375 V1411 V713 V1207 V1369 V67 V911 V934 V517 V1619 V425 V189 V1159 V975 V418 V367 V1189 V1548 V1322 V442 V1042 V488 V1281 V121 V1176 V1109 V1167 V19 V292 V650 V457 V316 V1565 V561 V187 V920 V1630 V630 V748 V1580 V346 V1149 V1151 V120 V1523 V1319 V1571 V1607 V411 V616 V1344 V1557 V147 V951 V445 V913 V727 V374 V1033 V485 V195 V623 V309 V625 V741 V36 V496 V499 V969 V1213 V566 V1160 V1437 V158 V935 V1301 V780 V1239 V1229 V546 V156 V688 V1240 V262 V1586 V857 V239 V584 V1005 V1048 V1256 V1495 V1577 V73 V478 V1356 V257 V1010 V575 V214 V1487 V245 V647 V1260 V833 V1187 V588 V560 V685 V547 V1291 V388 V1559 V424 V1325 V1150</td></tr><tr><td>24</td><td>-0.513139</td><td> accuracy:0.750791 loss:0.513139 num_examples_weighted:3792</td><td>135</td><td>V190 V1283 V871 V1068 V1199 V1485 V512 V1191 V1226 V1571 V226 V1460 V569 V1630 V911 V1286 V67 V261 V673 V428 V1325 V1553 V374 V28 V367 V457 V394 V442 V292 V1167 V1582 V121 V790 V625 V195 V147 V1071 V1523 V630 V1033 V1411 V913 V560 V648 V1580 V574 V158 V1369 V1042 V1181 V1291 V187 V1159 V189 V433 V1189 V488 V1150 V517 V975 V1281 V637 V250 V1619 V425 V955 V316 V1207 V925 V445 V411 V1151 V616 V309 V748 V346 V713 V1319 V580 V934 V1375 V1344 V547 V1213 V1109 V450 V969 V650 V388 V1607 V120 V499 V1559 V566 V685 V1577 V920 V478 V1586 V857 V239 V584 V1005 V1048 V1256 V1495 V73 V1240 V1356 V257 V1010 V575 V214 V1487 V245 V647 V1260 V262 V688 V156 V546 V1229 V1239 V780 V1301 V935 V833 V1160 V1187 V741 V588 V623 V951 V234 V418</td></tr><tr><td>25</td><td>-0.515482</td><td> accuracy:0.751055 loss:0.515482 num_examples_weighted:3792</td><td>122</td><td>V871 V512 V1191 V1283 V190 V261 V1485 V1068 V394 V574 V673 V911 V428 V1226 V955 V1199 V147 V1042 V790 V374 V67 V569 V650 V637 V1571 V250 V187 V580 V28 V517 V1325 V1553 V1286 V1411 V1159 V316 V457 V226 V1167 V630 V1151 V1369 V1291 V1619 V1630 V450 V560 V442 V648 V1150 V1281 V292 V1181 V623 V445 V625 V1523 V425 V309 V913 V1582 V158 V234 V418 V920 V1189 V1344 V713 V195 V1607 V616 V975 V925 V346 V1460 V969 V189 V1033 V1071 V367 V388 V1319 V411 V488 V588 V566 V1160 V1577 V1240 V1256 V1356 V257 V1010 V575 V214 V1487 V245 V647 V1260 V262 V688 V156 V546 V1229 V1239 V780 V478 V1187 V833 V935 V499 V1301 V73 V1586 V857 V239 V584 V1005 V1048 V1495 V547 V685</td></tr><tr><td>26</td><td>-0.512282</td><td> accuracy:0.756329 loss:0.512282 num_examples_weighted:3792</td><td>110</td><td>V871 V673 V1191 V1283 V1485 V512 V190 V1068 V1199 V1553 V226 V574 V1181 V1226 V1523 V790 V630 V1460 V1619 V1571 V445 V560 V1281 V911 V637 V1369 V425 V1033 V1582 V250 V955 V1286 V195 V650 V189 V394 V1630 V648 V616 V623 V925 V28 V316 V158 V292 V913 V374 V1189 V457 V625 V517 V67 V1411 V147 V1319 V187 V1150 V1159 V442 V411 V920 V1071 V1325 V1151 V1291 V346 V234 V1042 V547 V367 V569 V418 V1160 V499 V1495 V1344 V156 V214 V1487 V245 V647 V1260 V262 V1301 V688 V478 V546 V1229 V935 V1239 V833 V1187 V780 V1010 V1048 V1005 V584 V239 V857 V1586 V73 V1577 V1240 V1256 V1356 V257 V575 V488 V969 V588</td></tr><tr><td>27</td><td>-0.512737</td><td> accuracy:0.747099 loss:0.512737 num_examples_weighted:3792</td><td>99</td><td>V190 V1283 V871 V673 V1191 V226 V512 V574 V1042 V1485 V67 V1553 V1068 V911 V1630 V250 V1286 V367 V790 V234 V650 V1523 V630 V1181 V517 V1226 V189 V394 V1199 V1411 V569 V1571 V1325 V560 V418 V1460 V445 V28 V913 V1033 V955 V158 V1159 V648 V374 V625 V1281 V1369 V316 V147 V195 V1071 V1151 V637 V1619 V547 V187 V1189 V925 V1344 V616 V292 V1319 V1150 V442 V588 V969 V499 V1005 V546 V1229 V935 V1239 V833 V1187 V780 V1010 V1048 V688 V584 V239 V857 V1586 V73 V1240 V478 V575 V257 V1356 V1495 V1256 V156 V214 V1487 V245 V647 V1260 V262 V1301</td></tr><tr><td>28</td><td>-0.510276</td><td> accuracy:0.753692 loss:0.510276 num_examples_weighted:3792</td><td>90</td><td>V871 V190 V673 V1283 V1485 V1191 V574 V512 V790 V1226 V1068 V226 V630 V955 V569 V1042 V637 V1181 V250 V650 V367 V67 V374 V1571 V1199 V1286 V1411 V911 V560 V1553 V1281 V394 V1150 V189 V648 V1189 V147 V1033 V28 V445 V517 V1071 V1344 V625 V1523 V1325 V316 V234 V1619 V1460 V925 V195 V616 V158 V913 V1630 V292 V1369 V418 V588 V969 V1319 V499 V1356 V857 V257 V575 V478 V1240 V73 V1586 V1260 V1256 V156 V214 V1487 V245 V1301 V262 V647 V584 V688 V1048 V1010 V780 V1187 V833 V1239 V239 V935</td></tr><tr class=\"best\"><td>29</td><td>-0.507455</td><td> accuracy:0.753956 loss:0.507455 num_examples_weighted:3792</td><td>81</td><td>V673 V226 V1283 V1191 V871 V190 V574 V1226 V911 V1460 V1199 V1411 V1485 V1068 V637 V250 V1042 V394 V1553 V560 V189 V1286 V28 V517 V790 V1369 V158 V418 V1281 V512 V367 V195 V1571 V67 V955 V1523 V648 V1325 V630 V147 V569 V925 V445 V316 V625 V1181 V1619 V913 V1630 V650 V616 V374 V1189 V1344 V1319 V292 V969 V214 V1487 V245 V1301 V262 V647 V584 V688 V1048 V1010 V780 V1187 V239 V935 V1239 V833 V1256 V1356 V857 V257 V575 V478 V1240 V73</td></tr><tr><td>30</td><td>-0.511792</td><td> accuracy:0.751582 loss:0.511792 num_examples_weighted:3792</td><td>73</td><td>V871 V1191 V190 V673 V1283 V226 V1181 V574 V1226 V394 V1523 V1068 V67 V560 V790 V1042 V637 V911 V1485 V1571 V650 V1199 V512 V250 V625 V28 V1286 V1460 V316 V374 V925 V569 V189 V445 V955 V913 V630 V147 V1189 V1281 V195 V292 V1619 V158 V1369 V1411 V648 V418 V1325 V1630 V1344 V517 V367 V1319 V1553 V969 V1187 V73 V1240 V478 V575 V257 V857 V1356 V1256 V833 V1239 V935 V239 V780 V1010 V1048 V688</td></tr><tr><td>31</td><td>-0.510255</td><td> accuracy:0.753428 loss:0.510255 num_examples_weighted:3792</td><td>66</td><td>V1485 V190 V226 V1283 V871 V574 V67 V650 V1191 V1460 V673 V1181 V790 V418 V1199 V637 V1553 V512 V1411 V911 V1286 V316 V250 V367 V913 V1042 V28 V374 V925 V955 V1523 V517 V630 V1619 V292 V1571 V1068 V1226 V1281 V569 V560 V158 V445 V1630 V1369 V1189 V1319 V147 V625 V394 V189 V195 V1344 V969 V857 V688 V1048 V1010 V780 V239 V935 V1239 V833 V1256 V1356 V257</td></tr><tr><td>32</td><td>-0.508708</td><td> accuracy:0.747363 loss:0.508708 num_examples_weighted:3792</td><td>60</td><td>V673 V190 V871 V574 V512 V226 V1283 V1191 V1068 V1226 V790 V1553 V1286 V1485 V28 V1042 V560 V1523 V374 V394 V1411 V911 V569 V955 V650 V630 V147 V1571 V67 V1281 V637 V625 V1199 V1460 V292 V913 V367 V250 V517 V189 V1181 V1619 V158 V1189 V925 V418 V316 V1630 V1344 V1319 V1369 V969 V445 V257 V1356 V1256 V833 V1239 V935 V239</td></tr><tr><td>33</td><td>-0.509896</td><td> accuracy:0.753692 loss:0.509896 num_examples_weighted:3792</td><td>54</td><td>V226 V871 V190 V673 V1283 V574 V1485 V28 V67 V512 V1460 V1226 V1068 V394 V1553 V418 V517 V1523 V1630 V569 V1286 V189 V1411 V1191 V1042 V374 V790 V292 V625 V913 V925 V560 V316 V955 V250 V630 V911 V1189 V650 V147 V1619 V158 V1281 V1369 V1571 V1199 V1181 V367 V1319 V1344 V445 V239 V935 V1239</td></tr><tr><td>34</td><td>-0.507959</td><td> accuracy:0.753165 loss:0.507959 num_examples_weighted:3792</td><td>49</td><td>V871 V1191 V1283 V574 V190 V1485 V226 V1068 V394 V673 V1553 V1226 V418 V1181 V911 V925 V250 V1460 V790 V1042 V67 V28 V913 V1286 V1411 V147 V630 V1571 V374 V292 V650 V1630 V1369 V1199 V1189 V517 V569 V316 V367 V560 V625 V1281 V1523 V1344 V1619 V955 V512 V158 V189</td></tr><tr><td>35</td><td>-0.517889</td><td> accuracy:0.756065 loss:0.517889 num_examples_weighted:3792</td><td>45</td><td>V673 V226 V190 V871 V574 V1283 V790 V512 V1485 V1226 V394 V1068 V1411 V1199 V1553 V1181 V1571 V1286 V911 V1191 V1281 V1523 V67 V1042 V316 V250 V630 V189 V418 V28 V1619 V374 V913 V560 V650 V569 V955 V517 V1460 V925 V1369 V147 V1630 V625 V1189</td></tr><tr><td>36</td><td>-0.510443</td><td> accuracy:0.755274 loss:0.510443 num_examples_weighted:3792</td><td>41</td><td>V673 V190 V574 V1411 V1283 V1191 V1485 V871 V226 V790 V394 V1068 V1226 V1181 V1460 V67 V1553 V512 V569 V28 V1042 V418 V1286 V913 V374 V1369 V911 V925 V147 V517 V630 V1571 V1199 V650 V250 V625 V1630 V189 V316 V1281 V560</td></tr><tr><td>37</td><td>-0.514292</td><td> accuracy:0.751846 loss:0.514292 num_examples_weighted:3792</td><td>37</td><td>V871 V1283 V190 V1191 V673 V574 V1485 V790 V1226 V226 V374 V911 V1411 V1068 V1553 V394 V569 V650 V512 V67 V189 V250 V1181 V1460 V147 V517 V1630 V1286 V560 V925 V418 V1199 V28 V625 V1281 V913 V1571</td></tr><tr><td>38</td><td>-0.518338</td><td> accuracy:0.750791 loss:0.518338 num_examples_weighted:3792</td><td>34</td><td>V190 V512 V1068 V673 V1553 V1191 V226 V574 V394 V871 V790 V374 V67 V1283 V569 V1226 V1199 V1286 V1485 V1411 V1181 V147 V418 V189 V913 V28 V925 V1460 V560 V1281 V650 V1571 V250 V911</td></tr><tr><td>39</td><td>-0.515307</td><td> accuracy:0.748154 loss:0.515307 num_examples_weighted:3792</td><td>31</td><td>V1191 V871 V574 V673 V1283 V226 V190 V28 V512 V1226 V790 V1485 V1553 V1181 V1068 V1286 V67 V394 V1411 V560 V650 V925 V1199 V1571 V189 V1460 V569 V911 V913 V1281 V147</td></tr><tr><td>40</td><td>-0.511005</td><td> accuracy:0.753165 loss:0.511005 num_examples_weighted:3792</td><td>28</td><td>V673 V226 V190 V1283 V790 V1068 V574 V871 V1226 V1553 V1460 V512 V911 V1411 V569 V1571 V67 V28 V1199 V1191 V189 V1181 V394 V560 V147 V1286 V650 V1485</td></tr><tr><td>41</td><td>-0.517346</td><td> accuracy:0.750791 loss:0.517346 num_examples_weighted:3792</td><td>26</td><td>V673 V574 V871 V1191 V190 V226 V1485 V1283 V1068 V512 V1286 V911 V28 V1553 V394 V790 V67 V1411 V569 V1226 V650 V1571 V189 V1460 V560 V1199</td></tr><tr><td>42</td><td>-0.512426</td><td> accuracy:0.748945 loss:0.512426 num_examples_weighted:3792</td><td>24</td><td>V673 V190 V574 V790 V1191 V871 V226 V1283 V1226 V1553 V67 V1068 V569 V911 V1411 V394 V1460 V28 V1199 V650 V1485 V560 V1286 V512</td></tr><tr><td>43</td><td>-0.51368</td><td> accuracy:0.751055 loss:0.51368 num_examples_weighted:3792</td><td>22</td><td>V673 V871 V190 V1283 V226 V1553 V574 V790 V1226 V1068 V512 V1485 V911 V569 V1286 V67 V1460 V394 V650 V1191 V1199 V1411</td></tr><tr><td>44</td><td>-0.515758</td><td> accuracy:0.750791 loss:0.515758 num_examples_weighted:3792</td><td>20</td><td>V871 V190 V226 V574 V1191 V1283 V673 V67 V1286 V790 V1411 V1553 V1226 V911 V1460 V1068 V1485 V569 V650 V394</td></tr><tr><td>45</td><td>-0.513439</td><td> accuracy:0.750527 loss:0.513439 num_examples_weighted:3792</td><td>18</td><td>V1191 V226 V190 V673 V1283 V67 V574 V871 V790 V1226 V911 V569 V1553 V394 V650 V1286 V1411 V1460</td></tr><tr><td>46</td><td>-0.51819</td><td> accuracy:0.744726 loss:0.51819 num_examples_weighted:3792</td><td>17</td><td>V1191 V673 V871 V190 V226 V1283 V790 V574 V67 V1553 V1411 V569 V1226 V911 V650 V1286 V394</td></tr><tr><td>47</td><td>-0.516085</td><td> accuracy:0.750791 loss:0.516085 num_examples_weighted:3792</td><td>16</td><td>V1191 V673 V190 V871 V1283 V226 V790 V574 V67 V1286 V1553 V1226 V911 V1411 V650 V394</td></tr><tr><td>48</td><td>-0.514716</td><td> accuracy:0.752901 loss:0.514716 num_examples_weighted:3792</td><td>15</td><td>V1191 V673 V190 V1283 V871 V574 V226 V790 V1286 V650 V67 V911 V394 V1553 V1226</td></tr><tr><td>49</td><td>-0.519566</td><td> accuracy:0.751582 loss:0.519566 num_examples_weighted:3792</td><td>14</td><td>V1191 V673 V226 V190 V1286 V67 V394 V871 V650 V790 V1283 V911 V574 V1226</td></tr><tr><td>50</td><td>-0.527063</td><td> accuracy:0.751582 loss:0.527063 num_examples_weighted:3792</td><td>13</td><td>V1191 V673 V226 V190 V871 V790 V394 V574 V1283 V67 V650 V1286 V911</td></tr><tr><td>51</td><td>-0.541045</td><td> accuracy:0.746308 loss:0.541045 num_examples_weighted:3792</td><td>12</td><td>V1191 V226 V673 V190 V790 V574 V911 V871 V67 V650 V1283 V394</td></tr><tr><td>52</td><td>-0.534168</td><td> accuracy:0.74789 loss:0.534168 num_examples_weighted:3792</td><td>11</td><td>V673 V226 V1191 V67 V190 V790 V574 V871 V650 V1283 V911</td></tr><tr><td>53</td><td>-0.535473</td><td> accuracy:0.746835 loss:0.535473 num_examples_weighted:3792</td><td>10</td><td>V1191 V226 V673 V190 V790 V574 V67 V871 V1283 V911</td></tr><tr><td>54</td><td>-0.535162</td><td> accuracy:0.737342 loss:0.535162 num_examples_weighted:3792</td><td>9</td><td>V190 V226 V1191 V673 V67 V790 V871 V574 V911</td></tr><tr><td>55</td><td>-0.549058</td><td> accuracy:0.734441 loss:0.549058 num_examples_weighted:3792</td><td>8</td><td>V1191 V226 V190 V673 V871 V790 V574 V67</td></tr><tr><td>56</td><td>-0.551671</td><td> accuracy:0.719673 loss:0.551671 num_examples_weighted:3792</td><td>7</td><td>V190 V226 V1191 V871 V574 V673 V790</td></tr><tr><td>57</td><td>-0.567408</td><td> accuracy:0.719409 loss:0.567408 num_examples_weighted:3792</td><td>6</td><td>V226 V190 V871 V1191 V574 V673</td></tr><tr><td>58</td><td>-0.58834</td><td> accuracy:0.70385 loss:0.58834 num_examples_weighted:3792</td><td>5</td><td>V226 V190 V871 V1191 V673</td></tr><tr><td>59</td><td>-0.611016</td><td> accuracy:0.705169 loss:0.611016 num_examples_weighted:3792</td><td>4</td><td>V226 V871 V190 V1191</td></tr><tr><td>60</td><td>-0.654282</td><td> accuracy:0.696466 loss:0.654282 num_examples_weighted:3792</td><td>3</td><td>V226 V871 V1191</td></tr><tr><td>61</td><td>-0.759617</td><td> accuracy:0.673787 loss:0.759617 num_examples_weighted:3792</td><td>2</td><td>V871 V1191</td></tr><tr><td>62</td><td>-1.82378</td><td> accuracy:0.621572 loss:1.82378 num_examples_weighted:3792</td><td>1</td><td>V1191</td></tr></table></div><div id=\"57d7-96f5-1cb9-4452_body_training\" class=\"tab_content\"><p>The following evaluation is computed on the validation or out-of-bag dataset.</p><pre class=\"ydf_pre\">Number of predictions (without weights): 3792\n",
       "Number of predictions (with weights): 3792\n",
       "Task: CLASSIFICATION\n",
       "Label: class\n",
       "\n",
       "Accuracy: 0.753956  CI95[W][0.742176 0.765445]\n",
       "LogLoss: : 0.507455\n",
       "ErrorRate: : 0.246044\n",
       "\n",
       "Default Accuracy: : 0.5\n",
       "Default LogLoss: : 0.693147\n",
       "Default ErrorRate: : 0.5\n",
       "\n",
       "Confusion Table:\n",
       "truth\\prediction\n",
       "          0     1\n",
       "    0  1484   412\n",
       "    1   521  1375\n",
       "Total: 3792\n",
       "\n",
       "</pre><div style='display: grid; gap: 0px; grid-auto-columns: min-content;'><div style='grid-row:1 / span 1; grid-column:1 / span 1;'><script src='https://www.gstatic.com/external_hosted/plotly/plotly.min.js'></script>\n",
       "<div id=\"chart_57d7_96f5_1cb9_4452self_eval_item0\" style=\"display: inline-block;\" ></div>\n",
       "<script>\n",
       "  Plotly.newPlot(\n",
       "    'chart_57d7_96f5_1cb9_4452self_eval_item0',\n",
       "    [{\n",
       "x: [1,11,21,31,41,51,61,71,81,91,101,111,121,131,141,151,161,171,181,191,201,211,221,231,241,251,261,271,281,291,300],\n",
       "y: [0.636231,0.681734,0.717678,0.734968,0.742089,0.748154,0.749209,0.746308,0.744989,0.747099,0.750527,0.751055,0.750264,0.753692,0.753428,0.755011,0.755538,0.756065,0.756329,0.758175,0.756065,0.757648,0.757384,0.755274,0.758703,0.756593,0.758966,0.757911,0.757384,0.755538,0.753956],\n",
       "type: 'scatter',\n",
       "mode: 'lines',\n",
       "line: {\n",
       "  dash: 'solid',\n",
       "  width: 1\n",
       "},\n",
       "},\n",
       "],\n",
       "    {\n",
       "      width: 600,\n",
       "      height: 400,\n",
       "      title: '',\n",
       "      showlegend: true,\n",
       "      xaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'num trees',\n",
       "        },\n",
       "      font: {\n",
       "        size: 10,\n",
       "        },\n",
       "      yaxis: {\n",
       "        ticks: 'outside',\n",
       "        showgrid: true,\n",
       "        zeroline: false,\n",
       "        showline: true,\n",
       "        title: 'accuracy',\n",
       "        },\n",
       "      margin: {\n",
       "        l: 50,\n",
       "        r: 50,\n",
       "        b: 50,\n",
       "        t: 50,\n",
       "      },\n",
       "    },\n",
       "    {\n",
       "      modeBarButtonsToRemove: ['sendDataToCloud'],\n",
       "      displaylogo: false,displayModeBar: false,\n",
       "    }\n",
       "  );\n",
       "</script>\n",
       "</div></div></div><div id=\"57d7-96f5-1cb9-4452_body_variable_importance\" class=\"tab_content\"><p><a target=\"_blank\" href=\"https://ydf.readthedocs.io/en/latest/cli_user_manual#variable-importances\">Variable importances</a> measure the importance of an input feature for a model.</p><div id=\"57d7-96f5-1cb9-4452_vi\" class=\"variable_importance\"><select onchange=\"ydfShowVariableImportance('57d7-96f5-1cb9-4452_vi')\"><option value=\"INV_MEAN_MIN_DEPTH\">INV_MEAN_MIN_DEPTH</option><option value=\"MEAN_DECREASE_IN_ACCURACY\">MEAN_DECREASE_IN_ACCURACY</option><option value=\"MEAN_DECREASE_IN_AP_1_VS_OTHERS\">MEAN_DECREASE_IN_AP_1_VS_OTHERS</option><option value=\"MEAN_DECREASE_IN_AUC_1_VS_OTHERS\">MEAN_DECREASE_IN_AUC_1_VS_OTHERS</option><option value=\"MEAN_DECREASE_IN_PRAUC_1_VS_OTHERS\">MEAN_DECREASE_IN_PRAUC_1_VS_OTHERS</option><option value=\"NUM_AS_ROOT\">NUM_AS_ROOT</option><option value=\"NUM_NODES\">NUM_NODES</option><option value=\"SUM_SCORE\">SUM_SCORE</option></select><div id=\"57d7-96f5-1cb9-4452_vi_body_INV_MEAN_MIN_DEPTH\" class=\"content selected\"><pre class=\"ydf_pre\">    1. &quot;V1191&quot;  0.143075 ################\n",
       "    2.  &quot;V871&quot;  0.132044 ############\n",
       "    3.  &quot;V512&quot;  0.131939 ############\n",
       "    4.  &quot;V673&quot;  0.120332 #########\n",
       "    5.  &quot;V190&quot;  0.119444 #########\n",
       "    6.  &quot;V226&quot;  0.118587 ########\n",
       "    7. &quot;V1485&quot;  0.111161 ######\n",
       "    8.  &quot;V574&quot;  0.107600 #####\n",
       "    9. &quot;V1283&quot;  0.107139 #####\n",
       "   10. &quot;V1181&quot;  0.103886 ####\n",
       "   11. &quot;V1199&quot;  0.099565 ###\n",
       "   12. &quot;V1226&quot;  0.099382 ###\n",
       "   13. &quot;V1068&quot;  0.099066 ###\n",
       "   14. &quot;V1411&quot;  0.098907 ###\n",
       "   15.  &quot;V394&quot;  0.098121 ##\n",
       "   16. &quot;V1553&quot;  0.097360 ##\n",
       "   17. &quot;V1571&quot;  0.096900 ##\n",
       "   18.  &quot;V911&quot;  0.096269 ##\n",
       "   19. &quot;V1042&quot;  0.096151 ##\n",
       "   20.  &quot;V367&quot;  0.095712 ##\n",
       "   21.   &quot;V67&quot;  0.095178 ##\n",
       "   22.  &quot;V250&quot;  0.094569 #\n",
       "   23. &quot;V1460&quot;  0.094294 #\n",
       "   24.   &quot;V28&quot;  0.094227 #\n",
       "   25.  &quot;V637&quot;  0.093642 #\n",
       "   26. &quot;V1286&quot;  0.093543 #\n",
       "   27.  &quot;V913&quot;  0.093223 #\n",
       "   28.  &quot;V569&quot;  0.093187 #\n",
       "   29.  &quot;V648&quot;  0.093094 #\n",
       "   30.  &quot;V925&quot;  0.093071 #\n",
       "   31. &quot;V1523&quot;  0.092946 #\n",
       "   32.  &quot;V790&quot;  0.092468 #\n",
       "   33. &quot;V1630&quot;  0.092393 #\n",
       "   34. &quot;V1369&quot;  0.092380 #\n",
       "   35.  &quot;V418&quot;  0.092223 #\n",
       "   36. &quot;V1325&quot;  0.092060 #\n",
       "   37. &quot;V1281&quot;  0.092043 #\n",
       "   38.  &quot;V560&quot;  0.091778 #\n",
       "   39.  &quot;V292&quot;  0.091694 #\n",
       "   40.  &quot;V374&quot;  0.091539 #\n",
       "   41. &quot;V1619&quot;  0.091371 #\n",
       "   42.  &quot;V316&quot;  0.091239 #\n",
       "   43.  &quot;V147&quot;  0.091188 \n",
       "   44.  &quot;V650&quot;  0.090949 \n",
       "   45.  &quot;V955&quot;  0.090942 \n",
       "   46.  &quot;V189&quot;  0.090746 \n",
       "   47.  &quot;V445&quot;  0.090641 \n",
       "   48.  &quot;V630&quot;  0.090355 \n",
       "   49.  &quot;V158&quot;  0.090314 \n",
       "   50.  &quot;V625&quot;  0.089988 \n",
       "   51.  &quot;V195&quot;  0.089856 \n",
       "   52.  &quot;V517&quot;  0.089261 \n",
       "   53. &quot;V1189&quot;  0.088691 \n",
       "   54.  &quot;V616&quot;  0.088673 \n",
       "   55. &quot;V1344&quot;  0.088344 \n",
       "   56. &quot;V1319&quot;  0.087986 \n",
       "   57.  &quot;V969&quot;  0.087778 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_MEAN_DECREASE_IN_ACCURACY\" class=\"content\"><pre class=\"ydf_pre\">    1.  &quot;V226&quot;  0.004747 ################\n",
       "    2.  &quot;V560&quot;  0.003428 ##############\n",
       "    3.  &quot;V650&quot;  0.002637 #############\n",
       "    4.  &quot;V569&quot;  0.002373 ############\n",
       "    5.   &quot;V67&quot;  0.002373 ############\n",
       "    6. &quot;V1485&quot;  0.002373 ############\n",
       "    7. &quot;V1369&quot;  0.001582 ###########\n",
       "    8.  &quot;V316&quot;  0.001582 ###########\n",
       "    9. &quot;V1283&quot;  0.001582 ###########\n",
       "   10.  &quot;V673&quot;  0.001582 ###########\n",
       "   11.  &quot;V574&quot;  0.001319 ###########\n",
       "   12.  &quot;V374&quot;  0.000527 ##########\n",
       "   13. &quot;V1319&quot;  0.000527 ##########\n",
       "   14.  &quot;V292&quot;  0.000527 ##########\n",
       "   15.  &quot;V969&quot;  0.000264 #########\n",
       "   16. &quot;V1571&quot;  0.000264 #########\n",
       "   17.  &quot;V790&quot;  0.000264 #########\n",
       "   18.  &quot;V189&quot;  0.000264 #########\n",
       "   19. &quot;V1325&quot;  0.000264 #########\n",
       "   20.  &quot;V250&quot;  0.000264 #########\n",
       "   21.  &quot;V688&quot;  0.000000 #########\n",
       "   22.  &quot;V214&quot;  0.000000 #########\n",
       "   23. &quot;V1487&quot;  0.000000 #########\n",
       "   24.  &quot;V245&quot;  0.000000 #########\n",
       "   25.  &quot;V478&quot;  0.000000 #########\n",
       "   26. &quot;V1240&quot;  0.000000 #########\n",
       "   27. &quot;V1301&quot;  0.000000 #########\n",
       "   28.  &quot;V262&quot;  0.000000 #########\n",
       "   29.  &quot;V647&quot;  0.000000 #########\n",
       "   30.  &quot;V584&quot;  0.000000 #########\n",
       "   31.  &quot;V575&quot;  0.000000 #########\n",
       "   32. &quot;V1048&quot;  0.000000 #########\n",
       "   33.   &quot;V73&quot;  0.000000 #########\n",
       "   34. &quot;V1010&quot;  0.000000 #########\n",
       "   35.  &quot;V780&quot;  0.000000 #########\n",
       "   36. &quot;V1187&quot;  0.000000 #########\n",
       "   37.  &quot;V239&quot;  0.000000 #########\n",
       "   38.  &quot;V935&quot;  0.000000 #########\n",
       "   39. &quot;V1239&quot;  0.000000 #########\n",
       "   40.  &quot;V833&quot;  0.000000 #########\n",
       "   41. &quot;V1256&quot;  0.000000 #########\n",
       "   42. &quot;V1356&quot;  0.000000 #########\n",
       "   43.  &quot;V857&quot;  0.000000 #########\n",
       "   44.  &quot;V257&quot;  0.000000 #########\n",
       "   45. &quot;V1226&quot;  0.000000 #########\n",
       "   46.  &quot;V911&quot;  0.000000 #########\n",
       "   47.  &quot;V925&quot;  0.000000 #########\n",
       "   48.  &quot;V630&quot;  0.000000 #########\n",
       "   49.  &quot;V913&quot; -0.000264 #########\n",
       "   50. &quot;V1181&quot; -0.000527 ########\n",
       "   51. &quot;V1068&quot; -0.000527 ########\n",
       "   52.  &quot;V871&quot; -0.000527 ########\n",
       "   53.  &quot;V195&quot; -0.000527 ########\n",
       "   54.  &quot;V955&quot; -0.000527 ########\n",
       "   55. &quot;V1523&quot; -0.000527 ########\n",
       "   56. &quot;V1344&quot; -0.000527 ########\n",
       "   57. &quot;V1189&quot; -0.000527 ########\n",
       "   58.  &quot;V616&quot; -0.000527 ########\n",
       "   59. &quot;V1553&quot; -0.000791 ########\n",
       "   60.  &quot;V445&quot; -0.000791 ########\n",
       "   61.  &quot;V637&quot; -0.000791 ########\n",
       "   62.  &quot;V158&quot; -0.000791 ########\n",
       "   63.  &quot;V648&quot; -0.001055 ########\n",
       "   64.  &quot;V394&quot; -0.001055 ########\n",
       "   65.  &quot;V517&quot; -0.001055 ########\n",
       "   66.  &quot;V418&quot; -0.001055 ########\n",
       "   67. &quot;V1411&quot; -0.001055 ########\n",
       "   68.  &quot;V147&quot; -0.001055 ########\n",
       "   69.  &quot;V190&quot; -0.001846 ######\n",
       "   70. &quot;V1042&quot; -0.001846 ######\n",
       "   71. &quot;V1460&quot; -0.001846 ######\n",
       "   72.  &quot;V625&quot; -0.001846 ######\n",
       "   73.   &quot;V28&quot; -0.001846 ######\n",
       "   74. &quot;V1281&quot; -0.002110 ######\n",
       "   75.  &quot;V367&quot; -0.002110 ######\n",
       "   76. &quot;V1619&quot; -0.002373 ######\n",
       "   77. &quot;V1630&quot; -0.002901 #####\n",
       "   78. &quot;V1286&quot; -0.003165 #####\n",
       "   79. &quot;V1199&quot; -0.003165 #####\n",
       "   80.  &quot;V512&quot; -0.005274 ##\n",
       "   81. &quot;V1191&quot; -0.006857 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_MEAN_DECREASE_IN_AP_1_VS_OTHERS\" class=\"content\"><pre class=\"ydf_pre\">    1. &quot;V1191&quot;  0.011057 ################\n",
       "    2.  &quot;V673&quot;  0.006273 #########\n",
       "    3.  &quot;V190&quot;  0.004819 #######\n",
       "    4. &quot;V1283&quot;  0.003963 ######\n",
       "    5.  &quot;V871&quot;  0.003589 ######\n",
       "    6. &quot;V1181&quot;  0.003028 #####\n",
       "    7.  &quot;V560&quot;  0.002952 #####\n",
       "    8.  &quot;V574&quot;  0.002709 ####\n",
       "    9. &quot;V1199&quot;  0.002652 ####\n",
       "   10. &quot;V1226&quot;  0.002487 ####\n",
       "   11.  &quot;V226&quot;  0.002416 ####\n",
       "   12.  &quot;V911&quot;  0.002369 ####\n",
       "   13.  &quot;V512&quot;  0.002368 ####\n",
       "   14.  &quot;V637&quot;  0.002361 ####\n",
       "   15.   &quot;V67&quot;  0.002206 ####\n",
       "   16. &quot;V1068&quot;  0.002162 ####\n",
       "   17. &quot;V1571&quot;  0.001858 ###\n",
       "   18.  &quot;V790&quot;  0.001772 ###\n",
       "   19.  &quot;V625&quot;  0.001762 ###\n",
       "   20.  &quot;V316&quot;  0.001750 ###\n",
       "   21.  &quot;V250&quot;  0.001696 ###\n",
       "   22. &quot;V1460&quot;  0.001611 ###\n",
       "   23. &quot;V1042&quot;  0.001527 ###\n",
       "   24.  &quot;V394&quot;  0.001456 ###\n",
       "   25.  &quot;V650&quot;  0.001374 ###\n",
       "   26.   &quot;V28&quot;  0.001373 ###\n",
       "   27. &quot;V1523&quot;  0.001369 ###\n",
       "   28. &quot;V1286&quot;  0.001307 ##\n",
       "   29.  &quot;V913&quot;  0.001157 ##\n",
       "   30.  &quot;V630&quot;  0.001116 ##\n",
       "   31.  &quot;V955&quot;  0.001030 ##\n",
       "   32. &quot;V1325&quot;  0.000978 ##\n",
       "   33. &quot;V1189&quot;  0.000925 ##\n",
       "   34.  &quot;V147&quot;  0.000769 ##\n",
       "   35.  &quot;V158&quot;  0.000653 ##\n",
       "   36.  &quot;V925&quot;  0.000651 ##\n",
       "   37.  &quot;V374&quot;  0.000642 ##\n",
       "   38.  &quot;V517&quot;  0.000608 ##\n",
       "   39.  &quot;V648&quot;  0.000586 ##\n",
       "   40.  &quot;V292&quot;  0.000562 ##\n",
       "   41. &quot;V1281&quot;  0.000520 #\n",
       "   42.  &quot;V569&quot;  0.000476 #\n",
       "   43.  &quot;V445&quot;  0.000470 #\n",
       "   44.  &quot;V189&quot;  0.000431 #\n",
       "   45.  &quot;V418&quot;  0.000426 #\n",
       "   46. &quot;V1485&quot;  0.000326 #\n",
       "   47. &quot;V1619&quot;  0.000275 #\n",
       "   48. &quot;V1344&quot;  0.000263 #\n",
       "   49.  &quot;V195&quot;  0.000235 #\n",
       "   50.  &quot;V616&quot;  0.000194 #\n",
       "   51. &quot;V1553&quot;  0.000183 #\n",
       "   52. &quot;V1411&quot;  0.000171 #\n",
       "   53. &quot;V1369&quot;  0.000066 #\n",
       "   54. &quot;V1319&quot;  0.000044 #\n",
       "   55.  &quot;V969&quot;  0.000037 #\n",
       "   56. &quot;V1240&quot;  0.000000 #\n",
       "   57.   &quot;V73&quot;  0.000000 #\n",
       "   58.  &quot;V478&quot;  0.000000 #\n",
       "   59.  &quot;V575&quot;  0.000000 #\n",
       "   60.  &quot;V257&quot;  0.000000 #\n",
       "   61.  &quot;V857&quot;  0.000000 #\n",
       "   62. &quot;V1356&quot;  0.000000 #\n",
       "   63. &quot;V1256&quot;  0.000000 #\n",
       "   64.  &quot;V833&quot;  0.000000 #\n",
       "   65. &quot;V1239&quot;  0.000000 #\n",
       "   66.  &quot;V935&quot;  0.000000 #\n",
       "   67.  &quot;V239&quot;  0.000000 #\n",
       "   68. &quot;V1187&quot;  0.000000 #\n",
       "   69.  &quot;V780&quot;  0.000000 #\n",
       "   70. &quot;V1010&quot;  0.000000 #\n",
       "   71. &quot;V1048&quot;  0.000000 #\n",
       "   72.  &quot;V688&quot;  0.000000 #\n",
       "   73.  &quot;V584&quot;  0.000000 #\n",
       "   74.  &quot;V647&quot;  0.000000 #\n",
       "   75.  &quot;V262&quot;  0.000000 #\n",
       "   76. &quot;V1301&quot;  0.000000 #\n",
       "   77.  &quot;V245&quot;  0.000000 #\n",
       "   78. &quot;V1487&quot;  0.000000 #\n",
       "   79.  &quot;V214&quot;  0.000000 #\n",
       "   80. &quot;V1630&quot; -0.000319 \n",
       "   81.  &quot;V367&quot; -0.000942 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_MEAN_DECREASE_IN_AUC_1_VS_OTHERS\" class=\"content\"><pre class=\"ydf_pre\">    1.  &quot;V871&quot;  0.004678 ################\n",
       "    2. &quot;V1191&quot;  0.004097 ##############\n",
       "    3.  &quot;V190&quot;  0.004058 #############\n",
       "    4.  &quot;V673&quot;  0.004000 #############\n",
       "    5. &quot;V1283&quot;  0.003335 ###########\n",
       "    6.  &quot;V226&quot;  0.002918 ##########\n",
       "    7. &quot;V1181&quot;  0.002416 ########\n",
       "    8.  &quot;V574&quot;  0.002189 #######\n",
       "    9. &quot;V1226&quot;  0.002147 #######\n",
       "   10.  &quot;V394&quot;  0.002036 #######\n",
       "   11. &quot;V1523&quot;  0.001838 ######\n",
       "   12. &quot;V1068&quot;  0.001832 ######\n",
       "   13.   &quot;V67&quot;  0.001790 ######\n",
       "   14.  &quot;V560&quot;  0.001744 ######\n",
       "   15.  &quot;V790&quot;  0.001621 #####\n",
       "   16. &quot;V1042&quot;  0.001590 #####\n",
       "   17.  &quot;V637&quot;  0.001569 #####\n",
       "   18.  &quot;V911&quot;  0.001506 #####\n",
       "   19. &quot;V1485&quot;  0.001469 #####\n",
       "   20. &quot;V1571&quot;  0.001436 #####\n",
       "   21.  &quot;V650&quot;  0.001414 ####\n",
       "   22. &quot;V1199&quot;  0.001304 ####\n",
       "   23.  &quot;V512&quot;  0.001301 ####\n",
       "   24.  &quot;V250&quot;  0.001300 ####\n",
       "   25.  &quot;V625&quot;  0.001283 ####\n",
       "   26.   &quot;V28&quot;  0.001140 ####\n",
       "   27. &quot;V1286&quot;  0.001059 ###\n",
       "   28. &quot;V1460&quot;  0.001046 ###\n",
       "   29.  &quot;V316&quot;  0.000998 ###\n",
       "   30.  &quot;V374&quot;  0.000934 ###\n",
       "   31.  &quot;V925&quot;  0.000914 ###\n",
       "   32.  &quot;V569&quot;  0.000840 ##\n",
       "   33.  &quot;V189&quot;  0.000762 ##\n",
       "   34.  &quot;V445&quot;  0.000757 ##\n",
       "   35.  &quot;V955&quot;  0.000735 ##\n",
       "   36.  &quot;V913&quot;  0.000724 ##\n",
       "   37.  &quot;V630&quot;  0.000711 ##\n",
       "   38.  &quot;V147&quot;  0.000678 ##\n",
       "   39. &quot;V1189&quot;  0.000657 ##\n",
       "   40. &quot;V1281&quot;  0.000632 ##\n",
       "   41.  &quot;V195&quot;  0.000618 ##\n",
       "   42.  &quot;V292&quot;  0.000609 ##\n",
       "   43. &quot;V1619&quot;  0.000609 ##\n",
       "   44.  &quot;V158&quot;  0.000604 ##\n",
       "   45. &quot;V1369&quot;  0.000519 #\n",
       "   46. &quot;V1411&quot;  0.000517 #\n",
       "   47.  &quot;V648&quot;  0.000499 #\n",
       "   48.  &quot;V418&quot;  0.000440 #\n",
       "   49. &quot;V1325&quot;  0.000405 #\n",
       "   50. &quot;V1630&quot;  0.000390 #\n",
       "   51. &quot;V1344&quot;  0.000254 \n",
       "   52.  &quot;V517&quot;  0.000251 \n",
       "   53.  &quot;V367&quot;  0.000202 \n",
       "   54. &quot;V1319&quot;  0.000146 \n",
       "   55. &quot;V1553&quot;  0.000061 \n",
       "   56.  &quot;V969&quot;  0.000060 \n",
       "   57. &quot;V1187&quot;  0.000000 \n",
       "   58.   &quot;V73&quot;  0.000000 \n",
       "   59. &quot;V1240&quot;  0.000000 \n",
       "   60.  &quot;V478&quot;  0.000000 \n",
       "   61.  &quot;V575&quot;  0.000000 \n",
       "   62.  &quot;V257&quot;  0.000000 \n",
       "   63.  &quot;V857&quot;  0.000000 \n",
       "   64. &quot;V1356&quot;  0.000000 \n",
       "   65. &quot;V1256&quot;  0.000000 \n",
       "   66.  &quot;V833&quot;  0.000000 \n",
       "   67. &quot;V1239&quot;  0.000000 \n",
       "   68.  &quot;V935&quot;  0.000000 \n",
       "   69.  &quot;V239&quot;  0.000000 \n",
       "   70.  &quot;V780&quot;  0.000000 \n",
       "   71. &quot;V1010&quot;  0.000000 \n",
       "   72. &quot;V1048&quot;  0.000000 \n",
       "   73.  &quot;V688&quot;  0.000000 \n",
       "   74.  &quot;V584&quot;  0.000000 \n",
       "   75.  &quot;V647&quot;  0.000000 \n",
       "   76.  &quot;V262&quot;  0.000000 \n",
       "   77. &quot;V1301&quot;  0.000000 \n",
       "   78.  &quot;V245&quot;  0.000000 \n",
       "   79. &quot;V1487&quot;  0.000000 \n",
       "   80.  &quot;V214&quot;  0.000000 \n",
       "   81.  &quot;V616&quot; -0.000041 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_MEAN_DECREASE_IN_PRAUC_1_VS_OTHERS\" class=\"content\"><pre class=\"ydf_pre\">    1. &quot;V1191&quot;  0.010944 ################\n",
       "    2.  &quot;V673&quot;  0.006251 #########\n",
       "    3.  &quot;V190&quot;  0.004536 #######\n",
       "    4. &quot;V1283&quot;  0.003963 ######\n",
       "    5.  &quot;V871&quot;  0.003591 ######\n",
       "    6. &quot;V1181&quot;  0.003029 #####\n",
       "    7.  &quot;V560&quot;  0.002955 #####\n",
       "    8.  &quot;V574&quot;  0.002724 ####\n",
       "    9. &quot;V1199&quot;  0.002633 ####\n",
       "   10. &quot;V1226&quot;  0.002472 ####\n",
       "   11.  &quot;V226&quot;  0.002427 ####\n",
       "   12.  &quot;V911&quot;  0.002376 ####\n",
       "   13.  &quot;V637&quot;  0.002369 ####\n",
       "   14.  &quot;V512&quot;  0.002345 ####\n",
       "   15.   &quot;V67&quot;  0.002206 ####\n",
       "   16. &quot;V1068&quot;  0.002153 ####\n",
       "   17. &quot;V1571&quot;  0.001833 ###\n",
       "   18.  &quot;V790&quot;  0.001776 ###\n",
       "   19.  &quot;V625&quot;  0.001775 ###\n",
       "   20.  &quot;V316&quot;  0.001746 ###\n",
       "   21.  &quot;V250&quot;  0.001698 ###\n",
       "   22. &quot;V1460&quot;  0.001620 ###\n",
       "   23. &quot;V1042&quot;  0.001530 ###\n",
       "   24.  &quot;V394&quot;  0.001466 ###\n",
       "   25.   &quot;V28&quot;  0.001395 ###\n",
       "   26.  &quot;V650&quot;  0.001382 ###\n",
       "   27. &quot;V1523&quot;  0.001360 ###\n",
       "   28. &quot;V1286&quot;  0.001280 ##\n",
       "   29.  &quot;V913&quot;  0.001185 ##\n",
       "   30.  &quot;V630&quot;  0.001102 ##\n",
       "   31.  &quot;V955&quot;  0.001020 ##\n",
       "   32. &quot;V1325&quot;  0.000978 ##\n",
       "   33. &quot;V1189&quot;  0.000926 ##\n",
       "   34.  &quot;V147&quot;  0.000757 ##\n",
       "   35.  &quot;V925&quot;  0.000651 ##\n",
       "   36.  &quot;V158&quot;  0.000651 ##\n",
       "   37.  &quot;V517&quot;  0.000621 ##\n",
       "   38.  &quot;V374&quot;  0.000612 ##\n",
       "   39.  &quot;V648&quot;  0.000602 ##\n",
       "   40.  &quot;V292&quot;  0.000556 ##\n",
       "   41. &quot;V1281&quot;  0.000551 #\n",
       "   42.  &quot;V445&quot;  0.000482 #\n",
       "   43.  &quot;V569&quot;  0.000473 #\n",
       "   44.  &quot;V418&quot;  0.000443 #\n",
       "   45.  &quot;V189&quot;  0.000442 #\n",
       "   46. &quot;V1485&quot;  0.000345 #\n",
       "   47. &quot;V1619&quot;  0.000286 #\n",
       "   48. &quot;V1344&quot;  0.000262 #\n",
       "   49.  &quot;V195&quot;  0.000226 #\n",
       "   50.  &quot;V616&quot;  0.000201 #\n",
       "   51. &quot;V1553&quot;  0.000180 #\n",
       "   52. &quot;V1411&quot;  0.000178 #\n",
       "   53. &quot;V1369&quot;  0.000097 #\n",
       "   54. &quot;V1319&quot;  0.000043 #\n",
       "   55.  &quot;V969&quot;  0.000036 #\n",
       "   56.  &quot;V935&quot;  0.000000 #\n",
       "   57.  &quot;V647&quot;  0.000000 #\n",
       "   58. &quot;V1239&quot;  0.000000 #\n",
       "   59.  &quot;V833&quot;  0.000000 #\n",
       "   60. &quot;V1256&quot;  0.000000 #\n",
       "   61.  &quot;V239&quot;  0.000000 #\n",
       "   62. &quot;V1356&quot;  0.000000 #\n",
       "   63.  &quot;V857&quot;  0.000000 #\n",
       "   64.  &quot;V257&quot;  0.000000 #\n",
       "   65.  &quot;V575&quot;  0.000000 #\n",
       "   66.  &quot;V478&quot;  0.000000 #\n",
       "   67. &quot;V1240&quot;  0.000000 #\n",
       "   68.   &quot;V73&quot;  0.000000 #\n",
       "   69. &quot;V1187&quot;  0.000000 #\n",
       "   70.  &quot;V780&quot;  0.000000 #\n",
       "   71. &quot;V1010&quot;  0.000000 #\n",
       "   72. &quot;V1048&quot;  0.000000 #\n",
       "   73.  &quot;V688&quot;  0.000000 #\n",
       "   74.  &quot;V584&quot;  0.000000 #\n",
       "   75.  &quot;V262&quot;  0.000000 #\n",
       "   76. &quot;V1301&quot;  0.000000 #\n",
       "   77.  &quot;V245&quot;  0.000000 #\n",
       "   78. &quot;V1487&quot;  0.000000 #\n",
       "   79.  &quot;V214&quot;  0.000000 #\n",
       "   80. &quot;V1630&quot; -0.000293 \n",
       "   81.  &quot;V367&quot; -0.000931 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_NUM_AS_ROOT\" class=\"content\"><pre class=\"ydf_pre\">    1.  &quot;V512&quot; 49.000000 ################\n",
       "    2. &quot;V1191&quot; 48.000000 ###############\n",
       "    3.  &quot;V871&quot; 31.000000 ##########\n",
       "    4.  &quot;V190&quot; 31.000000 ##########\n",
       "    5.  &quot;V226&quot; 23.000000 #######\n",
       "    6.  &quot;V673&quot; 21.000000 ######\n",
       "    7. &quot;V1485&quot; 17.000000 #####\n",
       "    8. &quot;V1283&quot; 13.000000 ####\n",
       "    9. &quot;V1068&quot; 13.000000 ####\n",
       "   10. &quot;V1181&quot; 13.000000 ####\n",
       "   11.  &quot;V574&quot;  7.000000 ##\n",
       "   12.  &quot;V394&quot;  6.000000 #\n",
       "   13. &quot;V1199&quot;  5.000000 #\n",
       "   14.  &quot;V250&quot;  5.000000 #\n",
       "   15.  &quot;V367&quot;  4.000000 #\n",
       "   16. &quot;V1042&quot;  2.000000 \n",
       "   17. &quot;V1553&quot;  2.000000 \n",
       "   18. &quot;V1571&quot;  2.000000 \n",
       "   19.  &quot;V925&quot;  2.000000 \n",
       "   20. &quot;V1226&quot;  1.000000 \n",
       "   21.  &quot;V911&quot;  1.000000 \n",
       "   22. &quot;V1460&quot;  1.000000 \n",
       "   23. &quot;V1411&quot;  1.000000 \n",
       "   24. &quot;V1523&quot;  1.000000 \n",
       "   25. &quot;V1630&quot;  1.000000 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_NUM_NODES\" class=\"content\"><pre class=\"ydf_pre\">    1. &quot;V1191&quot; 2275.000000 ################\n",
       "    2.  &quot;V871&quot; 2255.000000 ###############\n",
       "    3.  &quot;V512&quot; 2132.000000 ##############\n",
       "    4.  &quot;V574&quot; 2130.000000 ##############\n",
       "    5.  &quot;V673&quot; 2096.000000 ##############\n",
       "    6.  &quot;V190&quot; 2087.000000 ##############\n",
       "    7.  &quot;V226&quot; 2081.000000 ##############\n",
       "    8. &quot;V1199&quot; 1956.000000 #############\n",
       "    9. &quot;V1485&quot; 1925.000000 #############\n",
       "   10. &quot;V1181&quot; 1920.000000 #############\n",
       "   11. &quot;V1226&quot; 1875.000000 #############\n",
       "   12. &quot;V1523&quot; 1870.000000 #############\n",
       "   13. &quot;V1460&quot; 1862.000000 #############\n",
       "   14. &quot;V1619&quot; 1854.000000 #############\n",
       "   15.  &quot;V911&quot; 1851.000000 #############\n",
       "   16. &quot;V1411&quot; 1818.000000 ############\n",
       "   17.  &quot;V394&quot; 1814.000000 ############\n",
       "   18. &quot;V1553&quot; 1789.000000 ############\n",
       "   19. &quot;V1571&quot; 1774.000000 ############\n",
       "   20.  &quot;V790&quot; 1773.000000 ############\n",
       "   21.  &quot;V445&quot; 1771.000000 ############\n",
       "   22.  &quot;V250&quot; 1762.000000 ############\n",
       "   23.  &quot;V637&quot; 1745.000000 ############\n",
       "   24.  &quot;V925&quot; 1724.000000 ############\n",
       "   25.   &quot;V28&quot; 1712.000000 ############\n",
       "   26.  &quot;V418&quot; 1711.000000 ############\n",
       "   27.  &quot;V316&quot; 1692.000000 ###########\n",
       "   28. &quot;V1283&quot; 1683.000000 ###########\n",
       "   29.  &quot;V913&quot; 1682.000000 ###########\n",
       "   30. &quot;V1369&quot; 1674.000000 ###########\n",
       "   31. &quot;V1325&quot; 1673.000000 ###########\n",
       "   32.  &quot;V650&quot; 1669.000000 ###########\n",
       "   33.  &quot;V955&quot; 1646.000000 ###########\n",
       "   34.  &quot;V569&quot; 1626.000000 ###########\n",
       "   35. &quot;V1286&quot; 1620.000000 ###########\n",
       "   36.  &quot;V147&quot; 1616.000000 ###########\n",
       "   37. &quot;V1630&quot; 1615.000000 ###########\n",
       "   38.  &quot;V648&quot; 1614.000000 ###########\n",
       "   39.  &quot;V367&quot; 1590.000000 ###########\n",
       "   40.  &quot;V374&quot; 1542.000000 ##########\n",
       "   41.  &quot;V292&quot; 1542.000000 ##########\n",
       "   42. &quot;V1281&quot; 1522.000000 ##########\n",
       "   43. &quot;V1042&quot; 1431.000000 ##########\n",
       "   44.  &quot;V189&quot; 1340.000000 #########\n",
       "   45.   &quot;V67&quot; 1310.000000 #########\n",
       "   46.  &quot;V560&quot; 1224.000000 ########\n",
       "   47. &quot;V1068&quot; 1110.000000 #######\n",
       "   48.  &quot;V630&quot; 971.000000 ######\n",
       "   49.  &quot;V625&quot; 969.000000 ######\n",
       "   50.  &quot;V195&quot; 901.000000 ######\n",
       "   51.  &quot;V517&quot; 677.000000 ####\n",
       "   52. &quot;V1189&quot; 505.000000 ###\n",
       "   53.  &quot;V158&quot; 432.000000 ##\n",
       "   54. &quot;V1344&quot; 296.000000 ##\n",
       "   55.  &quot;V616&quot; 204.000000 #\n",
       "   56. &quot;V1319&quot; 85.000000 \n",
       "   57.  &quot;V969&quot; 13.000000 \n",
       "</pre></div><div id=\"57d7-96f5-1cb9-4452_vi_body_SUM_SCORE\" class=\"content\"><pre class=\"ydf_pre\">    1. &quot;V1191&quot; 36645.825576 ################\n",
       "    2.  &quot;V512&quot; 31466.855118 #############\n",
       "    3.  &quot;V871&quot; 28373.888026 ############\n",
       "    4.  &quot;V673&quot; 22923.646195 #########\n",
       "    5.  &quot;V226&quot; 21574.990042 #########\n",
       "    6.  &quot;V190&quot; 21448.847693 #########\n",
       "    7. &quot;V1485&quot; 17756.433852 #######\n",
       "    8.  &quot;V574&quot; 16608.420783 #######\n",
       "    9. &quot;V1181&quot; 15829.372280 ######\n",
       "   10. &quot;V1283&quot; 14739.189821 ######\n",
       "   11. &quot;V1199&quot; 13802.414297 #####\n",
       "   12. &quot;V1226&quot; 13191.231814 #####\n",
       "   13. &quot;V1411&quot; 12818.601259 #####\n",
       "   14. &quot;V1553&quot; 11883.399495 #####\n",
       "   15.  &quot;V911&quot; 11856.001513 #####\n",
       "   16.  &quot;V394&quot; 11845.890281 #####\n",
       "   17. &quot;V1571&quot; 11507.955149 ####\n",
       "   18. &quot;V1460&quot; 11150.970524 ####\n",
       "   19.  &quot;V250&quot; 11007.265611 ####\n",
       "   20.  &quot;V637&quot; 10673.264952 ####\n",
       "   21. &quot;V1523&quot; 10598.071593 ####\n",
       "   22.   &quot;V28&quot; 10485.412553 ####\n",
       "   23.  &quot;V790&quot; 10097.421935 ####\n",
       "   24. &quot;V1619&quot; 10044.966466 ####\n",
       "   25.  &quot;V913&quot; 10022.922022 ####\n",
       "   26.  &quot;V367&quot; 9934.350338 ####\n",
       "   27.  &quot;V418&quot; 9745.277643 ####\n",
       "   28.  &quot;V925&quot; 9744.095508 ####\n",
       "   29. &quot;V1325&quot; 9715.455120 ####\n",
       "   30. &quot;V1286&quot; 9646.166967 ####\n",
       "   31. &quot;V1369&quot; 9541.837708 ####\n",
       "   32.  &quot;V648&quot; 9540.976848 ####\n",
       "   33. &quot;V1042&quot; 9534.211491 ####\n",
       "   34.  &quot;V569&quot; 9463.900537 ####\n",
       "   35. &quot;V1630&quot; 9407.778988 ####\n",
       "   36.  &quot;V445&quot; 9359.857176 ####\n",
       "   37.  &quot;V316&quot; 9249.789360 ###\n",
       "   38.  &quot;V650&quot; 9205.577760 ###\n",
       "   39.   &quot;V67&quot; 9109.293697 ###\n",
       "   40. &quot;V1068&quot; 9083.394089 ###\n",
       "   41.  &quot;V147&quot; 8962.366481 ###\n",
       "   42.  &quot;V955&quot; 8923.564473 ###\n",
       "   43.  &quot;V374&quot; 8846.483943 ###\n",
       "   44. &quot;V1281&quot; 8738.265529 ###\n",
       "   45.  &quot;V292&quot; 8569.927453 ###\n",
       "   46.  &quot;V560&quot; 7837.619595 ###\n",
       "   47.  &quot;V189&quot; 7607.412452 ###\n",
       "   48.  &quot;V630&quot; 5558.551227 ##\n",
       "   49.  &quot;V625&quot; 5186.007513 ##\n",
       "   50.  &quot;V195&quot; 5074.247309 ##\n",
       "   51.  &quot;V517&quot; 3842.728664 #\n",
       "   52.  &quot;V158&quot; 2983.182893 #\n",
       "   53. &quot;V1189&quot; 2784.258937 #\n",
       "   54. &quot;V1344&quot; 1655.575145 \n",
       "   55.  &quot;V616&quot; 1406.545845 \n",
       "   56. &quot;V1319&quot; 580.462978 \n",
       "   57.  &quot;V969&quot; 127.331590 \n",
       "</pre></div></div><p>Those variable importances are computed during training. More, and possibly more informative, variable importances are available when analyzing a model on a test dataset.</p></div><div id=\"57d7-96f5-1cb9-4452_body_structure\" class=\"tab_content\"><b>Num trees</b> : 300<br><p>Only printing the first tree.</p><pre class=\"ydf_pre\">Tree #0:\n",
       "    &quot;V1283&quot;&gt;=29.5 [s:0.0326858 n:3792 np:2707 miss:1] ; val:&quot;1&quot; prob:[0.490243, 0.509757]\n",
       "        â”œâ”€(pos)â”€ &quot;V226&quot;&gt;=308.5 [s:0.0481086 n:2707 np:769 miss:0] ; val:&quot;1&quot; prob:[0.410048, 0.589952]\n",
       "        |        â”œâ”€(pos)â”€ &quot;V574&quot;&gt;=631.5 [s:0.0207906 n:769 np:110 miss:0] ; val:&quot;0&quot; prob:[0.652796, 0.347204]\n",
       "        |        |        â”œâ”€(pos)â”€ &quot;V189&quot;&gt;=387.5 [s:0.0743645 n:110 np:27 miss:0] ; val:&quot;1&quot; prob:[0.409091, 0.590909]\n",
       "        |        |        |        â”œâ”€(pos)â”€ &quot;V790&quot;&gt;=82.5 [s:0.110739 n:27 np:18 miss:1] ; val:&quot;0&quot; prob:[0.740741, 0.259259]\n",
       "        |        |        |        |        â”œâ”€(pos)â”€ &quot;V445&quot;&gt;=405.5 [s:0.161884 n:18 np:5 miss:0] ; val:&quot;0&quot; prob:[0.888889, 0.111111]\n",
       "        |        |        |        |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |        |        |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.444444, 0.555556]\n",
       "        |        |        |        â””â”€(neg)â”€ &quot;V1042&quot;&gt;=243.5 [s:0.0611636 n:83 np:38 miss:0] ; val:&quot;1&quot; prob:[0.301205, 0.698795]\n",
       "        |        |        |                 â”œâ”€(pos)â”€ &quot;V1553&quot;&gt;=331 [s:0.127867 n:38 np:22 miss:0] ; val:&quot;1&quot; prob:[0.131579, 0.868421]\n",
       "        |        |        |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |        |                 |        â””â”€(neg)â”€ &quot;V925&quot;&gt;=134.5 [s:0.147392 n:16 np:5 miss:1] ; val:&quot;1&quot; prob:[0.3125, 0.6875]\n",
       "        |        |        |                 |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |        |                 |                 â””â”€(neg)â”€ &quot;V147&quot;&gt;=95.5 [s:0.114364 n:11 np:6 miss:0] ; val:&quot;1&quot; prob:[0.454545, 0.545455]\n",
       "        |        |        |                 |                          â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.666667, 0.333333]\n",
       "        |        |        |                 |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |        |        |                 â””â”€(neg)â”€ &quot;V569&quot;&gt;=369.5 [s:0.168891 n:45 np:37 miss:1] ; val:&quot;1&quot; prob:[0.444444, 0.555556]\n",
       "        |        |        |                          â”œâ”€(pos)â”€ &quot;V512&quot;&gt;=60 [s:0.145091 n:37 np:26 miss:1] ; val:&quot;1&quot; prob:[0.324324, 0.675676]\n",
       "        |        |        |                          |        â”œâ”€(pos)â”€ &quot;V1283&quot;&gt;=70 [s:0.209259 n:26 np:19 miss:1] ; val:&quot;1&quot; prob:[0.461538, 0.538462]\n",
       "        |        |        |                          |        |        â”œâ”€(pos)â”€ &quot;V1199&quot;&gt;=483 [s:0.49947 n:19 np:8 miss:0] ; val:&quot;0&quot; prob:[0.631579, 0.368421]\n",
       "        |        |        |                          |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.125, 0.875]\n",
       "        |        |        |                          |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |        |                          |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |        |                          |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |        |                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |        â””â”€(neg)â”€ &quot;V1042&quot;&gt;=442.5 [s:0.0258329 n:659 np:70 miss:0] ; val:&quot;0&quot; prob:[0.693475, 0.306525]\n",
       "        |        |                 â”œâ”€(pos)â”€ &quot;V1369&quot;&gt;=236.5 [s:0.0585335 n:70 np:19 miss:0] ; val:&quot;0&quot; prob:[0.957143, 0.0428571]\n",
       "        |        |                 |        â”œâ”€(pos)â”€ &quot;V1369&quot;&gt;=286 [s:0.157609 n:19 np:11 miss:0] ; val:&quot;0&quot; prob:[0.842105, 0.157895]\n",
       "        |        |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                 |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.625, 0.375]\n",
       "        |        |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                 â””â”€(neg)â”€ &quot;V630&quot;&gt;=515 [s:0.0189763 n:589 np:143 miss:0] ; val:&quot;0&quot; prob:[0.662139, 0.337861]\n",
       "        |        |                          â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=187 [s:0.0452712 n:143 np:9 miss:0] ; val:&quot;1&quot; prob:[0.496503, 0.503497]\n",
       "        |        |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                          |        â””â”€(neg)â”€ &quot;V648&quot;&gt;=84.5 [s:0.0500566 n:134 np:49 miss:0] ; val:&quot;0&quot; prob:[0.529851, 0.470149]\n",
       "        |        |                          |                 â”œâ”€(pos)â”€ &quot;V1619&quot;&gt;=289.5 [s:0.140299 n:49 np:32 miss:1] ; val:&quot;0&quot; prob:[0.734694, 0.265306]\n",
       "        |        |                          |                 |        â”œâ”€(pos)â”€ &quot;V1630&quot;&gt;=289.5 [s:0.181164 n:32 np:6 miss:0] ; val:&quot;0&quot; prob:[0.90625, 0.09375]\n",
       "        |        |                          |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |        |                          |                 |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                          |                 |        â””â”€(neg)â”€ &quot;V673&quot;&gt;=262 [s:0.359451 n:17 np:5 miss:0] ; val:&quot;1&quot; prob:[0.411765, 0.588235]\n",
       "        |        |                          |                 |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                          |                 |                 â””â”€(neg)â”€ &quot;V913&quot;&gt;=495.5 [s:0.17014 n:12 np:7 miss:0] ; val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "        |        |                          |                 |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                          |                 |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |        |                          |                 â””â”€(neg)â”€ &quot;V292&quot;&gt;=502 [s:0.075181 n:85 np:18 miss:0] ; val:&quot;1&quot; prob:[0.411765, 0.588235]\n",
       "        |        |                          |                          â”œâ”€(pos)â”€ &quot;V1630&quot;&gt;=314 [s:0.317535 n:18 np:6 miss:0] ; val:&quot;0&quot; prob:[0.777778, 0.222222]\n",
       "        |        |                          |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "        |        |                          |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                          |                          â””â”€(neg)â”€ &quot;V1281&quot;&gt;=16.5 [s:0.0743026 n:67 np:26 miss:1] ; val:&quot;1&quot; prob:[0.313433, 0.686567]\n",
       "        |        |                          |                                   â”œâ”€(pos)â”€ &quot;V560&quot;&gt;=72 [s:0.165212 n:26 np:8 miss:0] ; val:&quot;0&quot; prob:[0.538462, 0.461538]\n",
       "        |        |                          |                                   |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.125, 0.875]\n",
       "        |        |                          |                                   |        â””â”€(neg)â”€ &quot;V913&quot;&gt;=389.5 [s:0.296814 n:18 np:10 miss:1] ; val:&quot;0&quot; prob:[0.722222, 0.277778]\n",
       "        |        |                          |                                   |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                          |                                   |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.375, 0.625]\n",
       "        |        |                          |                                   â””â”€(neg)â”€ &quot;V1042&quot;&gt;=187.5 [s:0.152067 n:41 np:19 miss:1] ; val:&quot;1&quot; prob:[0.170732, 0.829268]\n",
       "        |        |                          |                                            â”œâ”€(pos)â”€ &quot;V1553&quot;&gt;=215.5 [s:0.15313 n:19 np:7 miss:1] ; val:&quot;1&quot; prob:[0.368421, 0.631579]\n",
       "        |        |                          |                                            |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.714286, 0.285714]\n",
       "        |        |                          |                                            |        â””â”€(neg)â”€ &quot;V1325&quot;&gt;=357.5 [s:0.132304 n:12 np:6 miss:0] ; val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "        |        |                          |                                            |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                          |                                            |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "        |        |                          |                                            â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                          â””â”€(neg)â”€ &quot;V512&quot;&gt;=40.5 [s:0.040361 n:446 np:399 miss:1] ; val:&quot;0&quot; prob:[0.715247, 0.284753]\n",
       "        |        |                                   â”œâ”€(pos)â”€ &quot;V445&quot;&gt;=83 [s:0.021925 n:399 np:382 miss:1] ; val:&quot;0&quot; prob:[0.761905, 0.238095]\n",
       "        |        |                                   |        â”œâ”€(pos)â”€ &quot;V67&quot;&gt;=7.5 [s:0.0190823 n:382 np:323 miss:1] ; val:&quot;0&quot; prob:[0.782723, 0.217277]\n",
       "        |        |                                   |        |        â”œâ”€(pos)â”€ &quot;V637&quot;&gt;=194 [s:0.0160351 n:323 np:214 miss:0] ; val:&quot;0&quot; prob:[0.752322, 0.247678]\n",
       "        |        |                                   |        |        |        â”œâ”€(pos)â”€ &quot;V28&quot;&gt;=377.5 [s:0.0476598 n:214 np:45 miss:0] ; val:&quot;0&quot; prob:[0.808411, 0.191589]\n",
       "        |        |                                   |        |        |        |        â”œâ”€(pos)â”€ &quot;V574&quot;&gt;=532.5 [s:0.159217 n:45 np:16 miss:0] ; val:&quot;0&quot; prob:[0.555556, 0.444444]\n",
       "        |        |                                   |        |        |        |        |        â”œâ”€(pos)â”€ &quot;V574&quot;&gt;=552 [s:0.183805 n:16 np:7 miss:0] ; val:&quot;1&quot; prob:[0.1875, 0.8125]\n",
       "        |        |                                   |        |        |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.428571, 0.571429]\n",
       "        |        |                                   |        |        |        |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                   |        |        |        |        |        â””â”€(neg)â”€ &quot;V637&quot;&gt;=280.5 [s:0.271619 n:29 np:12 miss:0] ; val:&quot;0&quot; prob:[0.758621, 0.241379]\n",
       "        |        |                                   |        |        |        |        |                 â”œâ”€(pos)â”€ &quot;V673&quot;&gt;=157 [s:0.679193 n:12 np:7 miss:1] ; val:&quot;1&quot; prob:[0.416667, 0.583333]\n",
       "        |        |                                   |        |        |        |        |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                   |        |        |        |        |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |        â””â”€(neg)â”€ &quot;V1042&quot;&gt;=121 [s:0.0329309 n:169 np:138 miss:1] ; val:&quot;0&quot; prob:[0.87574, 0.12426]\n",
       "        |        |                                   |        |        |        |                 â”œâ”€(pos)â”€ &quot;V625&quot;&gt;=5.5 [s:0.0286698 n:138 np:40 miss:1] ; val:&quot;0&quot; prob:[0.92029, 0.0797101]\n",
       "        |        |                                   |        |        |        |                 |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |                 |        â””â”€(neg)â”€ &quot;V871&quot;&gt;=454.5 [s:0.0515282 n:98 np:34 miss:0] ; val:&quot;0&quot; prob:[0.887755, 0.112245]\n",
       "        |        |                                   |        |        |        |                 |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |                 |                 â””â”€(neg)â”€ &quot;V1042&quot;&gt;=378 [s:0.058555 n:64 np:18 miss:0] ; val:&quot;0&quot; prob:[0.828125, 0.171875]\n",
       "        |        |                                   |        |        |        |                 |                          â”œâ”€(pos)â”€ &quot;V1191&quot;&gt;=508.5 [s:0.175575 n:18 np:9 miss:0] ; val:&quot;0&quot; prob:[0.611111, 0.388889]\n",
       "        |        |                                   |        |        |        |                 |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.888889, 0.111111]\n",
       "        |        |                                   |        |        |        |                 |                          |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "        |        |                                   |        |        |        |                 |                          â””â”€(neg)â”€ &quot;V1199&quot;&gt;=422.5 [s:0.0938064 n:46 np:29 miss:0] ; val:&quot;0&quot; prob:[0.913043, 0.0869565]\n",
       "        |        |                                   |        |        |        |                 |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |                 |                                   â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.764706, 0.235294]\n",
       "        |        |                                   |        |        |        |                 â””â”€(neg)â”€ &quot;V226&quot;&gt;=402.5 [s:0.133427 n:31 np:14 miss:0] ; val:&quot;0&quot; prob:[0.677419, 0.322581]\n",
       "        |        |                                   |        |        |        |                          â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=142.5 [s:0.0786035 n:14 np:5 miss:1] ; val:&quot;0&quot; prob:[0.928571, 0.0714286]\n",
       "        |        |                                   |        |        |        |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |        |                                   |        |        |        |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |                          â””â”€(neg)â”€ &quot;V925&quot;&gt;=142.5 [s:0.38462 n:17 np:6 miss:1] ; val:&quot;1&quot; prob:[0.470588, 0.529412]\n",
       "        |        |                                   |        |        |        |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |        |                                   â””â”€(neg)â”€ &quot;V569&quot;&gt;=377 [s:0.168225 n:11 np:5 miss:1] ; val:&quot;1&quot; prob:[0.181818, 0.818182]\n",
       "        |        |                                   |        |        |        |                                            â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |        |                                   |        |        |        |                                            â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                   |        |        |        â””â”€(neg)â”€ &quot;V445&quot;&gt;=331.5 [s:0.0479538 n:109 np:34 miss:0] ; val:&quot;0&quot; prob:[0.642202, 0.357798]\n",
       "        |        |                                   |        |        |                 â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=125.5 [s:0.0973661 n:34 np:24 miss:1] ; val:&quot;0&quot; prob:[0.852941, 0.147059]\n",
       "        |        |                                   |        |        |                 |        â”œâ”€(pos)â”€ &quot;V512&quot;&gt;=78 [s:0.0689547 n:24 np:19 miss:1] ; val:&quot;0&quot; prob:[0.958333, 0.0416667]\n",
       "        |        |                                   |        |        |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |                 |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |        |                                   |        |        |                 |        â””â”€(neg)â”€ &quot;V1281&quot;&gt;=11 [s:0.42281 n:10 np:5 miss:1] ; val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |        |                                   |        |        |                 |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |        |                                   |        |        |                 |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |                 â””â”€(neg)â”€ &quot;V1181&quot;&gt;=608 [s:0.0675164 n:75 np:6 miss:0] ; val:&quot;0&quot; prob:[0.546667, 0.453333]\n",
       "        |        |                                   |        |        |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                   |        |        |                          â””â”€(neg)â”€ &quot;V195&quot;&gt;=14.5 [s:0.12922 n:69 np:20 miss:0] ; val:&quot;0&quot; prob:[0.594203, 0.405797]\n",
       "        |        |                                   |        |        |                                   â”œâ”€(pos)â”€ &quot;V1199&quot;&gt;=238 [s:0.0734147 n:20 np:15 miss:1] ; val:&quot;0&quot; prob:[0.95, 0.05]\n",
       "        |        |                                   |        |        |                                   |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |                                   |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |        |                                   |        |        |                                   â””â”€(neg)â”€ &quot;V913&quot;&gt;=570 [s:0.173172 n:49 np:9 miss:0] ; val:&quot;1&quot; prob:[0.44898, 0.55102]\n",
       "        |        |                                   |        |        |                                            â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |                                            â””â”€(neg)â”€ &quot;V1369&quot;&gt;=135.5 [s:0.119999 n:40 np:23 miss:1] ; val:&quot;1&quot; prob:[0.325, 0.675]\n",
       "        |        |                                   |        |        |                                                     â”œâ”€(pos)â”€ &quot;V569&quot;&gt;=398 [s:0.240906 n:23 np:18 miss:1] ; val:&quot;1&quot; prob:[0.130435, 0.869565]\n",
       "        |        |                                   |        |        |                                                     |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                   |        |        |                                                     |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |        |                                   |        |        |                                                     â””â”€(neg)â”€ &quot;V226&quot;&gt;=351 [s:0.397062 n:17 np:8 miss:0] ; val:&quot;0&quot; prob:[0.588235, 0.411765]\n",
       "        |        |                                   |        |        |                                                              â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        |        |                                                              â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.222222, 0.777778]\n",
       "        |        |                                   |        |        â””â”€(neg)â”€ &quot;V574&quot;&gt;=608.5 [s:0.143968 n:59 np:5 miss:0] ; val:&quot;0&quot; prob:[0.949153, 0.0508475]\n",
       "        |        |                                   |        |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |        |                                   |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                   |        â””â”€(neg)â”€ &quot;V871&quot;&gt;=430 [s:0.256148 n:17 np:5 miss:0] ; val:&quot;1&quot; prob:[0.294118, 0.705882]\n",
       "        |        |                                   |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |        |                                   |                 â””â”€(neg)â”€ &quot;V394&quot;&gt;=261 [s:0.0783349 n:12 np:7 miss:1] ; val:&quot;1&quot; prob:[0.0833333, 0.916667]\n",
       "        |        |                                   |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                   |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |        |                                   â””â”€(neg)â”€ &quot;V925&quot;&gt;=99.5 [s:0.22509 n:47 np:31 miss:1] ; val:&quot;1&quot; prob:[0.319149, 0.680851]\n",
       "        |        |                                            â”œâ”€(pos)â”€ &quot;V637&quot;&gt;=233.5 [s:0.209387 n:31 np:5 miss:0] ; val:&quot;1&quot; prob:[0.0967742, 0.903226]\n",
       "        |        |                                            |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |        |                                            |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |        |                                            â””â”€(neg)â”€ &quot;V226&quot;&gt;=377 [s:0.175919 n:16 np:7 miss:0] ; val:&quot;0&quot; prob:[0.75, 0.25]\n",
       "        |        |                                                     â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |        |                                                     â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.555556, 0.444444]\n",
       "        |        â””â”€(neg)â”€ &quot;V1181&quot;&gt;=230.5 [s:0.0171625 n:1938 np:1811 miss:1] ; val:&quot;1&quot; prob:[0.313725, 0.686275]\n",
       "        |                 â”œâ”€(pos)â”€ &quot;V190&quot;&gt;=224.5 [s:0.0382685 n:1811 np:1556 miss:1] ; val:&quot;1&quot; prob:[0.289895, 0.710105]\n",
       "        |                 |        â”œâ”€(pos)â”€ &quot;V1553&quot;&gt;=143 [s:0.0165587 n:1556 np:1008 miss:1] ; val:&quot;1&quot; prob:[0.236504, 0.763496]\n",
       "        |                 |        |        â”œâ”€(pos)â”€ &quot;V1191&quot;&gt;=320.5 [s:0.0258361 n:1008 np:604 miss:1] ; val:&quot;1&quot; prob:[0.291667, 0.708333]\n",
       "        |                 |        |        |        â”œâ”€(pos)â”€ &quot;V1281&quot;&gt;=19.5 [s:0.0189604 n:604 np:143 miss:0] ; val:&quot;1&quot; prob:[0.374172, 0.625828]\n",
       "        |                 |        |        |        |        â”œâ”€(pos)â”€ &quot;V28&quot;&gt;=320 [s:0.0486455 n:143 np:29 miss:0] ; val:&quot;0&quot; prob:[0.545455, 0.454545]\n",
       "        |                 |        |        |        |        |        â”œâ”€(pos)â”€ &quot;V574&quot;&gt;=490 [s:0.304034 n:29 np:18 miss:1] ; val:&quot;1&quot; prob:[0.241379, 0.758621]\n",
       "        |                 |        |        |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |        |        |        â””â”€(neg)â”€ &quot;V250&quot;&gt;=184.5 [s:0.428026 n:11 np:5 miss:1] ; val:&quot;0&quot; prob:[0.636364, 0.363636]\n",
       "        |                 |        |        |        |        |        |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |        |        |        |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |        |        â””â”€(neg)â”€ &quot;V1553&quot;&gt;=272 [s:0.0788389 n:114 np:55 miss:0] ; val:&quot;0&quot; prob:[0.622807, 0.377193]\n",
       "        |                 |        |        |        |        |                 â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=117.5 [s:0.102568 n:55 np:41 miss:1] ; val:&quot;0&quot; prob:[0.818182, 0.181818]\n",
       "        |                 |        |        |        |        |                 |        â”œâ”€(pos)â”€ &quot;V1630&quot;&gt;=227.5 [s:0.0904802 n:41 np:13 miss:1] ; val:&quot;0&quot; prob:[0.926829, 0.0731707]\n",
       "        |                 |        |        |        |        |                 |        |        â”œâ”€(pos)â”€ &quot;V1369&quot;&gt;=189 [s:0.172484 n:13 np:6 miss:0] ; val:&quot;0&quot; prob:[0.769231, 0.230769]\n",
       "        |                 |        |        |        |        |                 |        |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |        |                 |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.571429, 0.428571]\n",
       "        |                 |        |        |        |        |                 |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |        |                 |        â””â”€(neg)â”€ &quot;V189&quot;&gt;=366 [s:0.283031 n:14 np:7 miss:0] ; val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                 |        |        |        |        |                 |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.857143, 0.142857]\n",
       "        |                 |        |        |        |        |                 |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.142857, 0.857143]\n",
       "        |                 |        |        |        |        |                 â””â”€(neg)â”€ &quot;V1181&quot;&gt;=264.5 [s:0.152324 n:59 np:46 miss:1] ; val:&quot;1&quot; prob:[0.440678, 0.559322]\n",
       "        |                 |        |        |        |        |                          â”œâ”€(pos)â”€ &quot;V394&quot;&gt;=234.5 [s:0.146994 n:46 np:36 miss:1] ; val:&quot;0&quot; prob:[0.565217, 0.434783]\n",
       "        |                 |        |        |        |        |                          |        â”œâ”€(pos)â”€ &quot;V871&quot;&gt;=293.5 [s:0.211741 n:36 np:17 miss:1] ; val:&quot;1&quot; prob:[0.444444, 0.555556]\n",
       "        |                 |        |        |        |        |                          |        |        â”œâ”€(pos)â”€ &quot;V1553&quot;&gt;=158.5 [s:0.164266 n:17 np:12 miss:1] ; val:&quot;1&quot; prob:[0.117647, 0.882353]\n",
       "        |                 |        |        |        |        |                          |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |        |                          |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |                 |        |        |        |        |                          |        |        â””â”€(neg)â”€ &quot;V955&quot;&gt;=157.5 [s:0.120459 n:19 np:6 miss:0] ; val:&quot;0&quot; prob:[0.736842, 0.263158]\n",
       "        |                 |        |        |        |        |                          |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |        |                          |        |                 â””â”€(neg)â”€ &quot;V1325&quot;&gt;=318.5 [s:0.344133 n:13 np:6 miss:0] ; val:&quot;0&quot; prob:[0.615385, 0.384615]\n",
       "        |                 |        |        |        |        |                          |        |                          â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |        |                          |        |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.285714, 0.714286]\n",
       "        |                 |        |        |        |        |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |        |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |        â””â”€(neg)â”€ &quot;V1460&quot;&gt;=358.5 [s:0.0198448 n:461 np:383 miss:1] ; val:&quot;1&quot; prob:[0.321041, 0.678959]\n",
       "        |                 |        |        |        |                 â”œâ”€(pos)â”€ &quot;V367&quot;&gt;=215.5 [s:0.0272781 n:383 np:373 miss:1] ; val:&quot;1&quot; prob:[0.360313, 0.639687]\n",
       "        |                 |        |        |        |                 |        â”œâ”€(pos)â”€ &quot;V1283&quot;&gt;=463.5 [s:0.0209178 n:373 np:18 miss:0] ; val:&quot;1&quot; prob:[0.343164, 0.656836]\n",
       "        |                 |        |        |        |                 |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                 |        |        â””â”€(neg)â”€ &quot;V367&quot;&gt;=236.5 [s:0.0343646 n:355 np:329 miss:1] ; val:&quot;1&quot; prob:[0.360563, 0.639437]\n",
       "        |                 |        |        |        |                 |        |                 â”œâ”€(pos)â”€ &quot;V650&quot;&gt;=330.5 [s:0.0239509 n:329 np:48 miss:0] ; val:&quot;1&quot; prob:[0.389058, 0.610942]\n",
       "        |                 |        |        |        |                 |        |                 |        â”œâ”€(pos)â”€ &quot;V560&quot;&gt;=195 [s:0.0779014 n:48 np:8 miss:0] ; val:&quot;1&quot; prob:[0.145833, 0.854167]\n",
       "        |                 |        |        |        |                 |        |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                 |        |        |        |                 |        |                 |        |        â””â”€(neg)â”€ &quot;V1199&quot;&gt;=447.5 [s:0.162412 n:40 np:34 miss:0] ; val:&quot;1&quot; prob:[0.075, 0.925]\n",
       "        |                 |        |        |        |                 |        |                 |        |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                 |        |                 |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                 |        |        |        |                 |        |                 |        â””â”€(neg)â”€ &quot;V226&quot;&gt;=114.5 [s:0.0126036 n:281 np:273 miss:1] ; val:&quot;1&quot; prob:[0.430605, 0.569395]\n",
       "        |                 |        |        |        |                 |        |                 |                 â”œâ”€(pos)â”€ &quot;V1286&quot;&gt;=174 [s:0.0220381 n:273 np:17 miss:0] ; val:&quot;1&quot; prob:[0.417582, 0.582418]\n",
       "        |                 |        |        |        |                 |        |                 |                 |        â”œâ”€(pos)â”€ &quot;V189&quot;&gt;=255 [s:0.0765409 n:17 np:5 miss:0] ; val:&quot;1&quot; prob:[0.0588235, 0.941176]\n",
       "        |                 |        |        |        |                 |        |                 |                 |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |        |        |                 |        |                 |                 |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                 |        |                 |                 |        â””â”€(neg)â”€ &quot;V189&quot;&gt;=77.5 [s:0.0426178 n:256 np:229 miss:1] ; val:&quot;1&quot; prob:[0.441406, 0.558594]\n",
       "        |                 |        |        |        |                 |        |                 |                 |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.393013, 0.606987]\n",
       "        |                 |        |        |        |                 |        |                 |                 |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.851852, 0.148148]\n",
       "        |                 |        |        |        |                 |        |                 |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.875, 0.125]\n",
       "        |                 |        |        |        |                 |        |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |        |                 â””â”€(neg)â”€ &quot;V445&quot;&gt;=318.5 [s:0.0362049 n:78 np:21 miss:0] ; val:&quot;1&quot; prob:[0.128205, 0.871795]\n",
       "        |                 |        |        |        |                          â”œâ”€(pos)â”€ &quot;V418&quot;&gt;=104.5 [s:0.192063 n:21 np:16 miss:1] ; val:&quot;1&quot; prob:[0.285714, 0.714286]\n",
       "        |                 |        |        |        |                          |        â”œâ”€(pos)â”€ &quot;V316&quot;&gt;=332.5 [s:0.166454 n:16 np:11 miss:0] ; val:&quot;1&quot; prob:[0.125, 0.875]\n",
       "        |                 |        |        |        |                          |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                          |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |                 |        |        |        |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |        |        |                          â””â”€(neg)â”€ &quot;V1619&quot;&gt;=297.5 [s:0.0676565 n:57 np:34 miss:1] ; val:&quot;1&quot; prob:[0.0701754, 0.929825]\n",
       "        |                 |        |        |        |                                   â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                                   â””â”€(neg)â”€ &quot;V1283&quot;&gt;=133 [s:0.148546 n:23 np:11 miss:1] ; val:&quot;1&quot; prob:[0.173913, 0.826087]\n",
       "        |                 |        |        |        |                                            â”œâ”€(pos)â”€ &quot;V625&quot;&gt;=6.5 [s:0.428026 n:11 np:6 miss:1] ; val:&quot;1&quot; prob:[0.363636, 0.636364]\n",
       "        |                 |        |        |        |                                            |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        |                                            |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |        |        |                                            â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |        â””â”€(neg)â”€ &quot;V790&quot;&gt;=83.5 [s:0.0239795 n:404 np:374 miss:1] ; val:&quot;1&quot; prob:[0.168317, 0.831683]\n",
       "        |                 |        |        |                 â”œâ”€(pos)â”€ &quot;V1411&quot;&gt;=168.5 [s:0.0497173 n:374 np:97 miss:1] ; val:&quot;1&quot; prob:[0.141711, 0.858289]\n",
       "        |                 |        |        |                 |        â”œâ”€(pos)â”€ &quot;V1571&quot;&gt;=483 [s:0.109586 n:97 np:9 miss:0] ; val:&quot;1&quot; prob:[0.340206, 0.659794]\n",
       "        |                 |        |        |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |                 |        |        â””â”€(neg)â”€ &quot;V560&quot;&gt;=1.5 [s:0.151375 n:88 np:32 miss:1] ; val:&quot;1&quot; prob:[0.272727, 0.727273]\n",
       "        |                 |        |        |                 |        |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 |        |                 â””â”€(neg)â”€ &quot;V673&quot;&gt;=154.5 [s:0.165326 n:56 np:13 miss:1] ; val:&quot;1&quot; prob:[0.428571, 0.571429]\n",
       "        |                 |        |        |                 |        |                          â”œâ”€(pos)â”€ &quot;V1485&quot;&gt;=256.5 [s:0.0787269 n:13 np:8 miss:0] ; val:&quot;0&quot; prob:[0.923077, 0.0769231]\n",
       "        |                 |        |        |                 |        |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |                 |        |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |        |                 |        |                          â””â”€(neg)â”€ &quot;V1630&quot;&gt;=305 [s:0.114544 n:43 np:8 miss:0] ; val:&quot;1&quot; prob:[0.27907, 0.72093]\n",
       "        |                 |        |        |                 |        |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.75, 0.25]\n",
       "        |                 |        |        |                 |        |                                   â””â”€(neg)â”€ &quot;V1619&quot;&gt;=275.5 [s:0.0825751 n:35 np:21 miss:1] ; val:&quot;1&quot; prob:[0.171429, 0.828571]\n",
       "        |                 |        |        |                 |        |                                            â”œâ”€(pos)â”€ &quot;V1523&quot;&gt;=58 [s:0.0723006 n:21 np:5 miss:1] ; val:&quot;1&quot; prob:[0.047619, 0.952381]\n",
       "        |                 |        |        |                 |        |                                            |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |        |                 |        |                                            |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 |        |                                            â””â”€(neg)â”€ &quot;V1068&quot;&gt;=962 [s:0.248792 n:14 np:9 miss:0] ; val:&quot;1&quot; prob:[0.357143, 0.642857]\n",
       "        |                 |        |        |                 |        |                                                     â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.111111, 0.888889]\n",
       "        |                 |        |        |                 |        |                                                     â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |        |                 |        â””â”€(neg)â”€ &quot;V190&quot;&gt;=442.5 [s:0.0315244 n:277 np:125 miss:0] ; val:&quot;1&quot; prob:[0.0722022, 0.927798]\n",
       "        |                 |        |        |                 |                 â”œâ”€(pos)â”€ &quot;V625&quot;&gt;=17.5 [s:0.0224812 n:125 np:8 miss:0] ; val:&quot;1&quot; prob:[0.008, 0.992]\n",
       "        |                 |        |        |                 |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.125, 0.875]\n",
       "        |                 |        |        |                 |                 |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 |                 â””â”€(neg)â”€ &quot;V1460&quot;&gt;=566 [s:0.0667914 n:152 np:13 miss:0] ; val:&quot;1&quot; prob:[0.125, 0.875]\n",
       "        |                 |        |        |                 |                          â”œâ”€(pos)â”€ &quot;V574&quot;&gt;=577 [s:0.666278 n:13 np:5 miss:0] ; val:&quot;0&quot; prob:[0.615385, 0.384615]\n",
       "        |                 |        |        |                 |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |                 |                          â””â”€(neg)â”€ &quot;V67&quot;&gt;=6.5 [s:0.0316712 n:139 np:95 miss:1] ; val:&quot;1&quot; prob:[0.0791367, 0.920863]\n",
       "        |                 |        |        |                 |                                   â”œâ”€(pos)â”€ &quot;V673&quot;&gt;=180.5 [s:0.0731905 n:95 np:5 miss:0] ; val:&quot;1&quot; prob:[0.115789, 0.884211]\n",
       "        |                 |        |        |                 |                                   |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |        |                 |                                   |        â””â”€(neg)â”€ &quot;V1286&quot;&gt;=141 [s:0.0388734 n:90 np:34 miss:0] ; val:&quot;1&quot; prob:[0.0777778, 0.922222]\n",
       "        |                 |        |        |                 |                                   |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 |                                   |                 â””â”€(neg)â”€ &quot;V637&quot;&gt;=169 [s:0.0905688 n:56 np:29 miss:0] ; val:&quot;1&quot; prob:[0.125, 0.875]\n",
       "        |                 |        |        |                 |                                   |                          â”œâ”€(pos)â”€ &quot;V790&quot;&gt;=138.5 [s:0.1855 n:29 np:10 miss:1] ; val:&quot;1&quot; prob:[0.241379, 0.758621]\n",
       "        |                 |        |        |                 |                                   |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |                 |        |        |                 |                                   |                          |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.0526316, 0.947368]\n",
       "        |                 |        |        |                 |                                   |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 |                                   â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                 â””â”€(neg)â”€ &quot;V925&quot;&gt;=164.5 [s:0.234453 n:30 np:8 miss:0] ; val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                 |        |        |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                          â””â”€(neg)â”€ &quot;V1571&quot;&gt;=567.5 [s:0.217654 n:22 np:9 miss:0] ; val:&quot;0&quot; prob:[0.681818, 0.318182]\n",
       "        |                 |        |        |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |        |                                   â””â”€(neg)â”€ &quot;V147&quot;&gt;=55.5 [s:0.469354 n:13 np:6 miss:1] ; val:&quot;1&quot; prob:[0.461538, 0.538462]\n",
       "        |                 |        |        |                                            â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |        |                                            â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.857143, 0.142857]\n",
       "        |                 |        |        â””â”€(neg)â”€ &quot;V512&quot;&gt;=15.5 [s:0.0321341 n:548 np:440 miss:1] ; val:&quot;1&quot; prob:[0.135036, 0.864964]\n",
       "        |                 |        |                 â”œâ”€(pos)â”€ &quot;V1485&quot;&gt;=218.5 [s:0.0294915 n:440 np:111 miss:0] ; val:&quot;1&quot; prob:[0.168182, 0.831818]\n",
       "        |                 |        |                 |        â”œâ”€(pos)â”€ &quot;V574&quot;&gt;=514.5 [s:0.0850539 n:111 np:85 miss:1] ; val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "        |                 |        |                 |        |        â”œâ”€(pos)â”€ &quot;V1344&quot;&gt;=0.5 [s:0.0842691 n:85 np:31 miss:1] ; val:&quot;1&quot; prob:[0.223529, 0.776471]\n",
       "        |                 |        |                 |        |        |        â”œâ”€(pos)â”€ &quot;V1181&quot;&gt;=314 [s:0.147462 n:31 np:26 miss:1] ; val:&quot;1&quot; prob:[0.451613, 0.548387]\n",
       "        |                 |        |                 |        |        |        |        â”œâ”€(pos)â”€ &quot;V569&quot;&gt;=373.5 [s:0.308699 n:26 np:16 miss:1] ; val:&quot;1&quot; prob:[0.346154, 0.653846]\n",
       "        |                 |        |                 |        |        |        |        |        â”œâ”€(pos)â”€ &quot;V512&quot;&gt;=68.5 [s:0.0774159 n:16 np:5 miss:1] ; val:&quot;1&quot; prob:[0.0625, 0.9375]\n",
       "        |                 |        |                 |        |        |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |                 |        |        |        |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |        |        |        |        |        â””â”€(neg)â”€ &quot;V790&quot;&gt;=116 [s:0.163897 n:10 np:5 miss:1] ; val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |                 |        |        |        |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |                 |        |                 |        |        |        |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |                 |        |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |                 |        |        |        â””â”€(neg)â”€ &quot;V630&quot;&gt;=517.5 [s:0.0548676 n:54 np:23 miss:0] ; val:&quot;1&quot; prob:[0.0925926, 0.907407]\n",
       "        |                 |        |                 |        |        |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |        |        |                 â””â”€(neg)â”€ &quot;V1523&quot;&gt;=83.5 [s:0.162396 n:31 np:13 miss:1] ; val:&quot;1&quot; prob:[0.16129, 0.83871]\n",
       "        |                 |        |                 |        |        |                          â”œâ”€(pos)â”€ &quot;V28&quot;&gt;=113 [s:0.259163 n:13 np:8 miss:1] ; val:&quot;1&quot; prob:[0.384615, 0.615385]\n",
       "        |                 |        |                 |        |        |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.625, 0.375]\n",
       "        |                 |        |                 |        |        |                          |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |        |        |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |        |        â””â”€(neg)â”€ &quot;V625&quot;&gt;=7 [s:0.165162 n:26 np:9 miss:1] ; val:&quot;0&quot; prob:[0.692308, 0.307692]\n",
       "        |                 |        |                 |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |                 |        |                 â””â”€(neg)â”€ &quot;V625&quot;&gt;=1.5 [s:0.500191 n:17 np:7 miss:1] ; val:&quot;0&quot; prob:[0.529412, 0.470588]\n",
       "        |                 |        |                 |        |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |        |                          â””â”€(neg)â”€ &quot;V1460&quot;&gt;=448 [s:0.0748818 n:10 np:5 miss:1] ; val:&quot;0&quot; prob:[0.9, 0.1]\n",
       "        |                 |        |                 |        |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |        |                 |        |                                   â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |                 |        â””â”€(neg)â”€ &quot;V147&quot;&gt;=50.5 [s:0.0282373 n:329 np:265 miss:1] ; val:&quot;1&quot; prob:[0.112462, 0.887538]\n",
       "        |                 |        |                 |                 â”œâ”€(pos)â”€ &quot;V250&quot;&gt;=163.5 [s:0.0233599 n:265 np:241 miss:1] ; val:&quot;1&quot; prob:[0.0716981, 0.928302]\n",
       "        |                 |        |                 |                 |        â”œâ”€(pos)â”€ &quot;V292&quot;&gt;=410.5 [s:0.0155445 n:241 np:178 miss:1] ; val:&quot;1&quot; prob:[0.0497925, 0.950207]\n",
       "        |                 |        |                 |                 |        |        â”œâ”€(pos)â”€ &quot;V790&quot;&gt;=109.5 [s:0.0194049 n:178 np:135 miss:1] ; val:&quot;1&quot; prob:[0.0674157, 0.932584]\n",
       "        |                 |        |                 |                 |        |        |        â”œâ”€(pos)â”€ &quot;V1523&quot;&gt;=100 [s:0.060763 n:135 np:12 miss:1] ; val:&quot;1&quot; prob:[0.0888889, 0.911111]\n",
       "        |                 |        |                 |                 |        |        |        |        â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=159 [s:0.135656 n:12 np:5 miss:0] ; val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                 |        |                 |                 |        |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |                 |                 |        |        |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.714286, 0.285714]\n",
       "        |                 |        |                 |                 |        |        |        |        â””â”€(neg)â”€ &quot;V226&quot;&gt;=132.5 [s:0.0314455 n:123 np:66 miss:1] ; val:&quot;1&quot; prob:[0.0487805, 0.95122]\n",
       "        |                 |        |                 |                 |        |        |        |                 â”œâ”€(pos)â”€ &quot;V250&quot;&gt;=214.5 [s:0.039098 n:66 np:22 miss:1] ; val:&quot;1&quot; prob:[0.0909091, 0.909091]\n",
       "        |                 |        |                 |                 |        |        |        |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                 |        |        |        |                 |        â””â”€(neg)â”€ &quot;V574&quot;&gt;=554.5 [s:0.0915788 n:44 np:24 miss:0] ; val:&quot;1&quot; prob:[0.136364, 0.863636]\n",
       "        |                 |        |                 |                 |        |        |        |                 |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.25, 0.75]\n",
       "        |                 |        |                 |                 |        |        |        |                 |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                 |        |        |        |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                 |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                 |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                 |        â””â”€(neg)â”€ &quot;V512&quot;&gt;=53 [s:0.260331 n:24 np:16 miss:1] ; val:&quot;1&quot; prob:[0.291667, 0.708333]\n",
       "        |                 |        |                 |                 |                 â”œâ”€(pos)â”€ &quot;V648&quot;&gt;=47.5 [s:0.0774159 n:16 np:5 miss:1] ; val:&quot;1&quot; prob:[0.0625, 0.9375]\n",
       "        |                 |        |                 |                 |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |                 |                 |                 |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                 |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.75, 0.25]\n",
       "        |                 |        |                 |                 â””â”€(neg)â”€ &quot;V630&quot;&gt;=480.5 [s:0.119641 n:64 np:38 miss:1] ; val:&quot;1&quot; prob:[0.28125, 0.71875]\n",
       "        |                 |        |                 |                          â”œâ”€(pos)â”€ &quot;V189&quot;&gt;=115.5 [s:0.219252 n:38 np:11 miss:1] ; val:&quot;1&quot; prob:[0.447368, 0.552632]\n",
       "        |                 |        |                 |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                          |        â””â”€(neg)â”€ &quot;V790&quot;&gt;=129 [s:0.201178 n:27 np:18 miss:1] ; val:&quot;0&quot; prob:[0.62963, 0.37037]\n",
       "        |                 |        |                 |                          |                 â”œâ”€(pos)â”€ &quot;V630&quot;&gt;=524 [s:0.20576 n:18 np:5 miss:0] ; val:&quot;1&quot; prob:[0.444444, 0.555556]\n",
       "        |                 |        |                 |                          |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                          |                 |        â””â”€(neg)â”€ &quot;V195&quot;&gt;=1.5 [s:0.666278 n:13 np:8 miss:1] ; val:&quot;0&quot; prob:[0.615385, 0.384615]\n",
       "        |                 |        |                 |                          |                 |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |                 |                          |                 |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 |                          |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |        |                 |                          â””â”€(neg)â”€ &quot;V512&quot;&gt;=62 [s:0.0667925 n:26 np:5 miss:1] ; val:&quot;1&quot; prob:[0.0384615, 0.961538]\n",
       "        |                 |        |                 |                                   â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |        |                 |                                   â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |        â””â”€(neg)â”€ &quot;V790&quot;&gt;=120.5 [s:0.0494697 n:255 np:87 miss:1] ; val:&quot;0&quot; prob:[0.615686, 0.384314]\n",
       "        |                 |                 â”œâ”€(pos)â”€ &quot;V250&quot;&gt;=197.5 [s:0.0595411 n:87 np:78 miss:1] ; val:&quot;1&quot; prob:[0.402299, 0.597701]\n",
       "        |                 |                 |        â”œâ”€(pos)â”€ &quot;V1286&quot;&gt;=183 [s:0.0794011 n:78 np:13 miss:0] ; val:&quot;1&quot; prob:[0.346154, 0.653846]\n",
       "        |                 |                 |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                 |        |        â””â”€(neg)â”€ &quot;V574&quot;&gt;=543 [s:0.0438343 n:65 np:37 miss:0] ; val:&quot;1&quot; prob:[0.415385, 0.584615]\n",
       "        |                 |                 |        |                 â”œâ”€(pos)â”€ &quot;V913&quot;&gt;=534 [s:0.237112 n:37 np:9 miss:0] ; val:&quot;0&quot; prob:[0.540541, 0.459459]\n",
       "        |                 |                 |        |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                 |        |                 |        â””â”€(neg)â”€ &quot;V445&quot;&gt;=266.5 [s:0.123037 n:28 np:14 miss:1] ; val:&quot;0&quot; prob:[0.714286, 0.285714]\n",
       "        |                 |                 |        |                 |                 â”œâ”€(pos)â”€ &quot;V1281&quot;&gt;=17.5 [s:0.47785 n:14 np:6 miss:1] ; val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                 |                 |        |                 |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                 |        |                 |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.875, 0.125]\n",
       "        |                 |                 |        |                 |                 â””â”€(neg)â”€ &quot;V1369&quot;&gt;=58.5 [s:0.0786035 n:14 np:9 miss:1] ; val:&quot;0&quot; prob:[0.928571, 0.0714286]\n",
       "        |                 |                 |        |                 |                          â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |                 |        |                 |                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |                 |        |                 â””â”€(neg)â”€ &quot;V673&quot;&gt;=111.5 [s:0.184043 n:28 np:10 miss:1] ; val:&quot;1&quot; prob:[0.25, 0.75]\n",
       "        |                 |                 |        |                          â”œâ”€(pos)â”€ &quot;V292&quot;&gt;=477.5 [s:0.0863046 n:10 np:5 miss:0] ; val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |                 |                 |        |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |                 |                 |        |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                 |                 |        |                          â””â”€(neg)â”€ &quot;V1199&quot;&gt;=339 [s:0.0755586 n:18 np:13 miss:1] ; val:&quot;1&quot; prob:[0.0555556, 0.944444]\n",
       "        |                 |                 |        |                                   â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                 |        |                                   â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "        |                 |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.888889, 0.111111]\n",
       "        |                 |                 â””â”€(neg)â”€ &quot;V1460&quot;&gt;=413 [s:0.121617 n:168 np:136 miss:1] ; val:&quot;0&quot; prob:[0.72619, 0.27381]\n",
       "        |                 |                          â”œâ”€(pos)â”€ &quot;V560&quot;&gt;=16.5 [s:0.0674278 n:136 np:44 miss:1] ; val:&quot;0&quot; prob:[0.838235, 0.161765]\n",
       "        |                 |                          |        â”œâ”€(pos)â”€ &quot;V445&quot;&gt;=429 [s:0.157739 n:44 np:6 miss:0] ; val:&quot;0&quot; prob:[0.636364, 0.363636]\n",
       "        |                 |                          |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                          |        |        â””â”€(neg)â”€ &quot;V445&quot;&gt;=281.5 [s:0.161961 n:38 np:15 miss:0] ; val:&quot;0&quot; prob:[0.736842, 0.263158]\n",
       "        |                 |                          |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |                          |        |                 â””â”€(neg)â”€ &quot;V445&quot;&gt;=255 [s:0.281351 n:23 np:6 miss:1] ; val:&quot;0&quot; prob:[0.565217, 0.434783]\n",
       "        |                 |                          |        |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                          |        |                          â””â”€(neg)â”€ &quot;V1523&quot;&gt;=61 [s:0.181909 n:17 np:8 miss:1] ; val:&quot;0&quot; prob:[0.764706, 0.235294]\n",
       "        |                 |                          |        |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |                          |        |                                   â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.555556, 0.444444]\n",
       "        |                 |                          |        â””â”€(neg)â”€ &quot;V517&quot;&gt;=162 [s:0.0861356 n:92 np:7 miss:0] ; val:&quot;0&quot; prob:[0.934783, 0.0652174]\n",
       "        |                 |                          |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.428571, 0.571429]\n",
       "        |                 |                          |                 â””â”€(neg)â”€ &quot;V1286&quot;&gt;=50 [s:0.0405526 n:85 np:69 miss:1] ; val:&quot;0&quot; prob:[0.976471, 0.0235294]\n",
       "        |                 |                          |                          â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |                          |                          â””â”€(neg)â”€ &quot;V871&quot;&gt;=258.5 [s:0.138077 n:16 np:10 miss:1] ; val:&quot;0&quot; prob:[0.875, 0.125]\n",
       "        |                 |                          |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |                          |                                   â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.666667, 0.333333]\n",
       "        |                 |                          â””â”€(neg)â”€ &quot;V574&quot;&gt;=526.5 [s:0.19502 n:32 np:15 miss:1] ; val:&quot;1&quot; prob:[0.25, 0.75]\n",
       "        |                 |                                   â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 |                                   â””â”€(neg)â”€ &quot;V1226&quot;&gt;=502 [s:0.31227 n:17 np:11 miss:1] ; val:&quot;1&quot; prob:[0.470588, 0.529412]\n",
       "        |                 |                                            â”œâ”€(pos)â”€ &quot;V913&quot;&gt;=343 [s:0.280038 n:11 np:6 miss:1] ; val:&quot;0&quot; prob:[0.727273, 0.272727]\n",
       "        |                 |                                            |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                 |                                            |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |                 |                                            â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                 â””â”€(neg)â”€ &quot;V1191&quot;&gt;=216 [s:0.122434 n:127 np:105 miss:1] ; val:&quot;0&quot; prob:[0.653543, 0.346457]\n",
       "        |                          â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=158.5 [s:0.14251 n:105 np:69 miss:0] ; val:&quot;0&quot; prob:[0.761905, 0.238095]\n",
       "        |                          |        â”œâ”€(pos)â”€ &quot;V1523&quot;&gt;=106.5 [s:0.0819693 n:69 np:24 miss:0] ; val:&quot;0&quot; prob:[0.927536, 0.0724638]\n",
       "        |                          |        |        â”œâ”€(pos)â”€ &quot;V316&quot;&gt;=261 [s:0.081216 n:24 np:10 miss:1] ; val:&quot;0&quot; prob:[0.791667, 0.208333]\n",
       "        |                          |        |        |        â”œâ”€(pos)â”€ &quot;V1460&quot;&gt;=377.5 [s:0.0863046 n:10 np:5 miss:1] ; val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "        |                          |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |                          |        |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                          |        |        |        â””â”€(neg)â”€ &quot;V445&quot;&gt;=231 [s:0.0786035 n:14 np:9 miss:1] ; val:&quot;0&quot; prob:[0.928571, 0.0714286]\n",
       "        |                          |        |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                          |        |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "        |                          |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                          |        â””â”€(neg)â”€ &quot;V1553&quot;&gt;=102 [s:0.1269 n:36 np:22 miss:1] ; val:&quot;1&quot; prob:[0.444444, 0.555556]\n",
       "        |                          |                 â”œâ”€(pos)â”€ &quot;V67&quot;&gt;=116 [s:0.381467 n:22 np:6 miss:0] ; val:&quot;0&quot; prob:[0.636364, 0.363636]\n",
       "        |                          |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                          |                 |        â””â”€(neg)â”€ &quot;V1181&quot;&gt;=198 [s:0.0956026 n:16 np:8 miss:1] ; val:&quot;0&quot; prob:[0.875, 0.125]\n",
       "        |                          |                 |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.75, 0.25]\n",
       "        |                          |                 |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "        |                          |                 â””â”€(neg)â”€ &quot;V250&quot;&gt;=225 [s:0.169755 n:14 np:5 miss:0] ; val:&quot;1&quot; prob:[0.142857, 0.857143]\n",
       "        |                          |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "        |                          |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        |                          â””â”€(neg)â”€ &quot;V28&quot;&gt;=231.5 [s:0.209267 n:22 np:6 miss:0] ; val:&quot;1&quot; prob:[0.136364, 0.863636]\n",
       "        |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.5, 0.5]\n",
       "        |                                   â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "        â””â”€(neg)â”€ &quot;V911&quot;&gt;=211 [s:0.0146104 n:1085 np:1005 miss:1] ; val:&quot;0&quot; prob:[0.690323, 0.309677]\n",
       "                 â”œâ”€(pos)â”€ &quot;V871&quot;&gt;=213.5 [s:0.0369922 n:1005 np:521 miss:1] ; val:&quot;0&quot; prob:[0.670647, 0.329353]\n",
       "                 |        â”œâ”€(pos)â”€ &quot;V637&quot;&gt;=73 [s:0.0194272 n:521 np:440 miss:1] ; val:&quot;0&quot; prob:[0.548944, 0.451056]\n",
       "                 |        |        â”œâ”€(pos)â”€ &quot;V67&quot;&gt;=268.5 [s:0.063186 n:440 np:76 miss:0] ; val:&quot;0&quot; prob:[0.590909, 0.409091]\n",
       "                 |        |        |        â”œâ”€(pos)â”€ &quot;V1411&quot;&gt;=159.5 [s:0.135164 n:76 np:44 miss:1] ; val:&quot;1&quot; prob:[0.210526, 0.789474]\n",
       "                 |        |        |        |        â”œâ”€(pos)â”€ &quot;V316&quot;&gt;=490 [s:0.0934224 n:44 np:8 miss:0] ; val:&quot;1&quot; prob:[0.363636, 0.636364]\n",
       "                 |        |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |        |        |        â””â”€(neg)â”€ &quot;V1571&quot;&gt;=463.5 [s:0.18802 n:36 np:7 miss:0] ; val:&quot;1&quot; prob:[0.444444, 0.555556]\n",
       "                 |        |        |        |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |        |        |                 â””â”€(neg)â”€ &quot;V1068&quot;&gt;=981.5 [s:0.126 n:29 np:12 miss:0] ; val:&quot;1&quot; prob:[0.310345, 0.689655]\n",
       "                 |        |        |        |        |                          â”œâ”€(pos)â”€ &quot;V616&quot;&gt;=87.5 [s:0.453913 n:12 np:6 miss:0] ; val:&quot;0&quot; prob:[0.583333, 0.416667]\n",
       "                 |        |        |        |        |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "                 |        |        |        |        |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |        |        |                          â””â”€(neg)â”€ &quot;V1619&quot;&gt;=304.5 [s:0.164266 n:17 np:5 miss:0] ; val:&quot;1&quot; prob:[0.117647, 0.882353]\n",
       "                 |        |        |        |        |                                   â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |        |        |        |        |                                   â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |        â””â”€(neg)â”€ &quot;V367&quot;&gt;=341.5 [s:0.03921 n:364 np:101 miss:0] ; val:&quot;0&quot; prob:[0.67033, 0.32967]\n",
       "                 |        |        |                 â”œâ”€(pos)â”€ &quot;V630&quot;&gt;=488 [s:0.0460358 n:101 np:47 miss:1] ; val:&quot;0&quot; prob:[0.871287, 0.128713]\n",
       "                 |        |        |                 |        â”œâ”€(pos)â”€ &quot;V1571&quot;&gt;=340 [s:0.105993 n:47 np:32 miss:1] ; val:&quot;0&quot; prob:[0.765957, 0.234043]\n",
       "                 |        |        |                 |        |        â”œâ”€(pos)â”€ &quot;V790&quot;&gt;=142 [s:0.469079 n:32 np:13 miss:1] ; val:&quot;0&quot; prob:[0.65625, 0.34375]\n",
       "                 |        |        |                 |        |        |        â”œâ”€(pos)â”€ &quot;V316&quot;&gt;=246.5 [s:0.170472 n:13 np:8 miss:1] ; val:&quot;1&quot; prob:[0.153846, 0.846154]\n",
       "                 |        |        |                 |        |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |                 |        |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |        |        |                 |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                 |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                 |        â””â”€(neg)â”€ &quot;V1630&quot;&gt;=289.5 [s:0.0657434 n:54 np:10 miss:0] ; val:&quot;0&quot; prob:[0.962963, 0.037037]\n",
       "                 |        |        |                 |                 â”œâ”€(pos)â”€ &quot;V445&quot;&gt;=379 [s:0.163897 n:10 np:5 miss:0] ; val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "                 |        |        |                 |                 |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                 |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "                 |        |        |                 |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                 â””â”€(neg)â”€ &quot;V630&quot;&gt;=525 [s:0.025366 n:263 np:22 miss:0] ; val:&quot;0&quot; prob:[0.593156, 0.406844]\n",
       "                 |        |        |                          â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=122.5 [s:0.41308 n:22 np:6 miss:1] ; val:&quot;1&quot; prob:[0.227273, 0.772727]\n",
       "                 |        |        |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.833333, 0.166667]\n",
       "                 |        |        |                          |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |                          â””â”€(neg)â”€ &quot;V316&quot;&gt;=251 [s:0.0368553 n:241 np:186 miss:1] ; val:&quot;0&quot; prob:[0.626556, 0.373444]\n",
       "                 |        |        |                                   â”œâ”€(pos)â”€ &quot;V1068&quot;&gt;=972 [s:0.0457374 n:186 np:71 miss:0] ; val:&quot;0&quot; prob:[0.698925, 0.301075]\n",
       "                 |        |        |                                   |        â”œâ”€(pos)â”€ &quot;V1553&quot;&gt;=117 [s:0.0800056 n:71 np:63 miss:1] ; val:&quot;0&quot; prob:[0.521127, 0.478873]\n",
       "                 |        |        |                                   |        |        â”œâ”€(pos)â”€ &quot;V1485&quot;&gt;=235 [s:0.10899 n:63 np:30 miss:0] ; val:&quot;1&quot; prob:[0.460317, 0.539683]\n",
       "                 |        |        |                                   |        |        |        â”œâ”€(pos)â”€ &quot;V673&quot;&gt;=219.5 [s:0.274359 n:30 np:15 miss:0] ; val:&quot;0&quot; prob:[0.7, 0.3]\n",
       "                 |        |        |                                   |        |        |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                   |        |        |        |        â””â”€(neg)â”€ &quot;V189&quot;&gt;=188.5 [s:0.172609 n:15 np:10 miss:1] ; val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |        |        |                                   |        |        |        |                 â”œâ”€(pos)â”€ &quot;V925&quot;&gt;=122.5 [s:0.163897 n:10 np:5 miss:1] ; val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |        |        |                                   |        |        |        |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |                                   |        |        |        |                 |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |        |        |                                   |        |        |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "                 |        |        |                                   |        |        |        â””â”€(neg)â”€ &quot;V394&quot;&gt;=269.5 [s:0.185051 n:33 np:12 miss:0] ; val:&quot;1&quot; prob:[0.242424, 0.757576]\n",
       "                 |        |        |                                   |        |        |                 â”œâ”€(pos)â”€ &quot;V1325&quot;&gt;=221.5 [s:0.453913 n:12 np:6 miss:1] ; val:&quot;0&quot; prob:[0.583333, 0.416667]\n",
       "                 |        |        |                                   |        |        |                 |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                   |        |        |                 |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "                 |        |        |                                   |        |        |                 â””â”€(neg)â”€ &quot;V394&quot;&gt;=225 [s:0.0723006 n:21 np:16 miss:1] ; val:&quot;1&quot; prob:[0.047619, 0.952381]\n",
       "                 |        |        |                                   |        |        |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |                                   |        |        |                          â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |        |        |                                   |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                   |        â””â”€(neg)â”€ &quot;V1369&quot;&gt;=176.5 [s:0.0750511 n:115 np:62 miss:0] ; val:&quot;0&quot; prob:[0.808696, 0.191304]\n",
       "                 |        |        |                                   |                 â”œâ”€(pos)â”€ &quot;V1286&quot;&gt;=97.5 [s:0.0824311 n:62 np:47 miss:1] ; val:&quot;0&quot; prob:[0.677419, 0.322581]\n",
       "                 |        |        |                                   |                 |        â”œâ”€(pos)â”€ &quot;V189&quot;&gt;=384 [s:0.136621 n:47 np:6 miss:0] ; val:&quot;0&quot; prob:[0.787234, 0.212766]\n",
       "                 |        |        |                                   |                 |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "                 |        |        |                                   |                 |        |        â””â”€(neg)â”€ &quot;V292&quot;&gt;=470.5 [s:0.166892 n:41 np:6 miss:0] ; val:&quot;0&quot; prob:[0.878049, 0.121951]\n",
       "                 |        |        |                                   |                 |        |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "                 |        |        |                                   |                 |        |                 â””â”€(neg)â”€ &quot;V790&quot;&gt;=243 [s:0.0525016 n:35 np:6 miss:0] ; val:&quot;0&quot; prob:[0.971429, 0.0285714]\n",
       "                 |        |        |                                   |                 |        |                          â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.833333, 0.166667]\n",
       "                 |        |        |                                   |                 |        |                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                   |                 |        â””â”€(neg)â”€ &quot;V445&quot;&gt;=298 [s:0.45629 n:15 np:6 miss:0] ; val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "                 |        |        |                                   |                 |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.833333, 0.166667]\n",
       "                 |        |        |                                   |                 |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |                                   |                 â””â”€(neg)â”€ &quot;V911&quot;&gt;=438.5 [s:0.0586667 n:53 np:12 miss:0] ; val:&quot;0&quot; prob:[0.962264, 0.0377358]\n",
       "                 |        |        |                                   |                          â”œâ”€(pos)â”€ &quot;V190&quot;&gt;=293.5 [s:0.17014 n:12 np:5 miss:1] ; val:&quot;0&quot; prob:[0.833333, 0.166667]\n",
       "                 |        |        |                                   |                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "                 |        |        |                                   |                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                   |                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                   â””â”€(neg)â”€ &quot;V1485&quot;&gt;=175.5 [s:0.290552 n:55 np:32 miss:1] ; val:&quot;1&quot; prob:[0.381818, 0.618182]\n",
       "                 |        |        |                                            â”œâ”€(pos)â”€ &quot;V1553&quot;&gt;=221.5 [s:0.195814 n:32 np:16 miss:1] ; val:&quot;0&quot; prob:[0.65625, 0.34375]\n",
       "                 |        |        |                                            |        â”œâ”€(pos)â”€ &quot;V1199&quot;&gt;=415.5 [s:0.452126 n:16 np:11 miss:0] ; val:&quot;1&quot; prob:[0.375, 0.625]\n",
       "                 |        |        |                                            |        |        â”œâ”€(pos)â”€ &quot;V28&quot;&gt;=192.5 [s:0.0771804 n:11 np:6 miss:1] ; val:&quot;1&quot; prob:[0.0909091, 0.909091]\n",
       "                 |        |        |                                            |        |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        |                                            |        |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |        |        |                                            |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                            |        â””â”€(neg)â”€ &quot;V673&quot;&gt;=175.5 [s:0.0774159 n:16 np:5 miss:1] ; val:&quot;0&quot; prob:[0.9375, 0.0625]\n",
       "                 |        |        |                                            |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "                 |        |        |                                            |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |        |                                            â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |        â””â”€(neg)â”€ &quot;V913&quot;&gt;=599.5 [s:0.107202 n:81 np:7 miss:0] ; val:&quot;1&quot; prob:[0.320988, 0.679012]\n",
       "                 |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |                 â””â”€(neg)â”€ &quot;V292&quot;&gt;=605.5 [s:0.167785 n:74 np:8 miss:0] ; val:&quot;1&quot; prob:[0.256757, 0.743243]\n",
       "                 |        |                          â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |                          â””â”€(neg)â”€ &quot;V394&quot;&gt;=261 [s:0.0575253 n:66 np:37 miss:1] ; val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "                 |        |                                   â”œâ”€(pos)â”€ &quot;V1485&quot;&gt;=244 [s:0.287673 n:37 np:12 miss:0] ; val:&quot;1&quot; prob:[0.27027, 0.72973]\n",
       "                 |        |                                   |        â”œâ”€(pos)â”€ &quot;V1281&quot;&gt;=19.5 [s:0.281914 n:12 np:5 miss:0] ; val:&quot;0&quot; prob:[0.75, 0.25]\n",
       "                 |        |                                   |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |        |                                   |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |        |                                   |        â””â”€(neg)â”€ &quot;V1619&quot;&gt;=329 [s:0.0598095 n:25 np:6 miss:0] ; val:&quot;1&quot; prob:[0.04, 0.96]\n",
       "                 |        |                                   |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.166667, 0.833333]\n",
       "                 |        |                                   |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        |                                   â””â”€(neg)â”€ &quot;V1181&quot;&gt;=441 [s:0.0637187 n:29 np:5 miss:0] ; val:&quot;1&quot; prob:[0.0344828, 0.965517]\n",
       "                 |        |                                            â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |        |                                            â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |        â””â”€(neg)â”€ &quot;V189&quot;&gt;=425 [s:0.0111656 n:484 np:56 miss:0] ; val:&quot;0&quot; prob:[0.801653, 0.198347]\n",
       "                 |                 â”œâ”€(pos)â”€ &quot;V673&quot;&gt;=233.5 [s:0.132042 n:56 np:46 miss:0] ; val:&quot;0&quot; prob:[0.625, 0.375]\n",
       "                 |                 |        â”œâ”€(pos)â”€ &quot;V292&quot;&gt;=438.5 [s:0.124174 n:46 np:15 miss:1] ; val:&quot;0&quot; prob:[0.73913, 0.26087]\n",
       "                 |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                 |        |        â””â”€(neg)â”€ &quot;V1619&quot;&gt;=324.5 [s:0.297843 n:31 np:13 miss:0] ; val:&quot;0&quot; prob:[0.612903, 0.387097]\n",
       "                 |                 |        |                 â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                 |        |                 â””â”€(neg)â”€ &quot;V913&quot;&gt;=585.5 [s:0.318257 n:18 np:9 miss:0] ; val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "                 |                 |        |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                 |        |                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.666667, 0.333333]\n",
       "                 |                 |        â””â”€(neg)â”€ &quot;V1199&quot;&gt;=351.5 [s:0.0748818 n:10 np:5 miss:1] ; val:&quot;1&quot; prob:[0.1, 0.9]\n",
       "                 |                 |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |                 |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                 â””â”€(neg)â”€ &quot;V1286&quot;&gt;=252 [s:0.0305215 n:428 np:10 miss:0] ; val:&quot;0&quot; prob:[0.824766, 0.175234]\n",
       "                 |                          â”œâ”€(pos)â”€ &quot;V637&quot;&gt;=298.5 [s:0.0748818 n:10 np:5 miss:0] ; val:&quot;1&quot; prob:[0.1, 0.9]\n",
       "                 |                          |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                          |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |                          â””â”€(neg)â”€ &quot;V1485&quot;&gt;=368.5 [s:0.0241285 n:418 np:73 miss:0] ; val:&quot;0&quot; prob:[0.842105, 0.157895]\n",
       "                 |                                   â”œâ”€(pos)â”€ &quot;V394&quot;&gt;=397 [s:0.0381037 n:73 np:5 miss:0] ; val:&quot;0&quot; prob:[0.986301, 0.0136986]\n",
       "                 |                                   |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "                 |                                   |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                   â””â”€(neg)â”€ &quot;V1068&quot;&gt;=973.5 [s:0.0281949 n:345 np:87 miss:0] ; val:&quot;0&quot; prob:[0.811594, 0.188406]\n",
       "                 |                                            â”œâ”€(pos)â”€ &quot;V292&quot;&gt;=557.5 [s:0.105623 n:87 np:18 miss:0] ; val:&quot;0&quot; prob:[0.643678, 0.356322]\n",
       "                 |                                            |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                            |        â””â”€(neg)â”€ &quot;V648&quot;&gt;=29 [s:0.150203 n:69 np:26 miss:1] ; val:&quot;0&quot; prob:[0.550725, 0.449275]\n",
       "                 |                                            |                 â”œâ”€(pos)â”€ &quot;V1619&quot;&gt;=355 [s:0.228202 n:26 np:5 miss:0] ; val:&quot;0&quot; prob:[0.884615, 0.115385]\n",
       "                 |                                            |                 |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |                                            |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                            |                 â””â”€(neg)â”€ &quot;V673&quot;&gt;=156 [s:0.139091 n:43 np:35 miss:1] ; val:&quot;1&quot; prob:[0.348837, 0.651163]\n",
       "                 |                                            |                          â”œâ”€(pos)â”€ &quot;V1286&quot;&gt;=77 [s:0.124311 n:35 np:24 miss:1] ; val:&quot;1&quot; prob:[0.228571, 0.771429]\n",
       "                 |                                            |                          |        â”œâ”€(pos)â”€ &quot;V418&quot;&gt;=106.5 [s:0.127707 n:24 np:18 miss:1] ; val:&quot;1&quot; prob:[0.0833333, 0.916667]\n",
       "                 |                                            |                          |        |        â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                                            |                          |        |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.333333, 0.666667]\n",
       "                 |                                            |                          |        â””â”€(neg)â”€ &quot;V630&quot;&gt;=527 [s:0.215793 n:11 np:5 miss:0] ; val:&quot;0&quot; prob:[0.545455, 0.454545]\n",
       "                 |                                            |                          |                 â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0.2, 0.8]\n",
       "                 |                                            |                          |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.833333, 0.166667]\n",
       "                 |                                            |                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.875, 0.125]\n",
       "                 |                                            â””â”€(neg)â”€ &quot;V1571&quot;&gt;=196 [s:0.0318577 n:258 np:244 miss:1] ; val:&quot;0&quot; prob:[0.868217, 0.131783]\n",
       "                 |                                                     â”œâ”€(pos)â”€ &quot;V250&quot;&gt;=247.5 [s:0.0318558 n:244 np:98 miss:0] ; val:&quot;0&quot; prob:[0.893443, 0.106557]\n",
       "                 |                                                     |        â”œâ”€(pos)â”€ &quot;V1630&quot;&gt;=285 [s:0.0395201 n:98 np:15 miss:0] ; val:&quot;0&quot; prob:[0.979592, 0.0204082]\n",
       "                 |                                                     |        |        â”œâ”€(pos)â”€ &quot;V913&quot;&gt;=452.5 [s:0.168337 n:15 np:5 miss:1] ; val:&quot;0&quot; prob:[0.866667, 0.133333]\n",
       "                 |                                                     |        |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "                 |                                                     |        |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                                     |        |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                                     |        â””â”€(neg)â”€ &quot;V316&quot;&gt;=220.5 [s:0.0549939 n:146 np:127 miss:1] ; val:&quot;0&quot; prob:[0.835616, 0.164384]\n",
       "                 |                                                     |                 â”œâ”€(pos)â”€ &quot;V1460&quot;&gt;=528 [s:0.0390997 n:127 np:100 miss:0] ; val:&quot;0&quot; prob:[0.889764, 0.110236]\n",
       "                 |                                                     |                 |        â”œâ”€(pos)â”€ &quot;V630&quot;&gt;=480.5 [s:0.098058 n:100 np:78 miss:1] ; val:&quot;0&quot; prob:[0.94, 0.06]\n",
       "                 |                                                     |                 |        |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                                     |                 |        |        â””â”€(neg)â”€ &quot;V1460&quot;&gt;=581.5 [s:0.280038 n:22 np:10 miss:0] ; val:&quot;0&quot; prob:[0.727273, 0.272727]\n",
       "                 |                                                     |                 |        |                 â”œâ”€(pos)â”€ &quot;V374&quot;&gt;=105.5 [s:0.42281 n:10 np:5 miss:1] ; val:&quot;1&quot; prob:[0.4, 0.6]\n",
       "                 |                                                     |                 |        |                 |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.8, 0.2]\n",
       "                 |                                                     |                 |        |                 |        â””â”€(neg)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                                                     |                 |        |                 â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                                     |                 |        â””â”€(neg)â”€ &quot;V195&quot;&gt;=0.5 [s:0.204282 n:27 np:19 miss:1] ; val:&quot;0&quot; prob:[0.703704, 0.296296]\n",
       "                 |                                                     |                 |                 â”œâ”€(pos)â”€ &quot;V226&quot;&gt;=269.5 [s:0.135491 n:19 np:13 miss:0] ; val:&quot;0&quot; prob:[0.894737, 0.105263]\n",
       "                 |                                                     |                 |                 |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                                     |                 |                 |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.666667, 0.333333]\n",
       "                 |                                                     |                 |                 â””â”€(neg)â”€ val:&quot;1&quot; prob:[0.25, 0.75]\n",
       "                 |                                                     |                 â””â”€(neg)â”€ &quot;V394&quot;&gt;=276.5 [s:0.41726 n:19 np:8 miss:0] ; val:&quot;1&quot; prob:[0.473684, 0.526316]\n",
       "                 |                                                     |                          â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                                                     |                          â””â”€(neg)â”€ &quot;V560&quot;&gt;=67 [s:0.168225 n:11 np:6 miss:0] ; val:&quot;0&quot; prob:[0.818182, 0.181818]\n",
       "                 |                                                     |                                   â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                 |                                                     |                                   â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.6, 0.4]\n",
       "                 |                                                     â””â”€(neg)â”€ &quot;V673&quot;&gt;=139 [s:0.361574 n:14 np:6 miss:1] ; val:&quot;1&quot; prob:[0.428571, 0.571429]\n",
       "                 |                                                              â”œâ”€(pos)â”€ val:&quot;1&quot; prob:[0, 1]\n",
       "                 |                                                              â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.75, 0.25]\n",
       "                 â””â”€(neg)â”€ &quot;V569&quot;&gt;=219 [s:0.0533978 n:80 np:72 miss:1] ; val:&quot;0&quot; prob:[0.9375, 0.0625]\n",
       "                          â”œâ”€(pos)â”€ &quot;V1630&quot;&gt;=359.5 [s:0.0738877 n:72 np:6 miss:0] ; val:&quot;0&quot; prob:[0.972222, 0.0277778]\n",
       "                          |        â”œâ”€(pos)â”€ val:&quot;0&quot; prob:[0.666667, 0.333333]\n",
       "                          |        â””â”€(neg)â”€ val:&quot;0&quot; prob:[1, 0]\n",
       "                          â””â”€(neg)â”€ val:&quot;0&quot; prob:[0.625, 0.375]\n",
       "</pre></div></div></div>"
      ],
      "text/plain": [
       "<ydf.utils.html.HtmlNotebookDisplay at 0x7f7a963d98d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection of a model with cheaper variable importances\n",
    "\n",
    "The previous model took some time to train. The main reason is that out-of-bag random forest variable importance (enabled with `compute_oob_variable_importances=True`) is expensive to compute. Alternatively, it is possible to do feature selection with cheap structural variable importances (though, the results are not as good):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run backward feature selection on 1636 features\n",
      "[Iteration 0] Train model on 1636 features\n",
      "Optimizing metric \"loss\". The available metrics are ['loss', 'num_examples_weighted', 'accuracy']\n",
      "[Iteration 0] Score:-0.551959 Metrics:{'loss': 0.5519588122415486, 'num_examples_weighted': 3792.0, 'accuracy': 0.7241561181434599}\n",
      "Guide feature selection using \"INV_MEAN_MIN_DEPTH\" variable importance. The available variable importances are ['NUM_NODES', 'SUM_SCORE', 'NUM_AS_ROOT', 'INV_MEAN_MIN_DEPTH']\n",
      "[Iteration 1] Train model on 1439 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:02.142809\n",
      "[Iteration 1] Score:-0.550163 Metrics:{'loss': 0.5501630669081473, 'num_examples_weighted': 3792.0, 'accuracy': 0.7209915611814346}\n",
      "[Iteration 2] Train model on 1296 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.947157\n",
      "[Iteration 2] Score:-0.551167 Metrics:{'loss': 0.5511673348405661, 'num_examples_weighted': 3792.0, 'accuracy': 0.7212552742616034}\n",
      "[Iteration 3] Train model on 1167 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.767325\n",
      "[Iteration 3] Score:-0.54512 Metrics:{'loss': 0.5451197947457999, 'num_examples_weighted': 3792.0, 'accuracy': 0.7302215189873418}\n",
      "[Iteration 4] Train model on 1051 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.619976\n",
      "[Iteration 4] Score:-0.54432 Metrics:{'loss': 0.544320323971888, 'num_examples_weighted': 3792.0, 'accuracy': 0.729957805907173}\n",
      "[Iteration 5] Train model on 946 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.511177\n",
      "[Iteration 5] Score:-0.544704 Metrics:{'loss': 0.5447036220995662, 'num_examples_weighted': 3792.0, 'accuracy': 0.7267932489451476}\n",
      "[Iteration 6] Train model on 852 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.353484\n",
      "[Iteration 6] Score:-0.542823 Metrics:{'loss': 0.5428225361223368, 'num_examples_weighted': 3792.0, 'accuracy': 0.7360232067510548}\n",
      "[Iteration 7] Train model on 767 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.227989\n",
      "[Iteration 7] Score:-0.54294 Metrics:{'loss': 0.5429396850972059, 'num_examples_weighted': 3792.0, 'accuracy': 0.7289029535864979}\n",
      "[Iteration 8] Train model on 691 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.145872\n",
      "[Iteration 8] Score:-0.536023 Metrics:{'loss': 0.5360227765661334, 'num_examples_weighted': 3792.0, 'accuracy': 0.7325949367088608}\n",
      "[Iteration 9] Train model on 622 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:01.038713\n",
      "[Iteration 9] Score:-0.536345 Metrics:{'loss': 0.5363452149990293, 'num_examples_weighted': 3792.0, 'accuracy': 0.7278481012658228}\n",
      "[Iteration 10] Train model on 560 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.961483\n",
      "[Iteration 10] Score:-0.538167 Metrics:{'loss': 0.5381672166689213, 'num_examples_weighted': 3792.0, 'accuracy': 0.7339135021097046}\n",
      "[Iteration 11] Train model on 504 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.902620\n",
      "[Iteration 11] Score:-0.535654 Metrics:{'loss': 0.5356539857548265, 'num_examples_weighted': 3792.0, 'accuracy': 0.7273206751054853}\n",
      "[Iteration 12] Train model on 454 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.844499\n",
      "[Iteration 12] Score:-0.531586 Metrics:{'loss': 0.5315862291090717, 'num_examples_weighted': 3792.0, 'accuracy': 0.7312763713080169}\n",
      "[Iteration 13] Train model on 409 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.790032\n",
      "[Iteration 13] Score:-0.52936 Metrics:{'loss': 0.529360163875832, 'num_examples_weighted': 3792.0, 'accuracy': 0.7386603375527426}\n",
      "[Iteration 14] Train model on 369 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.730465\n",
      "[Iteration 14] Score:-0.52757 Metrics:{'loss': 0.5275697079769587, 'num_examples_weighted': 3792.0, 'accuracy': 0.7354957805907173}\n",
      "[Iteration 15] Train model on 333 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.686591\n",
      "[Iteration 15] Score:-0.527646 Metrics:{'loss': 0.5276461803994853, 'num_examples_weighted': 3792.0, 'accuracy': 0.7365506329113924}\n",
      "[Iteration 16] Train model on 300 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.644446\n",
      "[Iteration 16] Score:-0.525423 Metrics:{'loss': 0.5254225039931765, 'num_examples_weighted': 3792.0, 'accuracy': 0.7365506329113924}\n",
      "[Iteration 17] Train model on 270 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.601740\n",
      "[Iteration 17] Score:-0.527827 Metrics:{'loss': 0.5278265483015244, 'num_examples_weighted': 3792.0, 'accuracy': 0.7349683544303798}\n",
      "[Iteration 18] Train model on 243 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.565323\n",
      "[Iteration 18] Score:-0.526025 Metrics:{'loss': 0.5260247390217222, 'num_examples_weighted': 3792.0, 'accuracy': 0.7420886075949367}\n",
      "[Iteration 19] Train model on 219 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.530170\n",
      "[Iteration 19] Score:-0.526643 Metrics:{'loss': 0.5266434014890292, 'num_examples_weighted': 3792.0, 'accuracy': 0.7368143459915611}\n",
      "[Iteration 20] Train model on 198 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.516604\n",
      "[Iteration 20] Score:-0.523294 Metrics:{'loss': 0.5232937450406244, 'num_examples_weighted': 3792.0, 'accuracy': 0.7423523206751055}\n",
      "[Iteration 21] Train model on 179 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.487829\n",
      "[Iteration 21] Score:-0.526272 Metrics:{'loss': 0.5262718796621977, 'num_examples_weighted': 3792.0, 'accuracy': 0.7354957805907173}\n",
      "[Iteration 22] Train model on 162 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.454017\n",
      "[Iteration 22] Score:-0.525777 Metrics:{'loss': 0.5257771438292723, 'num_examples_weighted': 3792.0, 'accuracy': 0.7383966244725738}\n",
      "[Iteration 23] Train model on 146 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.441345\n",
      "[Iteration 23] Score:-0.526585 Metrics:{'loss': 0.5265850477229572, 'num_examples_weighted': 3792.0, 'accuracy': 0.7376054852320675}\n",
      "[Iteration 24] Train model on 132 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.411893\n",
      "[Iteration 24] Score:-0.528028 Metrics:{'loss': 0.5280284808348384, 'num_examples_weighted': 3792.0, 'accuracy': 0.7386603375527426}\n",
      "[Iteration 25] Train model on 119 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.385073\n",
      "[Iteration 25] Score:-0.526257 Metrics:{'loss': 0.5262570344267771, 'num_examples_weighted': 3792.0, 'accuracy': 0.7407700421940928}\n",
      "[Iteration 26] Train model on 108 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.375447\n",
      "[Iteration 26] Score:-0.525778 Metrics:{'loss': 0.5257777381078772, 'num_examples_weighted': 3792.0, 'accuracy': 0.7381329113924051}\n",
      "[Iteration 27] Train model on 98 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.352707\n",
      "[Iteration 27] Score:-0.528158 Metrics:{'loss': 0.5281579625610565, 'num_examples_weighted': 3792.0, 'accuracy': 0.7362869198312236}\n",
      "[Iteration 28] Train model on 89 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.373138\n",
      "[Iteration 28] Score:-0.527079 Metrics:{'loss': 0.5270788383762006, 'num_examples_weighted': 3792.0, 'accuracy': 0.73707805907173}\n",
      "[Iteration 29] Train model on 81 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.388836\n",
      "[Iteration 29] Score:-0.536969 Metrics:{'loss': 0.5369688785725316, 'num_examples_weighted': 3792.0, 'accuracy': 0.7331223628691983}\n",
      "[Iteration 30] Train model on 73 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.374465\n",
      "[Iteration 30] Score:-0.524971 Metrics:{'loss': 0.5249709710911042, 'num_examples_weighted': 3792.0, 'accuracy': 0.7357594936708861}\n",
      "[Iteration 31] Train model on 66 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.392811\n",
      "[Iteration 31] Score:-0.532895 Metrics:{'loss': 0.5328945312494282, 'num_examples_weighted': 3792.0, 'accuracy': 0.7376054852320675}\n",
      "[Iteration 32] Train model on 60 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.423367\n",
      "[Iteration 32] Score:-0.527182 Metrics:{'loss': 0.5271819541240622, 'num_examples_weighted': 3792.0, 'accuracy': 0.7357594936708861}\n",
      "[Iteration 33] Train model on 54 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.392741\n",
      "[Iteration 33] Score:-0.528723 Metrics:{'loss': 0.5287229971995062, 'num_examples_weighted': 3792.0, 'accuracy': 0.73707805907173}\n",
      "[Iteration 34] Train model on 49 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.319166\n",
      "[Iteration 34] Score:-0.528887 Metrics:{'loss': 0.5288865787803808, 'num_examples_weighted': 3792.0, 'accuracy': 0.732331223628692}\n",
      "[Iteration 35] Train model on 45 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.300177\n",
      "[Iteration 35] Score:-0.528979 Metrics:{'loss': 0.528979440804233, 'num_examples_weighted': 3792.0, 'accuracy': 0.7397151898734177}\n",
      "[Iteration 36] Train model on 41 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.309298\n",
      "[Iteration 36] Score:-0.533305 Metrics:{'loss': 0.5333054795554732, 'num_examples_weighted': 3792.0, 'accuracy': 0.7365506329113924}\n",
      "[Iteration 37] Train model on 37 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.249781\n",
      "[Iteration 37] Score:-0.538639 Metrics:{'loss': 0.5386387339801987, 'num_examples_weighted': 3792.0, 'accuracy': 0.7389240506329114}\n",
      "[Iteration 38] Train model on 34 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.230058\n",
      "[Iteration 38] Score:-0.540805 Metrics:{'loss': 0.5408050190958976, 'num_examples_weighted': 3792.0, 'accuracy': 0.7333860759493671}\n",
      "[Iteration 39] Train model on 31 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.225195\n",
      "[Iteration 39] Score:-0.540682 Metrics:{'loss': 0.5406823953358704, 'num_examples_weighted': 3792.0, 'accuracy': 0.7341772151898734}\n",
      "[Iteration 40] Train model on 28 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.227766\n",
      "[Iteration 40] Score:-0.542312 Metrics:{'loss': 0.5423118199284664, 'num_examples_weighted': 3792.0, 'accuracy': 0.7365506329113924}\n",
      "[Iteration 41] Train model on 26 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.223830\n",
      "[Iteration 41] Score:-0.53337 Metrics:{'loss': 0.533370310344138, 'num_examples_weighted': 3792.0, 'accuracy': 0.7341772151898734}\n",
      "[Iteration 42] Train model on 24 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.205201\n",
      "[Iteration 42] Score:-0.546512 Metrics:{'loss': 0.5465117353147273, 'num_examples_weighted': 3792.0, 'accuracy': 0.7315400843881856}\n",
      "[Iteration 43] Train model on 22 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.201109\n",
      "[Iteration 43] Score:-0.536157 Metrics:{'loss': 0.5361573547169071, 'num_examples_weighted': 3792.0, 'accuracy': 0.7254746835443038}\n",
      "[Iteration 44] Train model on 20 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.202987\n",
      "[Iteration 44] Score:-0.55134 Metrics:{'loss': 0.5513404927957942, 'num_examples_weighted': 3792.0, 'accuracy': 0.7302215189873418}\n",
      "[Iteration 45] Train model on 18 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.239007\n",
      "[Iteration 45] Score:-0.55627 Metrics:{'loss': 0.5562704338921521, 'num_examples_weighted': 3792.0, 'accuracy': 0.7223101265822784}\n",
      "[Iteration 46] Train model on 17 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.198862\n",
      "[Iteration 46] Score:-0.555487 Metrics:{'loss': 0.5554866864572413, 'num_examples_weighted': 3792.0, 'accuracy': 0.7212552742616034}\n",
      "[Iteration 47] Train model on 16 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.182395\n",
      "[Iteration 47] Score:-0.548997 Metrics:{'loss': 0.548997306794946, 'num_examples_weighted': 3792.0, 'accuracy': 0.7228375527426161}\n",
      "[Iteration 48] Train model on 15 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.182531\n",
      "[Iteration 48] Score:-0.545317 Metrics:{'loss': 0.5453172084761937, 'num_examples_weighted': 3792.0, 'accuracy': 0.7283755274261603}\n",
      "[Iteration 49] Train model on 14 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.180795\n",
      "[Iteration 49] Score:-0.566814 Metrics:{'loss': 0.5668138865045496, 'num_examples_weighted': 3792.0, 'accuracy': 0.7265295358649789}\n",
      "[Iteration 50] Train model on 13 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.176973\n",
      "[Iteration 50] Score:-0.549675 Metrics:{'loss': 0.5496751580255711, 'num_examples_weighted': 3792.0, 'accuracy': 0.7283755274261603}\n",
      "[Iteration 51] Train model on 12 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.176852\n",
      "[Iteration 51] Score:-0.569089 Metrics:{'loss': 0.5690891260837739, 'num_examples_weighted': 3792.0, 'accuracy': 0.7241561181434599}\n",
      "[Iteration 52] Train model on 11 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.177858\n",
      "[Iteration 52] Score:-0.570919 Metrics:{'loss': 0.5709192840562237, 'num_examples_weighted': 3792.0, 'accuracy': 0.7191455696202531}\n",
      "[Iteration 53] Train model on 10 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.173121\n",
      "[Iteration 53] Score:-0.556233 Metrics:{'loss': 0.5562327019322589, 'num_examples_weighted': 3792.0, 'accuracy': 0.7194092827004219}\n",
      "[Iteration 54] Train model on 9 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.162711\n",
      "[Iteration 54] Score:-0.569022 Metrics:{'loss': 0.5690220831412005, 'num_examples_weighted': 3792.0, 'accuracy': 0.7172995780590717}\n",
      "[Iteration 55] Train model on 8 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.168023\n",
      "[Iteration 55] Score:-0.591226 Metrics:{'loss': 0.5912258343182638, 'num_examples_weighted': 3792.0, 'accuracy': 0.7078059071729957}\n",
      "[Iteration 56] Train model on 7 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.164583\n",
      "[Iteration 56] Score:-0.59405 Metrics:{'loss': 0.5940503574409699, 'num_examples_weighted': 3792.0, 'accuracy': 0.7128164556962026}\n",
      "[Iteration 57] Train model on 6 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.163499\n",
      "[Iteration 57] Score:-0.615233 Metrics:{'loss': 0.6152327838542926, 'num_examples_weighted': 3792.0, 'accuracy': 0.7030590717299579}\n",
      "[Iteration 58] Train model on 5 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.162986\n",
      "[Iteration 58] Score:-0.614771 Metrics:{'loss': 0.6147712802283323, 'num_examples_weighted': 3792.0, 'accuracy': 0.6954113924050633}\n",
      "[Iteration 59] Train model on 4 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.159788\n",
      "[Iteration 59] Score:-0.627355 Metrics:{'loss': 0.6273552657896849, 'num_examples_weighted': 3792.0, 'accuracy': 0.6975210970464135}\n",
      "[Iteration 60] Train model on 3 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.159901\n",
      "[Iteration 60] Score:-0.71192 Metrics:{'loss': 0.7119195723415201, 'num_examples_weighted': 3792.0, 'accuracy': 0.6890822784810127}\n",
      "[Iteration 61] Train model on 2 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.152983\n",
      "[Iteration 61] Score:-0.792582 Metrics:{'loss': 0.7925818626617105, 'num_examples_weighted': 3792.0, 'accuracy': 0.6677215189873418}\n",
      "[Iteration 62] Train model on 1 features\n",
      "Train model on 3792 examples\n",
      "Model trained in 0:00:00.135520\n",
      "[Iteration 62] Score:-1.82378 Metrics:{'loss': 1.8237768061927428, 'num_examples_weighted': 3792.0, 'accuracy': 0.6215717299578059}\n",
      "The best subset of features was found at iteration 20 with score:-0.523294, metrics:{'loss': 0.5232937450406244, 'num_examples_weighted': 3792.0, 'accuracy': 0.7423523206751055} and 198/1636 selected features\n",
      "Accuracy: 0.7263222632226323\n",
      "Number of features: 198\n"
     ]
    }
   ],
   "source": [
    "model_3 = ydf.RandomForestLearner(\n",
    "    label=\"class\",\n",
    "    # Enable feature selection\n",
    "    feature_selector=ydf.BackwardSelectionFeatureSelector(\n",
    "        # Allow to fallback on structural variable importance\n",
    "        # if not other variable importance is available.\n",
    "        allow_structural_variable_importance=True\n",
    "    ),\n",
    ").train(train_dataset)\n",
    "\n",
    "evaluation_3 = model_3.evaluate(test_dataset)\n",
    "\n",
    "print(\"Accuracy:\", evaluation_3.accuracy)\n",
    "print(\"Number of features:\", len(model_3.input_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this model as an accuracy of 0.726322 which is better than the model without feature selection (0.7170) but worst than the model with the OOB variable importances (0.7478).\n",
    "Also, less features were removed (the final model has 198 features instead of 81)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To go further\n",
    "\n",
    "In those examples, we showed how to use the Random Forest learner. Feature selection is also possible with other learners such as Gradient Boosted Trees and CART. Notably, while Random Forest is generally better than Gradient Boosted Trees on datasets with a small number of examples compared to the number of input features, feature selection on Gradient Boosted Trees is faster.\n",
    "\n",
    "In the example above, the models were trained without a validation dataset. Instead, if you provide a validation dataset (using the `valid` argument of the `train` method), the objective metric and variable importance will be computed on this validation dataset. This is especially useful if you suspect your dataset has a distribution shift. Using a validation dataset is also significantly faster for the Random Forest learner. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
